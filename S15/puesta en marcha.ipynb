{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MA6202: Laboratorio de Ciencia de Datos\n",
    "**Profesor: Nicolás Caro** \n",
    "\n",
    "**22/07/2020 - S15** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Puesta en Marcha: Introducción a Flask\n",
    "\n",
    "La puesta en marcha o *despliegue* consiste en el flujo de trabajo necesario para hacer que una aplicación pasa de un estado de desarrollo experimental (prueba de concepto) a ser una versión de *producción* donde el usuario final tendrá acceso. \n",
    "\n",
    "Para poner en marcha nuestros proyectos de ciencia de datos, haremos uso de *aplicaciones web*. Estas consisten programas diseñados para ejecutarse desde un servidor web. Esta aproximación nos permitirá facilitar resultados y visualizaciones a una amplia variedad de sistemas. \n",
    "\n",
    "En Python existen conjuntos de herramientas para desarrollo, dentro de estas se utilizará Flask por su enfoque minimal. Antes de estudiar esta herramienta, estudiamos el manejo de entornos virtuales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambientes Virtuales\n",
    "\n",
    "Cada aplicación de Python posee sus requerimientos en cuanto a las librerías sobre las cuales se basa. Esto hace que en algunas oportunidades se trabaje en aplicaciones que requieran distintas versiones de una misma librería o que trabajen sobre versiones distintas de Python. En este caso, una instalación global de Python no permitiría trabajar de manera fluida, pues se necesitaría reinstalar paquetes de distintas versiones cada vez que se cambie de aplicación.\n",
    "\n",
    "Para solucionar el problema anterior aparecen los **entornos virtuales**, estos consisten en conjuntos de carpetas autocontenidos en cuanto a sus dependencias, para lograr esta independencia, cada entorno irtual presenta su propia instalación de Python, pudiendo elegir incluso que versión del lenguaje se desea instalar. \n",
    "\n",
    "En general se recomienda el uso de entornos virtuales para manejar dependencias de proyectos de software basados en Python, tanto en el desarrollo de estos como como en su puesta en marcha o producción.\n",
    "\n",
    "Python 3 posee un módulo que permite crear entornos virtuales, este es `venv`, cabe destacar que esta librería no es la única forma de manejar entornos virtuales (ej: conda ofrece una herramienta similar) pero si es la estándar en el stack de Python. \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Para crear un entorno virtual, nos localizamos en la carpeta raíz de nuestro proyecto, en este caso será `./ProjectLab`, sobre tal carpeta inicializamos un entorno virtual usando `venv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ProjectLab\n",
    "!python -m venv entorno_virtual\n",
    "\n",
    "#En windows la orden es\n",
    "#py -m venv entorno_virtual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con lo anterior, se ha creado un a carpeta llamada `./ProjectLab/entorno_virtual` la cual contiene el código necesario para generar un entorno virtual independiente de la instalación global de Python. Para acceder a dicho entorno ejecutamos el archivo `activate` dentro de la carpeta `./entorno_virtual/bin` esto se hace por medio de:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!. entorno_virtual/bin/activate\n",
    "\n",
    "# En Windows\n",
    "# entorno_virtual\\Scripts\\activate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "una vez activado un entorno virtual, no encontramos localmente dentro una nueva instalción de Python, la cual maneja sus propias dependencias, en este caso, si buscamos importar un paquete de la instalación global (ej: Numpy o cualquier otra librería de terceros), no se podrá acceder, pues mientras el ambiente viertual este activado, se ignora la instalación global. \n",
    "\n",
    "Para trabajar en conjunción con notebooks de Jupyter podemos instalar la librería `ipykernel` en nuestro entorno virtual, esto lo hacemos por medio de las ordenes:\n",
    "\n",
    "```\n",
    "user@ruta/a/proyecto$ . entorno_virtual/bin/activate  \n",
    "user@ruta/a/proyecto$ pip install ipykernel\n",
    "```\n",
    "\n",
    "Donde `user@ruta/a/proyecto$` hace referencia a que se debe ejecutar en la carpeta de nuestro proyecto `ProjectLab` desde una consola, no funcionará usando el comando `!` en una celda. \n",
    "\n",
    "**Obs:** En Windows equivale a activar el ambiente y luego instalar el paquete `ipykernel` usando pip. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez instalada la librería podemos registrar el kernel asociado a nuestro entorno virtual, para ello ejecutamos \n",
    "\n",
    "```\n",
    "(entorno_virtual) user@ruta/a/proyecto$ python -m ipykernel install --user --name entorno_virtual --display-name \"Python (entorno_virtual)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde la linea `(entorno_virtual) user@ruta/a/proyecto$` indica que se ejecuta dentro de un entorno virtual activado. Esta linea instala un kernel llamado `entorno_virtual` asociado a nuestro entorno de ejecución actual (indicado en la consola) que corresponde en este caso al entorno virtual activado. El nombre con el cual aparece dicho kernel en un notebook de Jupyter será `\"Python (entorno_virtual)\"`. Finalmente, podemos cambiar de kernel usando la opción `kernel -> change kernel -> Python (entorno_virtual)` (puede ser necesario actualizar la página sociada al notebook actual). \n",
    "\n",
    "\n",
    "Con lo anterior, el notebook se reinicia pero se tiene conexión directa con el entorno virtual creado.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se cambia de kernel pasando al creado recientemente, como nos encontramos en una instalación nueva de Python no habrá disponibilidad a paquetes instalados en el entorno global. Intentaremos acceder a NumPy desde este nuevo kernel asociado al entorno virtual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    import numpy\n",
    "    print('Numpy instalado en este entorno virtual')\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    \n",
    "    print('Modulo NumPy no encontrado en este entorno virtual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lo que confirmamos que nos encontramos accediendo al nuevo entorno virtual. Lamentablemente, el comando `!` está asociado al entorno en el cual se ejecuta Jupyter al iniciar y no al kernel con el que estamos trabajando, por tal motivo, si ejecutamos `!pip install numpy` desde una celda, no se instalará en el entorno virtual. Para instalar librerías, necesitamos por tanto instalarlas directamente desde la consola habiendo activado el entorno virtual previamente. \n",
    "\n",
    "Para salir de un entorno virtual basta con *desactivarlo*, esto se hace ejecutando la orden `deactivate` desde la consola en un entorno virtual previamente activado. Este comando se agrega a la consola / terminal cada vez que activamos un entorno virtual por lo que podemos ejecutarla desde cualquier carpeta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción a Flask\n",
    "\n",
    "`Flask` es un *micro web framework* escrito en Python. Es decir, corresponde a un conjunto de herramientas para desarrollo web (web framework) minimal (micro). En este sentido, el prefijo *micro* hacer referencia a que una aplicación debe ser sencilla en sus componentes, esto no afecta a la funcionalidad, pues se tiene acceso a multiples extensiones, así la minimalidad del framework hace referencia a mantener un núcleo simple pero a la vez extensible, logrando así que el programador tenga total control sobre que componentes integrar, evitando redundancia en el código y complejidades extra.\n",
    "\n",
    "Para instalar Flask hacemos uso de la sintaxis usual. Esta vez se recomienda hacerlo dentro de un entorno virtual, de manera que se pueda desarrollar una aplicación web autocontenida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Instale flask `pip install flask` dentro del entorno virtual `entorno_virtual` creado anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flask permite crear aplicación que hace uso de la convención WSGI (**W**eb  **S**erver **G**ateway **I**nterface - *whiskey*), esta consiste en un protocolo de servidores web para el manejo de consultas por medio de aplicaciones o frameworks de Python. \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Creamos una aplicación minimal usando Flask, para ello importamos la clase `Flask`, la cual corresponde a una aplicación WSGI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego se crea un objeto como instancia de dicha clase. El primer argumento de este objeto en su constructor, será el nombre del módulo al cual dicha aplicación pertenece. Al usar solamente un módulo, se recomienda utilizar la variable de sistema `__name__`. Esta variable entrega el nombre del módulo sobre el cual se ejecuta. Observemos su comportamiento :\n",
    "\n",
    "1. Se crea un módulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file modulo_1.py\n",
    "\n",
    "def func1():\n",
    "    print('Valor de __name__ : ' + __name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Se importa dicho módulo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modulo_1\n",
    "\n",
    "modulo_1.func1() \n",
    "print('Valor de __name__ fuera del modulo:' + __name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que dentro del módulo creado, la variable `__name__` cambia, esto permite asociar una aplicación de Flask al módulo sobre el cual se desea operar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lo anterior, se iniciado una aplicación de Flask, la cual buscará dependendcias en el módulo `__name__` (`__main__` en este caso).\n",
    "\n",
    "El siguiente paso es usar un *decorador de ruta*  `route(url)`, el cual le dice a Flask que acción tomar (que función ejecutar) cuando se accede a la dirección `url`. De esta manera, se define una función de bienvenida, la cual se activa cuando accedemos a la dirección raíz de la aplicación `/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/') \n",
    "def bienvenida():\n",
    "    return 'Bienvenida/o a la app minimal!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "el nombre de la función decorada se utiliza para generar urls de manera automática, la orden `return` de la función será el mensaje mostrado en el navegador.\n",
    "\n",
    "Procedemos a crear un módulo con la aplicación generada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6
    ]
   },
   "outputs": [],
   "source": [
    "%%file app_minimal.py\n",
    "from flask import Flask \n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/') \n",
    "def bienvenida():\n",
    "    return 'Bienvenida/o a la app minimal!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar la aplicación generada se puede utilizar el comando `flask` desde la terminal (análogo a pip), sin embargo, antes de ejecutarla, es necesario indicarle a la terminal que aplicación se ejecutará con el comando `flask`. Esto corresponde a una variable del sistema llamada `FLASK_APP`, darle el valor que necesitamos utilizamos el comando `export`(linux - mac):\n",
    "\n",
    "```\n",
    "(entorno_virtual) user@ruta/a/ProjecLab$ export FLASK_APP=app_minimal.py\n",
    "```\n",
    "\n",
    "En windows se utiliza `C:\\ruta\\a\\ProjecLab>set FLASK_APP=app_minimal.py`. Luego de exportar la aplicación desde la terminal, se procede a lanzar la aplcación usando el comando:\n",
    "\n",
    "```\n",
    "(entorno_virtual) user@ruta/a/ProjecLab$ flask run\n",
    "```\n",
    "\n",
    "El ejecutar dicha orden se indica una url con la cual se accede a la aplicación, por lo general es del tipo `http://127.0.0.1:5000/`. Con esto, hemos utilizado el servidor interno de flask en nuestro computador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La aplicación anterior se mantiene funcionando como servidor de desarrollo pero requiere de un reinicio cada vez que se se haga un cambio al código. Para solucionar dicho problema existe el modo de depuración *debug mode*. Al activar este modo el servidor se recarga a si mismo al generar cambios en los módulos que lo componen. \n",
    "\n",
    "La activación del modo de depuración se hace por medio de la variable de sistema `FLASK_ENV` que se debe exportar como `development`.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se exporta el ambiente de depuración y se corre nuevamente la aplicación usando `flask run` todo desde la terminal.\n",
    "\n",
    "```\n",
    "(entorno_virtual) user@ruta/a/ProjecLab$ export FLASK_ENV=development\n",
    "(entorno_virtual) user@ruta/a/ProjecLab$ flask run\n",
    "```\n",
    "\n",
    "Con lo anterior, se activa el depurador, la carga automática y activa el modo de depuración en la aplicación creada. \n",
    "\n",
    "Aunque el depurardor permite la ejecución de código arbitrario, lo que se traduce en riesgos de seguridad, por tal motivo no debe ser utilizado en entornos de producción. \n",
    "\n",
    "Se cambia el texto de la aplicación y se recarga en el navegador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file app_minimal.py\n",
    "from flask import Flask \n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/') \n",
    "def bienvenida():\n",
    "    return 'Bienvenida/o a la app minimal! con depuracion'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Introduzca un error en la función anterior e identifiquelo a partir del depurador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ruteo de urls**\n",
    "\n",
    "Una aplicación web utiliza urls para acceder a sus funcionalidades y contenidos, esto además ayuda a los usuarios a comprender la estructura del sitio que se les presenta. Para manejar el acceso a urls dentro de la aplicación hacemos uso del decorador `@app.route()`.\n",
    "\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se construye una página de bienvenida una de saludo dentro de la aplicación `aplicación minimal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file app_minimal.py\n",
    "from flask import Flask \n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/') \n",
    "def bienvenida():\n",
    "    return 'Bienvenida/o a la app minimal!'\n",
    "\n",
    "@app.route('/hola') \n",
    "def funcion_saludo():\n",
    "    return 'Has accedido a la pagina de saludo. Hola ...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accedemos a la nueva función usando la url `127.0.0.1:5000/hola`.\n",
    "\n",
    "Como es posible observar, el decorador recibe un string indicando la url a la cual se asocia el procedimiento de una función. Este tipo de dato permite generar urls dinámicas utilizando **reglas variables**. \n",
    "\n",
    "Una regla variable permite aplicar funciones sobre urls dinámicas, para ello se utiliza la sintaxis `<variable>` entro de la url a la cual está asociada la función que deseamos operar. De tal manera, si creamos una función `func ` que recibe como argumento una la variable `id`  entonces se puede generar una regla variable con la url `/ruta/a/la/url/<id>` luego decoramos la función `func` con la url anterior.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se implementa el caso recientemente explorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file app_minimal.py\n",
    "from flask import Flask \n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/') \n",
    "def bienvenida():\n",
    "    return 'Bienvenida/o a la app minimal!'\n",
    "\n",
    "@app.route('/hola/<user>') \n",
    "def funcion_saludo(user):\n",
    "    return 'Has accedido a la pagina de saludo. Hola ...' + user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al acceder a la aplicación anterior, si entramos a la url `127.0.0.1:5000/hola/estudiante` recibimos la ejecución de `funcion_saludo('estudiante')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Se pueden indicar tipos de dato `string`,`int`,`float`,`path`,`uuid` mediante la notación `<dtype:varname>` donde `dtype` es el tipo de dato escogido para operar. Implemente una función que reciba un valor de punto flotante como input. ¿Qué diferencia hay entre `string` y `path`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flask opera sobre las urls entregadas de manera especial. Por ejemplo, al agregar las funciones `proyectos` y `acerca` estamos asociando las urls `'/proyectos/'` y `'/acerca'`. \n",
    "\n",
    "En el caso de`'/proyectos/'`, si accedemos a la url `127.0.0.1:5000/proyectos`, seremos redirigidos a `127.0.0.1:5000/proyectos/` es decir agregará el simbolo `/` al final de la url. \n",
    "\n",
    "Por el contrario, si accedemos a `127.0.0.1:5000/acerca/` no habrá redirección a la versión sin `/`. Es decir acceder a `127.0.0.1:5000/acerca/` lanza un error pero `127.0.0.1:5000/proyectos` no. Para entender este comportamiento, pensemos en las urls como rutas de acceso a carpetas (terminan con `/`) y archivos (terminan si `/`). Si intentamos a acceder a una carpeta con notación de archivo, seremos redirigidos a la carpeta, por otra parte, si intentamos acceder a un archivo con notación de carpeta habrá un error pues no existe una ruta con el nombre que buscamos. \n",
    "\n",
    "Se comprueba lo anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file app_minimal.py\n",
    "from flask import Flask \n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/') \n",
    "def bienvenida():\n",
    "    return 'Bienvenida/o a la app minimal!'\n",
    "\n",
    "@app.route('/hola/<user>') \n",
    "def funcion_saludo(user):\n",
    "    return 'Has accedido a la pagina de saludo. Hola ...' + user\n",
    "\n",
    "@app.route('/proyectos/')\n",
    "def proyectos():\n",
    "    return 'Pagina de proyectos.'\n",
    "\n",
    "@app.route('/acerca')\n",
    "def acerca():\n",
    "    return 'Pagina de informacion sobre el autor'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para construir urls a una función en especifico se puede utilizar la función `url_for()`, esta función toma como argumento el nombre de una función y un diccionario de argumentos pcional `**kwargs` asociado a la función. La utilidad de `url_for()` es permitir la construcción de urls dinámicas, en el sentido de que modificar múltiples rutas se hace más sencillo que cambiarlas manualmente una por una. \n",
    "\n",
    "La construcción de urls por medio de ese método hace un manejo automático de caracteres especiales, además las rutas generadas se construyen de manera absoluta, evitando comportamientos inesperados usuales con rutas relativas en navegadores. \n",
    "\n",
    "Si se tiene una aplicación ubicada fuera de la raiz de la url (ej: aplicación ubicada en `/aplicación` en vez de `/`),  `url_for` permite su manejo sencillo. \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se utiliza el context manager `text_request_context()` del módulo `app`. Este permite navegar por una aplicación de Flask desde Python, emulando accesos a funciones.  Se procede a obtener las urls para las funciones anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file app_minimal.py\n",
    "from flask import Flask, url_for\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def bienvenida():\n",
    "    return 'Bienvenida/o a la app minimal!'\n",
    "\n",
    "@app.route('/hola/Fulano')\n",
    "def funcion_saludo_0():\n",
    "    return 'SI ...'\n",
    "\n",
    "@app.route('/hola/<user>')\n",
    "def funcion_saludo(user,va):\n",
    "    return 'Has accedido a la pagina de saludo. Hola ...' + user + va\n",
    "\n",
    "@app.route('/proyectos/')\n",
    "def proyectos():\n",
    "    return 'Pagina de proyectos.'\n",
    "\n",
    "\n",
    "@app.route('/acerca')\n",
    "def acerca():\n",
    "    return 'Pagina de informacion sobre el autor'\n",
    "\n",
    "\n",
    "with app.test_request_context():\n",
    "    print('URL: Funcion bienvendia ->', url_for('bienvenida'))\n",
    "    print('URL: Funcion funcion_saludo ->',url_for('funcion_saludo_0'))\n",
    "    print('URL: Funcion funcion_saludo ->',url_for('funcion_saludo', user='Fulano'))\n",
    "    print('URL: Funcion acerca ->',url_for('acerca'))\n",
    "    print('URL: Funcion proyectos ->',url_for('proyectos'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las aplicaciones web utilizan distintos métodos HTTP (Hypertext Transfer Protocol) al momento de acceder a urls, estose se conocen tambien como verbos, dentro de los cuales se pueden reconocer `GET` consultar sobre un recurso especifico (solo reciben datos), `HEAD`, de la misma manera que `GET` solo recibe información pero acá se busca acceder al identificador de la información y no a su contenido total (cuerpo), `POST`, este método se utiliza para enviar recursos causando por lo genear un cambio de estado en el servidor y `PUT` que remplaza recursos objetivo.\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "1. Investigue sobre los verbos HTTP existentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por defecto, una ruta de Flask soo responde a consultas `GET`, sin embargo, el ecorador `app.route()` puede recibir el argumento `methods` con el cual se pueden especificar los tipos de métodos HTTP soportados por la función que decora.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Para utilizar el parámetro `methods` en el enrutamiento, importamos el objeto `requests` con el cual se puede identificar que tipo de consulta. \n",
    "\n",
    "Generaremos una página de `login` desde la cual un usuario podrá registrarse, haremos uso de la función `url_for()` y de la capacidad de Flask para manejar ordenes HTML. \n",
    "\n",
    "En primer lugar, creamos la página en la cual se hará el registro, esta consiste en un archivo HTML con la siguiente estructura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.getcwd()\n",
    "\n",
    "try:\n",
    "    os.mkdir('templates')\n",
    "\n",
    "except FileExistsError:\n",
    "    print('Directorio ya existe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un template de login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file templates/login.html \n",
    "<html>\n",
    "   <body>\n",
    "    \n",
    "      <form action = \"http://localhost:5000/login\" method = \"post\">\n",
    "         <p>Buenas, ingresa tu nombre:</p>\n",
    "         <p><input type = \"text\" name = \"nm\" /></p>\n",
    "         <p><input type = \"submit\" value = \"submit\" /></p>\n",
    "      </form>\n",
    "    \n",
    "   </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código anterior genera una página que consulta por la variable `name` generando una consulta `POST`. El texto ingresado en la página se almacena con el campo `nm` dentro de un diccionario asociado a la consulta `POST`. \n",
    "\n",
    "Añadiremos la función `exito` que recibe un nombre y le da la bienvenida, tambén implementamos la función de login, esta utiliza el objeto `request` para diferenciar que tipo de consulta se está realizando. Se decora dicha función utilizando el parámetro `methods`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file app_minimal.py\n",
    "from flask import Flask, redirect, url_for, request, render_template\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/') \n",
    "def bienvenida():\n",
    "    return render_template('login.html')\n",
    "\n",
    "@app.route('/hola/<user>') \n",
    "def funcion_saludo(user):\n",
    "    return 'Has accedido a la pagina de saludo. Hola ...' + user\n",
    "\n",
    "@app.route('/proyectos/')\n",
    "def proyectos():\n",
    "    return 'Pagina de proyectos.'\n",
    "\n",
    "@app.route('/acerca')\n",
    "def acerca():\n",
    "    return 'Pagina de informacion sobre el autor'\n",
    "\n",
    "#Exito\n",
    "@app.route('/exito/<name>') \n",
    "def exito(name):\n",
    "    return f'Bienvenido {name}'  \n",
    "\n",
    "#Login\n",
    "@app.route('/login', methods = ['POST', 'GET'])\n",
    "def login():\n",
    "    if request.method == 'POST':\n",
    "        user = request.form['nm'] \n",
    "        return  redirect(url_for('exito',name = user))\n",
    "    else:\n",
    "        user = request.args.get('nm')\n",
    "        return 'Login fallido'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo anterior utilizamos la función `render_template`. Con esta función es posible renderizar plantillas HTML directamente desde Flask. Para usar dicha funcionalidad  se requiere ubicar las plantillas a utilizar en una carpeta llamada `templates`, en este caso, si trabajamos con estructura de módulo, las carpetas deberían seguir el patrón:\n",
    "\n",
    "```\n",
    "|- /modulo.py\n",
    "|- /templates\n",
    "|-----/template_html_a_utilizar.html\n",
    "```\n",
    "\n",
    "En el caso de trabajar con librerías:\n",
    "\n",
    "```\n",
    "|- /libreria\n",
    "|----/__init__.py\n",
    "|----/templates\n",
    "|---------/template_html_a_utilizar.html\n",
    "```\n",
    "\n",
    "Para generar plantillas (*templates*) se puede utilizr [Jinga2](jinja.pocoo.org/docs/templates/). Esta librería consiste en un lenguaje de diseño diámico sencillo y rápido. Veamos un ejemplo de template Jinga2 y observemos como interactua con el entorno de Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '--Test--'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "<!doctype html>\n",
    "<title> En Flask esto se vería así: </title>\n",
    "\n",
    "{% if name %}\n",
    "    <h1> Variable inicializada {{name}} </h1>\n",
    "\n",
    "{% else %}\n",
    "    <h1> No hay variable inicializada!</h1>\n",
    "\n",
    "{% endif %}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que se tiene acceso a funcionalidades de Python mediante la sintaxis `{% %}`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, podemos utilizar una estructura de herencia de plantillas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file templates/layout.html \n",
    "\n",
    "<!doctype html>\n",
    "<html>\n",
    "  <head>\n",
    "    {% block head %}\n",
    "    <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css')}}\">\n",
    "    <title>{% block title %} {% endblock %} - App Minimal - </title>\n",
    "    {% endblock %}\n",
    "    \n",
    "  </head>\n",
    "\n",
    "  <body>\n",
    "    \n",
    "    <div id=\"content\">\n",
    "    {% block content %}\n",
    "    \n",
    "    {% endblock %}\n",
    "    </div>\n",
    "    \n",
    "    <div id=\"footer\">\n",
    "      {% block footer %}\n",
    "        Copyright 2020 by <a href=\"https://github.com/NicoCaro/DataScienceLab\"> DaScLab </a>.\n",
    "      {% endblock %}\n",
    "    </div>\n",
    "    \n",
    "  </body>\n",
    "\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código anterior genera una plantilla HTML utilizando Jinga2. Esta se constituye por bloques, los bloques tienen un nombre asociado y se comportan como atributos de un objeto. En este caso el template se denomina `layout.html` y se ubica en la carpeta `template`. Los bloques definidos son `head`, `content` y `footer`. \n",
    "\n",
    "Finalmente, dentro del template utilizamos la función de Python (Flask) `url_for('static', filename='style.css')` la cual accede a la url del componente `static`. Este componente viene por defecto en las aplicaciones de flask (no fue definido en `app_minimal.py`) y permite acceder a archivos ubicados en la carpeta `\\static`. Esta debe seguir la lógica de carpetas presentada en `templates`. En la carpeta `\\static` ubicamos un código de estilo `CSS` llamada `style.css`. Esta  cambia el color de los párrafos (`<p>`), headers (`<h1>`) y del cuerpo (`body`).\n",
    "\n",
    "Podemos pensar que `layout.html` se comporta como una clase, sobre la cual se puede hacer herencia simple. Para ello crearemos una reestructuración del template de `login` para que herede los atributos de `layout.html`. Observe que se puede hacer *overriding* de manera natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file templates/login.html\n",
    "{% extends \"layout.html\" %}\n",
    "\n",
    "{% block title %} Login {% endblock %}\n",
    "\n",
    "{% block head %}\n",
    "{{ super() }}\n",
    "Buenas, ingresa tu nombre:\n",
    "{% endblock %} \n",
    "                \n",
    "{% block content %}   \n",
    "{{ super() }}\n",
    " <form action = \"{{ url_for('login', filename='style.css')}}\", method = \"post\">\n",
    "         <p><input type = \"text\" name = \"nm\" /></p>\n",
    "         <p><input type = \"submit\" value = \"submit\" /></p>\n",
    "      </form>             \n",
    "{% endblock %}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La orden `super()` actua de la misma forma que en un esquema de herencia de objetos. \n",
    "\n",
    "Se construye un template para la página de redirección `extio.html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file templates/exito.html\n",
    "{% extends \"layout.html\" %}\n",
    "\n",
    "{% block title %} Exito! {% endblock %}\n",
    "\n",
    "{% block head %}\n",
    "{{ super() }}\n",
    "\n",
    "Login Exitoso\n",
    "\n",
    "{% endblock %} \n",
    "                \n",
    "{% block content %}  \n",
    "    {{ super() }}\n",
    "    Bienvenido {{name}}!\n",
    "{% endblock %}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente cambiamos los parámetros que permiten ejecutar nuestra aplicación sobre los templates creados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file app_minimal.py\n",
    "from flask import Flask, redirect, url_for, request, render_template\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/') \n",
    "def bienvenida():\n",
    "    return render_template('login.html') \n",
    "\n",
    "@app.route('/hola/<user>') \n",
    "def funcion_saludo(user):\n",
    "    return 'Has accedido a la pagina de saludo. Hola ...' + user\n",
    "\n",
    "@app.route('/proyectos/')\n",
    "def proyectos():\n",
    "    return 'Pagina de proyectos.'\n",
    "\n",
    "@app.route('/acerca')\n",
    "def acerca():\n",
    "    return 'Pagina de informacion sobre el autor'\n",
    "\n",
    "#Exito\n",
    "@app.route('/exito/<name>') \n",
    "def exito(name):\n",
    "    return render_template('exito.html', name = name)   \n",
    "\n",
    "#Login\n",
    "@app.route('/login',methods = ['POST', 'GET'])\n",
    "def login():\n",
    "    if request.method == 'POST':\n",
    "        user = request.form['nm'] \n",
    "        return  redirect(url_for('exito',name = user))\n",
    "    else:\n",
    "        user = request.args.get('nm')\n",
    "        return redirect(url_for('exito',name = user))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interludio: Flujo de trabajo \n",
    "\n",
    "El flujo de trabajo de un proyecto de ciencia de datos puede dividirse según ciertos entornos que caracterizan las etapas de desarrollo. \n",
    "\n",
    "El primer paso es comenzar un *entorno de investigación* donde se hace el prototipado del proyecto de ciencia de datos. En este entorno se producen los bloques iniciales de carga de datos, analisis exploratorio y limpieza, ingenieria de características, entrenamiento, validación y finalmente evaluación del modelo final. \n",
    "\n",
    "Al entorno anterior le sigue una etapa de *desarrollo* o *entorno de desarrollo* en el cual se generan los modulos y librerías necesarias para poner en marcha el proyecto. \n",
    "\n",
    "Finalmente, se pasa al *entorno de producción* donde se tiene un proyecto accesible, robusto, controlable y reproducible al que tendrá acceso el usuario final. Sobre esta última etapa se busca construir actualizaciones que mejoren el funcionamiento, ya sea en cuanto a las dependiencias de software como tambien en relación a las dependencias con las fuentes de datos. \n",
    "\n",
    "**Ejemplo - Entorno de Investigación**\n",
    "\n",
    "Se simula un entorno sencillo de investigación, haremos un seguimiento desde su formulación hasta su puesta en producción. \n",
    "\n",
    "El conjunto de datos a trabajar consiste en la base de datos del Titanic, esta base entrega información sobre los pasajeros a bordo del barco homónimo, indicando si estos sobrevivieron al accidente en el que se vieron envueltos. \n",
    "\n",
    "Procedemos a cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6202)\n",
    "\n",
    "train = pd.read_csv('ProjectLab/data/train.csv') \n",
    "test = pd.read_csv('ProjectLab/data/test.csv')\n",
    "\n",
    "data = pd.concat((train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es hacer una exploración inicial del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver una variedad de tipos de datos, además de valores faltantes, las variables disponibles consisten en:\n",
    "\n",
    "- PassengerId: Identificador por pasajero.\n",
    "- Survived: Variable de respuesta booleana\n",
    "- Pclass: Clase asociada al boleto.\n",
    "- Name: Nombre del pasajero.\n",
    "- Sex: Sexo de pasajero.\n",
    "- Age: Edad.\n",
    "- Sibsp: Número de hermanos o parejas viajando en compañia.\n",
    "- Parch: Numero de padres o hijos viajando en compañia.\n",
    "- Ticket: Identificador del boleto.\n",
    "- Fare: Costo del boleto.\n",
    "- Cabin: Numero de cabina asociada al pasajero.\n",
    "- Embarked: Lugar de embarcación del pasajero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que en promedio el $38\\%$ de los pasajeros sobrevivieron el accidente. \n",
    "\n",
    "En cuanto a los valores faltantes se tiene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es una exploración visual del conjunto de datos, para ello, se comienza por estudiar las correlaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "corr = corr = train.corr()\n",
    "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien es una exploración inicial, se aprecia una correlación entre `Pclass`, `Fare` y la variable de respuesta. Se estudia la distribución de la variable de respuesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,5))\n",
    "\n",
    "sns.countplot(x='Survived', data = train)\n",
    "print(train['Survived'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estudiamos la la variable de respuesta según `Pclass`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,10))\n",
    "\n",
    "ax1 = plt.subplot(2,1,1)\n",
    "ax1 = sns.countplot(x = 'Pclass', hue = 'Survived', data = train)\n",
    "ax1.set_title('Supervivencia según Clase', fontsize = 20)\n",
    "ax1.set_xticklabels(['1 Alta','2 Media','3 Baja'], fontsize = 15)\n",
    "ax1.set_ylim(0,400)\n",
    "ax1.set_xlabel('Clase',fontsize = 15) \n",
    "ax1.set_ylabel('Conteo',fontsize = 15)\n",
    "ax1.legend(['No','Si'])\n",
    "\n",
    "# Pointplot Pclass type\n",
    "ax2 = plt.subplot(2,1,2)\n",
    "sns.pointplot(x='Pclass', y='Survived', data=train)\n",
    "ax2.set_xlabel('Clase',fontsize = 15)\n",
    "ax2.set_ylabel('% Sobrevive',fontsize = 15)\n",
    "ax2.set_title('Porcentaje de supervivencia por Clase', fontsize = 20);\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo anterior confirma la correlación observada, mientras mayor la clase del boleto, más probable es sobrevivir. \n",
    "\n",
    "En cuanto a la edad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survived = 'Sobrevive'\n",
    "not_survived = 'No sobrevive'\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(12, 6))\n",
    "\n",
    "women = train[train['Sex']=='female']\n",
    "men = train[train['Sex']=='male']\n",
    "\n",
    "ax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=20, label = survived, ax = axes[0], kde =False)\n",
    "ax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=20, label = not_survived, ax = axes[0], kde =False)\n",
    "\n",
    "ax.legend(fontsize=12) \n",
    "\n",
    "ax.set_title('Mujer', fontsize=20)\n",
    "ax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=20, label = survived, ax = axes[1], kde = False)\n",
    "ax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=20, label = not_survived, ax = axes[1], kde = False)\n",
    "\n",
    "ax.legend(fontsize=12)\n",
    "ax.set_title('Hombre', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se tiende que en general las mujeres tuvieron un más alto porcentaje de supervivencia sin importar la edad. En ambos sexos, las probabilidaddes de sobrevivir tienden a acumularse en la juventud. \n",
    "\n",
    "Aunque es posible continuar analizando relaciones visuales en el conjunto de datos, pasaremos a la *ingeniería de características*. \n",
    "\n",
    "Se crea la variable **family survival** que busca encapsular relaciones de supervivencia de familias. De esta forma, se genera una relación de similitud basándose en la información contenida en el boleto de cada pasajero. Así, grupos similares tendrán una probabilidad similar de supervivencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se llena el valor Fare con la media total\n",
    "\n",
    "def na_mean_fill(df, col='Fare'):\n",
    "    '''Cambia los datos faltantes de la columna col por la media en df.'''\n",
    "\n",
    "    data = df.copy()\n",
    "    data[col].fillna(data[col].mean(), inplace = True)\n",
    "    return data \n",
    "\n",
    "data = na_mean_fill(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a trabajar con la variable `Family_Survival`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se extrae el apellido\n",
    "def last_name_gen(df):\n",
    "    '''Genera la columna Last_name a partir de Name en el DataFrame df.\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    data: Pandas.DataFrame\n",
    "        Un conjunto de datos con la variable Last_Name agregada.\n",
    "    '''\n",
    "    \n",
    "    data = df.copy()\n",
    "    data['Last_Name'] = data['Name'].apply(lambda x: str.split(x, \",\")[0])\n",
    "\n",
    "    return data\n",
    "\n",
    "data = last_name_gen(data)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se genera una rutina para crear la variable`Family_Survival`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def family_dict_gen(df, default_survival_chance=0.5):\n",
    "    '''Genera un DataFrame con probabilidades de supervivencia por familia.\n",
    "    \n",
    "    El conjunto de datos debe tener las columnas: Survived, Name, Last_Name, \n",
    "    Fare, Ticket, PassengerId, SibSp, Parch, Age y Cabin. Se genera un  \n",
    "    DataFrame con la relación familia -> fecuencia de supervivenca.\n",
    "\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "\n",
    "    data: Pandas.DataFrame\n",
    "        Un conjunto con la relacion familia - frecuencia de supervivencia\n",
    "    '''\n",
    "\n",
    "    data = df.copy()\n",
    "\n",
    "    # Utiliza un valor prior para la probabilidad de sobrevivir\n",
    "    data['Family_Survival'] = default_survival_chance\n",
    "\n",
    "    # Se agrupa el conjunto de datos por apellido y fare\n",
    "    for grp, grp_df in data[['Survived', 'Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n",
    "                             'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n",
    "\n",
    "        # Si no es igual a 1, se encuentra una familia\n",
    "        # en tal caso se calcula una probabilidad de supervivencia\n",
    "        if (len(grp_df) != 1):\n",
    "            for ind, row in grp_df.iterrows():\n",
    "                smax = grp_df.drop(ind)['Survived'].max()\n",
    "                smin = grp_df.drop(ind)['Survived'].min()\n",
    "                passID = row['PassengerId']\n",
    "                if (smax == 1.0):\n",
    "                    data.loc[data['PassengerId'] ==\n",
    "                             passID, 'Family_Survival'] = 1\n",
    "                elif (smin == 0.0):\n",
    "                    data.loc[data['PassengerId'] ==\n",
    "                             passID, 'Family_Survival'] = 0\n",
    "\n",
    "    # Luego se agrupa la informacion por tickets y se procede\n",
    "    for _, grp_df in data.groupby('Ticket'):\n",
    "        if (len(grp_df) != 1):\n",
    "            for ind, row in grp_df.iterrows():\n",
    "                if (row['Family_Survival'] == 0) | (row['Family_Survival'] == 0.5):\n",
    "                    smax = grp_df.drop(ind)['Survived'].max()\n",
    "                    smin = grp_df.drop(ind)['Survived'].min()\n",
    "                    passID = row['PassengerId']\n",
    "                    if (smax == 1.0):\n",
    "                        data.loc[data['PassengerId'] ==\n",
    "                                 passID, 'Family_Survival'] = 1\n",
    "                    elif (smin == 0.0):\n",
    "                        data.loc[data['PassengerId'] ==\n",
    "                                 passID, 'Family_Survival'] = 0\n",
    "\n",
    "    return data.groupby('Last_Name').mean()['Family_Survival']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_dict = family_dict_gen(data) \n",
    "family_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finalmente se crea la variable `Family_Survival`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def family_survival_gen(df, family_dict=family_dict, default_prob=0.5):\n",
    "    '''Recibe observaciones y asigna probabilidad de supervivencia por familia.\n",
    "\n",
    "    Permite generar una variable donde se asigna una proporcion de sobrevivencia\n",
    "    por familia. Para ello toma un DataFrame con las relaciones por familia si \n",
    "    el apellido asociado a la observacion x en df esta en el conjunto  de \n",
    "    familias, le asigna el valor indicado en family_dict. En caso contrario \n",
    "    le asigna la probabilidad default_prob. \n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    data : Pandas.DataFrame \n",
    "        Conjunto de datos con la variable family_survival generada. \n",
    "    '''\n",
    "    \n",
    "    data = df.copy()\n",
    "    data['Family_Survival'] = default_prob\n",
    "\n",
    "    for x in data.itertuples():\n",
    "        if x.Last_Name in family_dict:\n",
    "            prob = family_dict.loc[x.Last_Name]\n",
    "            data.loc[data['PassengerId'] == x.PassengerId,'Family_Survival'] = prob\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = family_survival_gen(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pasajeros con informacion de familia:\",\n",
    "      data.loc[data['Family_Survival'] != 0.5].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se reinicia el conjunto de índices para continuar trabajando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)\n",
    "data = data.drop('Survived', axis=1)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se trabaja la variable `Fare`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig  = plt.plot(figsize = [12,12])  \n",
    "plt.hist(data['Fare'], bins=40)\n",
    "\n",
    "plt.xlabel('Fare', fontdict={'fontsize':15})\n",
    "plt.ylabel('Conteo', fontdict={'fontsize':15})\n",
    "\n",
    "plt.title('Distribucion de Precio',  fontdict={'fontsize':20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribución presenta asimetria estadística, dentro de las opciones que se tienen para trabajar con este tipo de dato, se encuentran las transformaciones de potencia, o la categorización. En este caso, se procede a categorizar la variable en 4 niveles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_to_cat(df,col = 'Fare' , cuts = 4):\n",
    "    '''Toma un conjunto de datos df y transfroma la columna col en categorica.\n",
    "    \n",
    "    La cantidad de bins viene dada por el parametro cuts.\n",
    "    \n",
    "    Retorna:\n",
    "    -------- \n",
    "    \n",
    "    data: Pandas.DataFrame\n",
    "        Un conjunto de datos con la variable col recategorizada. \n",
    "    '''\n",
    "    lbl = LabelEncoder() \n",
    "    data = df.copy()\n",
    "    \n",
    "    data[col] = pd.qcut(data[col], cuts) \n",
    "    data[col] = lbl.fit_transform(data[col]) \n",
    "    \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = col_to_cat(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a observar el resultado de la categorización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig  = plt.plot(figsize = [12,12])  \n",
    "\n",
    "sns.countplot(data['Fare'])\n",
    "plt.xlabel('Fare Bin',fontdict={'fontsize':15})\n",
    "plt.ylabel('Conteo',fontdict={'fontsize':15})\n",
    "plt.title('Fare Bins',fontdict={'fontsize':20});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede trabajar la variable `Name`. En este caso, los nombres no como identificadores no entregan información, sin embargo, los nombres de esta base continen expresiones del tipo `Mr.` o `Mrs.` que pueden ayudar a categorizar entre hombres y mujeres. \n",
    "\n",
    "Se procede a extraer dicha información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(name):\n",
    "    '''Obtiene le titulo asociado a una persona.'''\n",
    "    \n",
    "    if '.' in name:\n",
    "        return name.split(',')[1].split('.')[0].strip()\n",
    "    else:\n",
    "        return 'desconocido' \n",
    "\n",
    "def get_title_list(df): \n",
    "    '''Dado un conjunto de datos obtiene titulos a partir de la columna Name.\n",
    "    \n",
    "    Retorna:\n",
    "    -------- \n",
    "    \n",
    "    title_data: list \n",
    "        Lista con los titulos encontrados en df.\n",
    "        \n",
    "    '''\n",
    "    data = df.copy() \n",
    "    titles_data = sorted(set([x for x in data['Name'].map(lambda x: get_title(x))]))\n",
    "    \n",
    "    return titles_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan los titulos obtenidos del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_data = get_title_list(data)\n",
    "titles_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se observan 18 valores únicos, se procede a reducir la dimensionalidad de la categoría reduciendo solo a `Mr`,`Mrs`, `Miss` y `Master`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_title(x):\n",
    "    '''Reduce el titulo a Mr,Mrs o Miss segun corresponda.'''\n",
    "    \n",
    "    title = x['Title']\n",
    "    if title in ['Capt', 'Col', 'Don', 'Jonkheer', 'Major', 'Rev', 'Sir']:\n",
    "        return 'Mr'\n",
    "    elif title in ['the Countess', 'Mme', 'Lady','Dona']:\n",
    "        return 'Mrs'\n",
    "    elif title in ['Mlle', 'Ms']:\n",
    "        return 'Miss'\n",
    "    elif title =='Dr':\n",
    "        if x['Sex']=='male':\n",
    "            return 'Mr'\n",
    "        else:\n",
    "            return 'Mrs'\n",
    "    else:\n",
    "        return title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplica la transformación anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_gen(df):\n",
    "    ''' Genera la columna Title a partir de Name en el DataFrame df.\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    data: Pandas.DataFrame\n",
    "        Un conjunto de datos con la variable Title agregada.\n",
    "    ''' \n",
    "    \n",
    "    data = df.copy() \n",
    "    data['Title'] = data['Name'].map(lambda x: get_title(x))\n",
    "    data['Title'] = data.apply(set_title, axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa la distribución según nueva categoría, la categría `Master` no es transformada por la función anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = title_gen(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Title'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a la variable `Age` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Informacion faltante: ', pd.isnull(data['Age']).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se utiliza imputación de datos por grupo, el grupo corresponde al titulo asociado anteriormente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_imputer(df): \n",
    "    ''' Llena la información faltante de la columna Age en df. \n",
    "    \n",
    "    Utiliza una agrupacion por titulo y aplica un llenado por mediana de grupo.\n",
    "    \n",
    "    Retorna:\n",
    "    -------- \n",
    "    \n",
    "    data: Pandas.DataFrame \n",
    "        Conjunto de datos con la variable Age completada.\n",
    "    '''\n",
    "    data = df.copy()\n",
    "    data['Age'] = data.groupby('Title')['Age'].apply(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = age_imputer(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lo anterior, se llenan los datos faltantes por grupo utilizando la mediana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = [7,7])\n",
    "\n",
    "plt.hist(data['Age'], bins=40)\n",
    "plt.xlabel('Age',fontdict={'fontsize':15})\n",
    "plt.ylabel('Conteo',fontdict={'fontsize':15})\n",
    "plt.title('Distribucion de Edades',fontdict={'fontsize':20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siguiendo la idea utilizada en `Fare` se usan 4 categorias para representar las edades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = col_to_cat(data, col='Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lo que se obtiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[7,7])\n",
    "plt.xticks(rotation='90')\n",
    "sns.countplot(data['Age'])\n",
    "\n",
    "plt.xlabel('Bins de Edad',fontdict={'fontsize':15})\n",
    "plt.ylabel('Conteo',fontdict={'fontsize':15})\n",
    "plt.title('Age Bins',fontdict={'fontsize':20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a transformar los titulos a valores ordinales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odc = OrdinalEncoder() \n",
    "data['Title'] = odc.fit_transform(data['Title'].values.reshape([-1,1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a la variable `Sex`, simplemente se hace una codificación *one-hot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False)\n",
    "data['Sex'] = ohe.fit_transform(data['Sex'].values.reshape([-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos de `Embarked` se imputan usando la moda y se codifican de manera ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_fill(df, col): \n",
    "    '''Llena los valores faltantes de col en df usando la moda global.\n",
    "    \n",
    "    Retorna: \n",
    "    -------- \n",
    "    \n",
    "    data: Pandas.DataFrame \n",
    "        Conjunto de datos con la informacion de col completada. \n",
    "    ''' \n",
    "    \n",
    "    data = df.copy() \n",
    "    data[col] = data[col].fillna(data[col].mode()[0])  \n",
    "    \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mode_fill(data,col = 'Embarked' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ode_embarked =  OrdinalEncoder() \n",
    "data['Embarked'] = ode_embarked.fit_transform(data['Embarked'].values.reshape([-1,1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `Cabin` está formateada según letras numeros que indican el piso del barco (letra) y número de cabina en tal piso (número). Es posible que exista alguna relación entre el piso de la cabina y la posibilidad de sobrevivir. Se trabajan los valores de esta columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Cabin'].head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, se cambian los valores perdidos por `desconocido`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_desconocido(df, col):\n",
    "    ''' Toma un conjunto de datos df y llena la columna col con el valor\n",
    "    'desconocido'.\n",
    "    \n",
    "    Retorna:\n",
    "    -------- \n",
    "    \n",
    "    data : Pandas.DataFrame\n",
    "        Conjunto de datos con la variable col completada. \n",
    "\n",
    "    '''\n",
    "    data = df.copy()\n",
    "    data[col].fillna('desconocido', inplace = True) \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fill_desconocido(data, 'Cabin') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se extrae la letra de cada cabina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Cabin'].map(lambda x: x[0]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las cabinas con letra `d` asociada poseen información faltante y sobrepasan a las demás categorías, basándose en esto, se construyen 2 nuevos grupos, aquellos con información en su cabina y aquellos sin información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown_cabin(cabin):\n",
    "    if cabin != 'd':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cabin_cat(df):\n",
    "    '''Genera dos categorias en df basandose en la columna Cabin.\n",
    "    \n",
    "    Retorna:\n",
    "    -------\n",
    "    \n",
    "    data : Pandas.DataFrame \n",
    "        Conjunto de datos con la categorizacion creada. \n",
    "    '''\n",
    "    \n",
    "    data = df.copy() \n",
    "    data['Cabin'] = data['Cabin'].map(lambda x: x[0])\n",
    "    data['Cabin'] = data['Cabin'].apply(lambda x: unknown_cabin(x))\n",
    "\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cabin_cat(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables `SibSp` y `Parch` se combinan en `Family Size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def family_size_gen(df): \n",
    "    '''Genera la columna Family_Size a partir de SibSp y Parch en df. \n",
    "    \n",
    "    Retorna:\n",
    "    ------- \n",
    "    data : Pandas.DataFrame \n",
    "        Conjunto de datos con la columna creada. \n",
    "    \n",
    "    '''\n",
    "    data = df.copy()\n",
    "    data['Family_Size'] = data['SibSp'] + data['Parch']\n",
    "    \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = family_size_gen(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables que no serán utilizadas son eliminadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['Name', 'Parch', 'SibSp', 'Ticket', 'Last_Name', 'PassengerId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(to_drop, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez explorado el conjunto de datos, haciendo la ingeniería de características necesaria, se procede a implementar un conjunto de algoritmos de aprendizaje de máquinas. En este contexto, se procede a empaquetar los métodos de preprocesamiento para incluirlos en una pipeline de entrenamiento. \n",
    "\n",
    "A los métodos anteriores añadimos un preprocesador por estandarización, además se empaquetan los preprocesadores de codificación ordinal y one hot. Para finalizar se crea una función que encapsula la eliminación de columnas no deseadas y otra que permite estandarizar un DataFrame manteniendo la estructura de columnas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oh_encoder_col(df, col='Sex'):\n",
    "    '''Genera codificacion One hot en df basandose en la columna col.\n",
    "\n",
    "    Retorna:\n",
    "    -------\n",
    "\n",
    "    data : Pandas.DataFrame\n",
    "        Conjunto de datos con el encoding creado.\n",
    "    '''\n",
    "    data = df.copy()\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    data[col] = ohe.fit_transform(data[col].values.reshape([-1, 1]))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def ord_encoder_col(df, col='Title'):\n",
    "    '''Genera codificacion Ordinal df basandose en la columna col.\n",
    "\n",
    "    Retorna:\n",
    "    -------\n",
    "\n",
    "    data : Pandas.DataFrame \n",
    "        Conjunto de datos con el encoding creado. \n",
    "    '''\n",
    "\n",
    "    data = df.copy()\n",
    "    ode = OrdinalEncoder()\n",
    "    data[col] = ode.fit_transform(data[col].values.reshape([-1, 1]))\n",
    "\n",
    "    return data\n",
    "\n",
    "to_drop = ['Name', 'Parch', 'SibSp', 'Ticket', 'Last_Name', 'PassengerId']\n",
    "def drop_cols(df, cols = to_drop):\n",
    "    '''Elimina las columans cols del DataFrame df.\n",
    "    \n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    data : Pandas.DataFrame \n",
    "        Conjunto de datos con las columnas borradas. \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    data = df.copy() \n",
    "    data.drop(cols, axis = 1, inplace=True)\n",
    "    \n",
    "    return data \n",
    "\n",
    "def std_scaler(df):\n",
    "    '''Toma un DataFrame y lo estandariza, retorna un DataFrame.'''\n",
    "    \n",
    "    std_scaler = StandardScaler()\n",
    "    \n",
    "    data = df.copy()\n",
    "    cols = data.columns\n",
    "    \n",
    "    return pd.DataFrame(std_scaler.fit_transform(data),columns = cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea la Pipeline asociada al proceso de exploración y se preprocesa el conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer as F\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "\n",
    "data = pd.read_csv('ProjectLab/data/train.csv')\n",
    "test_data = data.sample(frac=0.1, random_state = 5)\n",
    "\n",
    "train_idx = data.index.difference(test_data.index) \n",
    "family_dict = family_dict_gen(last_name_gen(data.loc[train_idx]))\n",
    "\n",
    "y_train = data.loc[train_idx,'Survived'] \n",
    "x_train = data.drop('Survived', axis=1).loc[train_idx]\n",
    "\n",
    "steps = [('Fare mean fill', F(na_mean_fill)),\n",
    "         ('Last_Name gen', F(last_name_gen)),\n",
    "         ('Family_Survival gen', F(lambda df: family_survival_gen(df,family_dict=family_dict))), \n",
    "         ('Fare to cat', F(col_to_cat)),\n",
    "         ('Title var gen', F(title_gen)),\n",
    "         ('Age imputer', F(age_imputer)),\n",
    "         ('Age to cat', F(lambda df: col_to_cat(df, col='Age'))),\n",
    "         ('Title to ordinal', F(ord_encoder_col)),\n",
    "         ('Sex to One Hot Enc', F(oh_encoder_col)),\n",
    "         ('Fill with the mean Embarked', F(lambda df: mode_fill(df, col='Embarked'))),\n",
    "         ('Embarked to Ordinal', F(lambda df: ord_encoder_col(df, col='Embarked'))),\n",
    "         ('Fill desconocido Cabin', F(lambda df: fill_desconocido(df, col='Cabin'))),\n",
    "         ('Cabin to Cat', F(cabin_cat)),\n",
    "         ('Family size gen', F(family_size_gen)),\n",
    "         ('drop cols', F(drop_cols)),\n",
    "         ('Scaler', F(std_scaler))\n",
    "         ]\n",
    "\n",
    "\n",
    "data_prep = Pipeline(steps=steps)\n",
    "x_train = data_prep.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se procede a entrenar modelos de Aprendizaje automático sobre el conjunto de datos, los modelos seleccionados corresponden a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "ran = RandomForestClassifier(random_state=1)\n",
    "knn = KNeighborsClassifier()\n",
    "log = LogisticRegression()\n",
    "xgb = XGBClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "svc = SVC(probability=True)\n",
    "ext = ExtraTreesClassifier()\n",
    "ada = AdaBoostClassifier()\n",
    "gnb = GaussianNB()\n",
    "gpc = GaussianProcessClassifier()\n",
    "bag = BaggingClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos modelos son evaluados usando un esquema de validación cruzada, se procede a almacenar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import model_selection\n",
    "\n",
    "models = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]         \n",
    "scores = []\n",
    "\n",
    "for mod in models:\n",
    "    mod.fit(x_train, y_train)\n",
    "    acc = cross_val_score(mod, x_train, y_train, scoring = \"accuracy\", cv = 10)\n",
    "    scores.append(acc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados obtenidos corresponden a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'K Nearest Neighbour', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier'],\n",
    "    'Score': scores})\n",
    "\n",
    "result_df = results.sort_values(by='Score', ascending=False).reset_index(drop=True)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = [9,7])   \n",
    "\n",
    "sns.barplot(x='Score', y = 'Model', data = result_df, color = 'c')\n",
    "\n",
    "plt.title('Accuracy por modelo  \\n', fontsize = 20)  \n",
    "plt.xlabel('Accuracy  (%)', fontsize = 15)\n",
    "plt.ylabel('Algoritmo', fontsize = 15)\n",
    "plt.xlim(0.80, 0.87);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede apreciar que el algoritmo con mejor rendimiento es el clasificador `SVC`. El modelo XGBoost permite obtener un indicador de las caracteristicas más importantes dentro del proceso de predicción, podemos acceder a estos puntajes por medio de "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_plotting(data, x, y, palette, title):\n",
    "    \n",
    "    sns.set(style=\"whitegrid\")\n",
    "    ft = sns.PairGrid(data, y_vars=y, x_vars=x, height=5, aspect=1.5) \n",
    "    ft.map(sns.stripplot, orient='h', palette=palette,\n",
    "           edgecolor=\"black\", size=10)\n",
    "\n",
    "    for ax, title in zip(ft.axes.flat, titles):\n",
    "        ax.set_title(title, fontsize = 18)\n",
    "        ax.xaxis.grid(False) \n",
    "        ax.yaxis.grid(True)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "fi = {'Features': x_train.columns.tolist(), 'Importance': xgb.feature_importances_}\n",
    "importance = pd.DataFrame(fi, index=None).sort_values(\n",
    "    'Importance', ascending=False)\n",
    "\n",
    "titles = ['Importancia de Caracterísitcas en Prediccion según XGBoost'] \n",
    "importance_plotting(importance, 'Importance', 'Features', 'Reds_r', titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, el sexo del pasajero es la característica más influyente bajo este modelo, de la misma forma, se encuentra una relación entre el parentezco dentro de los pasajeros por medio de `Family_Survival` y la cabina en la cual se viaja. \n",
    "\n",
    "En cuanto a las relaciones lineales se pueden observar los coeficientes de la regresión logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = {'Features':x_train.columns.tolist(), 'Importance':np.transpose(log.coef_[0])}\n",
    "importance = pd.DataFrame(fi, index=None).sort_values('Importance', ascending=False)\n",
    "\n",
    "titles = ['Importancia de Caracterísitcas en Prediccion según LogReg']\n",
    "importance_plotting(importance, 'Importance', 'Features', 'Reds_r', titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa cierta concordancia con las importancias anteriores, en este caso aparece `Cabin` y `Fare` Como indicadores lineales de supervivencia sumados a `Family_Survival` y a `Sex`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basándonos en lo anterior se efectua un proceso de **selección de características**, para ello se construye una tabla con la información por modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting feature importances for the 5 models where we can\n",
    "gbc_imp = pd.DataFrame({'Feature': x_train.columns,\n",
    "                        'gbc importance': gbc.feature_importances_})\n",
    "xgb_imp = pd.DataFrame(\n",
    "    {'Feature': x_train.columns, 'xgb importance': xgb.feature_importances_})\n",
    "ran_imp = pd.DataFrame(\n",
    "    {'Feature': x_train.columns, 'ran importance': ran.feature_importances_})\n",
    "ext_imp = pd.DataFrame(\n",
    "    {'Feature': x_train.columns, 'ext importance': ext.feature_importances_})\n",
    "ada_imp = pd.DataFrame(\n",
    "    {'Feature': x_train.columns, 'ada importance': ada.feature_importances_})\n",
    "\n",
    "# Merging results into a single dataframe\n",
    "importances = gbc_imp.merge(xgb_imp, on='Feature').merge(\n",
    "    ran_imp, on='Feature').merge(ext_imp, on='Feature').merge(ada_imp, on='Feature')\n",
    "\n",
    "importances['Average'] = importances.mean(axis=1)\n",
    "\n",
    "\n",
    "importances = importances.sort_values(\n",
    "    by='Average', ascending=False).reset_index(drop=True)\n",
    "\n",
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando esta información podemos tener una idea de como las carácteristicas influyen en el promedio de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = {'Features':importances['Feature'], 'Importance':importances['Average']}\n",
    "\n",
    "importance = pd.DataFrame(fi, index=None).sort_values('Importance', ascending=False)\n",
    "titles = ['Importancia de Features Promedio']\n",
    "\n",
    "# Plotting graph\n",
    "importance_plotting(importance, 'Importance', 'Features', 'Reds_r', titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las columnas asociadas a `Embarked` y `Cabin` no aportan mucho en general por lo que se seleccionan como variables a eliminar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop.extend(['Embarked', 'Cabin'])\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = drop_cols(x_train, cols=['Embarked', 'Cabin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizada la selección de características, se reentrenan los modelos, se actualiza la Pipeline para generar datos sin las características que deseamos exlcuir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('ProjectLab/data/train.csv')\n",
    "test_data = data.sample(frac=0.1, random_state = 5)\n",
    "\n",
    "train_idx = data.index.difference(test_data.index) \n",
    "family_dict = family_dict_gen(last_name_gen(data.loc[train_idx]))\n",
    "\n",
    "y_train = data.loc[train_idx,'Survived'] \n",
    "x_train = data.drop('Survived', axis=1).loc[train_idx]\n",
    "\n",
    "# Actualiza la pipeline eliminando las nuevas caracteristicas\n",
    "steps = [('Fare mean fill', F(na_mean_fill)),\n",
    "         ('Last_Name gen', F(last_name_gen)),\n",
    "         ('Family_Survival gen', F(lambda df: family_survival_gen(df,family_dict=family_dict))), \n",
    "         ('Fare to cat', F(col_to_cat)),\n",
    "         ('Title var gen', F(title_gen)),\n",
    "         ('Age imputer', F(age_imputer)),\n",
    "         ('Age to cat', F(lambda df: col_to_cat(df, col='Age'))),\n",
    "         ('Title to ordinal', F(ord_encoder_col)),\n",
    "         ('Sex to One Hot Enc', F(oh_encoder_col)),\n",
    "         ('Fill with the mean Embarked', F(lambda df: mode_fill(df, col='Embarked'))),\n",
    "         ('Embarked to Ordinal', F(lambda df: ord_encoder_col(df, col='Embarked'))),\n",
    "         ('Fill desconocido Cabin', F(lambda df: fill_desconocido(df, col='Cabin'))),\n",
    "         ('Cabin to Cat', F(cabin_cat)),\n",
    "         ('Family size gen', F(family_size_gen)),\n",
    "         ('drop cols', F(lambda df: drop_cols(df, cols=to_drop))),\n",
    "         ('Scaler', F(std_scaler))\n",
    "         ]\n",
    "\n",
    "data_prep = Pipeline(steps=steps)\n",
    "\n",
    "def make_mod_pipe(mod): return Pipeline(\n",
    "    steps=[('preprocess', data_prep), ('clf', mod)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a reentrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]         \n",
    "scores_v2 = []\n",
    "\n",
    "for mod in models:\n",
    "    mod_pipe = make_mod_pipe(mod) \n",
    "    \n",
    "    mod_pipe.fit(x_train, y_train)\n",
    "    \n",
    "    acc = cross_val_score(mod_pipe, x_train, y_train, scoring = \"accuracy\", cv = 10, n_jobs = -1)\n",
    "    scores_v2.append(acc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtienen los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'K Nearest Neighbour', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier'],\n",
    "    'Score original': scores,\n",
    "    'Score seleccion': scores_v2})\n",
    "\n",
    "result_df = results.sort_values(\n",
    "    by='Score seleccion', ascending=False).reset_index(drop=True)\n",
    "result_df['Diferencia'] = result_df['Score seleccion'] - \\\n",
    "    result_df['Score original']\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[9, 7])\n",
    "\n",
    "sns.barplot(x='Score seleccion', y='Model',\n",
    "            data=result_df, color='c', label='seleccion')\n",
    "sns.barplot(x='Score original', y='Model',\n",
    "            data=result_df, color='r', label='original')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.title('Accuracy por Modelo \\n', fontsize=20)\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.ylabel('Algoritmo')\n",
    "plt.xlim(0.80, 0.87)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar un aumento global al aplicar selección de características en los modelos. El siguiente paso es la **obtención de hiperparámetros**. \n",
    "\n",
    "En este caso generamos un conjunto de hipeparámetros sobre los cuales se hará una búsqueda por grilla según validación cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actualiza la pipeline eliminando fuga en family gen\n",
    "def step_gen(model):\n",
    "    '''Genera pasos para predicir usando model.'''\n",
    "\n",
    "    steps = [('Fare mean fill', F(na_mean_fill)),\n",
    "             ('Last_Name gen', F(last_name_gen)),\n",
    "             ('Family_Survival gen', F(family_survival_gen)),\n",
    "             ('Fare to cat', F(col_to_cat)),\n",
    "             ('Title var gen', F(title_gen)),\n",
    "             ('Age imputer', F(age_imputer)),\n",
    "             ('Age to cat', F(lambda df: col_to_cat(df, col='Age'))),\n",
    "             ('Title to ordinal', F(ord_encoder_col)),\n",
    "             ('Sex to One Hot Enc', F(oh_encoder_col)),\n",
    "             ('Fill with the mean Embarked', F(\n",
    "                 lambda df: mode_fill(df, col='Embarked'))),\n",
    "             ('Embarked to Ordinal', F(lambda df: ord_encoder_col(df, col='Embarked'))),\n",
    "             ('Fill desconocido Cabin', F(\n",
    "                 lambda df: fill_desconocido(df, col='Cabin'))),\n",
    "             ('Cabin to Cat', F(cabin_cat)),\n",
    "             ('Family size gen', F(family_size_gen)),\n",
    "             ('drop cols', F(lambda df: drop_cols(df, cols=to_drop))),\n",
    "             ('Scaler', F(std_scaler)),\n",
    "             ('clf', model)\n",
    "             ]\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se preparan los datos para este paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('ProjectLab/data/train.csv')\n",
    "test_data = data.sample(frac=0.1, random_state = 5)\n",
    "\n",
    "train_idx = data.index.difference(test_data.index) \n",
    "\n",
    "y_train = data.loc[train_idx,'Survived'] \n",
    "x_train = data.drop('Survived', axis=1).loc[train_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se busca un esquema de Grid search, para implementarlo se empaquetan las funciones usuales. En este caso se debe tener en cuenta la **reproducibilidad** por lo que se asignará un valor de estado a la semilla aleatoria cuando sea posible. Por último se debe tener en cuanta la **persistencia** de los modelos, es decir, la capacidad de utilizarlos en entornos distintos al actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_train(model, hyperparams, x_train=x_train, folds = 3):\n",
    "    '''Recibe un modelo base y lo entrena usando GridSearchCV.\n",
    "    \n",
    "    Recibe un modelo y un conjunto de hiperparametros para realizar busqueda \n",
    "    por grilla. \n",
    "    \n",
    "    Parametros:\n",
    "    ---------- \n",
    "    \n",
    "    model: sklearn.estimator\n",
    "        Clasificador de sklearn.\n",
    "    \n",
    "    hyperams: dict \n",
    "        Diccionario de hiperparametros compatibles con model.\n",
    "    \n",
    "    x_train: pandas.DataFrame\n",
    "        Conjunto de datos sobre el cual entrenar. \n",
    "    \n",
    "    folds: int \n",
    "        numero de folds asociados al esquema de grilla por validacion cruzada.\n",
    "        \n",
    "    Retorna: \n",
    "    -------- \n",
    "    \n",
    "    res: dict \n",
    "        Entrega un diccionaro con el mejor estimador encontrado (best_estimator)\n",
    "        y el objeto grilla asociado (gs_object).\n",
    "    '''\n",
    "\n",
    "    h_pars = {'clf__'+k: val for k, val in hyperparams.items()}\n",
    "    tuning_pipe = Pipeline(steps=step_gen(model))\n",
    "\n",
    "    gs = GridSearchCV(estimator=tuning_pipe, param_grid=h_pars,\n",
    "                      verbose=1, cv=folds, scoring=\"accuracy\", n_jobs=-1)\n",
    "    \n",
    "    print('Entrenando ...')\n",
    "    gs.fit(x_train, y_train)\n",
    "\n",
    "    return {'best_model': gs.best_estimator_['clf'], 'gs_object': gs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para asegurar persistencia de los modelos utilizamos la librería `joblib`. Se procede a entrenar un modelo de clasificación basado en vectores de soporte SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os \n",
    "\n",
    "try:\n",
    "    os.mkdir('ProjectLab/tuned_models')\n",
    "    print('Se crea la carpeta exitosamente')\n",
    "    \n",
    "except FileExistsError as error:\n",
    "    print('La carpeta ya existe:', error)\n",
    "    \n",
    "path = 'ProjectLab/tuned_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = [0.001, 0.01, 0.1, 1, 5, 10, 15, 20, 50, 100]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "\n",
    "hyperparams = dict()  # {'C': Cs, 'gamma': gammas}\n",
    "\n",
    "try:\n",
    "    svc = joblib.load(path + 'svc')\n",
    "\n",
    "except FileNotFoundError:\n",
    "\n",
    "    res = grid_train(model=SVC(probability = True, random_state = 0),\n",
    "                     hyperparams=hyperparams)\n",
    "\n",
    "    svc = res['best_model']\n",
    "    joblib.dump(svc, path + 'svc')\n",
    "\n",
    "    print('Mejor puntaje:', res['gs_object'].best_score_)\n",
    "\n",
    "print('Mejor modelo:', svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso de persistencia anterior se encapsula en la siguiente función\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(base_model=None, hyperparams=None, model_name=None, path=path):\n",
    "    ''' Busca un modelo en path + model_name o entrena uno nuevo.\n",
    "\n",
    "    Busca un modelo usando path + model_name. Si tal modelo no se encuentra, \n",
    "    entrena un esquema de grilla con validacion cruzada sobre el modelo \n",
    "    base_model usando los hiperparametros hyperparams\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        model = joblib.load(path+model_name)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "\n",
    "        res = grid_train(model=base_model,\n",
    "                         hyperparams=hyperparams)\n",
    "\n",
    "        model = res['best_model']\n",
    "        joblib.dump(model, path + model_name)\n",
    "\n",
    "        print('Mejor puntaje:', res['gs_object'].best_score_)\n",
    "        \n",
    "    \n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "    \n",
    "    print('Mejor modelo:', model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridearch para Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "n_estimators = [100, 500, 750, 1000, 1250, 1400]\n",
    "\n",
    "hyperparams = {'learning_rate': learning_rate,\n",
    "               'n_estimators': n_estimators}\n",
    "\n",
    "gbc = get_model(base_model=GradientBoostingClassifier(random_state=0),\n",
    "                hyperparams=hyperparams, model_name='gbc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede con Regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = ['l1', 'l2']\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "hyperparams = {'penalty': penalty, 'C': C}\n",
    "\n",
    "log = get_model(base_model=LogisticRegression(random_state=0),\n",
    "                hyperparams=hyperparams, model_name='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de XGBoost se hace un búsqueda dividida en múltiples pasos debido a la gran cantidad de hiperparámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
    "n_estimators = [10, 25, 50, 75, 100, 250]\n",
    "\n",
    "\n",
    "hyperparams = {'learning_rate': learning_rate, 'n_estimators': n_estimators}\n",
    "\n",
    "xgb = get_model(base_model=XGBClassifier(random_state=0),\n",
    "                hyperparams=hyperparams, model_name='xgb_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = [3, 4, 5, 6, 7]\n",
    "min_child_weight = [1, 2, 3, 4]\n",
    "\n",
    "hyperparams = {'max_depth': max_depth, 'min_child_weight': min_child_weight}\n",
    "\n",
    "xgb = get_model(base_model=xgb, hyperparams=hyperparams, model_name='xgb_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = [0.02,0.1,0.2]\n",
    "\n",
    "hyperparams = {'gamma': gamma}\n",
    "\n",
    "xgb = get_model(base_model=xgb, hyperparams=hyperparams, model_name='xgb_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]\n",
    "colsample_bytree = [0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
    "    \n",
    "hyperparams = {'subsample': subsample, 'colsample_bytree': colsample_bytree}\n",
    "\n",
    "xgb = get_model(base_model=xgb, hyperparams=hyperparams, model_name='xgb_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_alpha = [1e-5, 1e-2, 0.1, 1, 100]\n",
    "    \n",
    "hyperparams = {'reg_alpha': reg_alpha}\n",
    "\n",
    "xgb = get_model(base_model=xgb, hyperparams=hyperparams, model_name='xgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenado el modelo xgb se procede con el clasificador basado en procesos gaussianos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_restarts_optimizer = [0, 1, 2, 3]\n",
    "max_iter_predict = [1, 2, 5, 10, 20, 35, 50, 100]\n",
    "warm_start = [True, False]\n",
    "\n",
    "\n",
    "hyperparams = {'n_restarts_optimizer': n_restarts_optimizer,\n",
    "               'max_iter_predict': max_iter_predict, 'warm_start': warm_start}\n",
    "\n",
    "gpc = get_model(base_model=GaussianProcessClassifier(\n",
    "    random_state=0), hyperparams=hyperparams, model_name='gpc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se continua con AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10, 25, 50, 75]\n",
    "learning_rate = [0.001, 0.01, 0.1, 0.5, 1]\n",
    "\n",
    "\n",
    "hyperparams = {'n_estimators': n_estimators, 'learning_rate': learning_rate}\n",
    "\n",
    "ada = get_model(base_model=AdaBoostClassifier(random_state=0),\n",
    "                hyperparams=hyperparams, model_name='ada')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede con knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, 18, 20]\n",
    "algorithm = ['auto']\n",
    "weights = ['uniform', 'distance']\n",
    "leaf_size = [1, 2, 3, 4, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "hyperparams = {'algorithm': algorithm, 'weights': weights, 'leaf_size': leaf_size,\n",
    "               'n_neighbors': n_neighbors}\n",
    "\n",
    "knn = get_model(base_model=KNeighborsClassifier(),\n",
    "                hyperparams=hyperparams, model_name='knn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrena el clasificador Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10, 25, 50]\n",
    "max_depth = [3, None]\n",
    "max_features = [1, 3, 5]\n",
    "min_samples_split =[4, 6, 8]\n",
    "min_samples_leaf = [4, 6, 8, 10]\n",
    "\n",
    "hyperparams = {'n_estimators': n_estimators, 'max_depth': max_depth, 'max_features': max_features,\n",
    "               'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "ran = get_model(base_model=RandomForestClassifier(random_state=0),\n",
    "                hyperparams=hyperparams, model_name='ran')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pasa a Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10, 25, 50]\n",
    "max_depth = [3, None]\n",
    "max_features = [1, 3, 5]\n",
    "min_samples_split = [4, 6, 8, 10]\n",
    "min_samples_leaf = [2, 4, 6]\n",
    "\n",
    "hyperparams = {'n_estimators': n_estimators, 'max_depth': max_depth, 'max_features': max_features,\n",
    "               'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "ext = get_model(base_model=ExtraTreesClassifier(random_state=0),\n",
    "                hyperparams=hyperparams, model_name='ext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente entrenamos el clasificador basado en bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10, 15, 20, 50, 75, 100]\n",
    "max_samples = [5, 10, 15, 20, 30, 50]\n",
    "max_features = [5, 7]\n",
    "\n",
    "hyperparams = {'n_estimators': n_estimators, 'max_samples': max_samples, 'max_features': max_features}\n",
    "\n",
    "bag = get_model(base_model=BaggingClassifier(random_state=0),\n",
    "                hyperparams=hyperparams, model_name='bag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes no posee hiperparámetros por lo que no se entrena usando este esquema. \n",
    "\n",
    "Se procede a estudiar el efecto global de los modelos entrenados sobre los hiperparámetros encontrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vars = ['Pclass','Sex','Age', 'Fare',\n",
    "              'Family_Survival', 'Title', 'Family_Size']\n",
    "\n",
    "X = data_prep.transform(test_data.drop('Survived', axis=1))\n",
    "X = X.reindex(columns=final_vars)\n",
    "y = test_data['Survived']\n",
    "\n",
    "train = data_prep.transform(x_train).reindex(columns = final_vars);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists\n",
    "models = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]         \n",
    "scores_v3 = []\n",
    "\n",
    "train = data_prep.transform(x_train)\n",
    "\n",
    "# Fit & cross-validate\n",
    "for mod in models:\n",
    "    mod.fit(train, y_train)\n",
    "    acc = cross_val_score(mod, train, y_train, scoring = \"accuracy\", cv = 10)\n",
    "    scores_v3.append(acc.mean())\n",
    "\n",
    "# Creating a table of results, ranked highest to lowest\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'K Nearest Neighbour', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier'],\n",
    "    'original': scores,\n",
    "    'feature selection': scores_v2,\n",
    "    'tuned': scores_v3})\n",
    "\n",
    "result_df = results.sort_values(by='tuned', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo anterior se visualiza de la siguiente manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[9, 7])\n",
    "\n",
    "sns.barplot(x='tuned', y='Model', data=result_df, color='r', label='Tuned',edgecolor = 'k')\n",
    "\n",
    "sns.barplot(x='feature selection', y='Model', data=result_df,\n",
    "            color='b', label='Selection', alpha=0.5, edgecolor = 'k') \n",
    "\n",
    "sns.barplot(x='original', y='Model', data=result_df,\n",
    "            color='w', label='Original', alpha=.9, edgecolor = 'c')    \n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.title('Accuracy por modelo', fontsize=20)\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.ylabel('Algoritmo')\n",
    "plt.xlim(0.82, 0.88);  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede apreciar que salvo para la regresión logística, gaussian naive bayes y gradient boosting, los modelos con hiper parámetros ajustados superan tanto a las versiones originales como a la aplicación de selección simple de características. En general se ve un buen aumento en presición al comparar con los resultados iniciales. \n",
    "\n",
    "El paso final es combinar estos modelos por medio de un clasificador de voto por mayoria. Comenzamos entrenando un modelo con esquema de votación duro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_hard_clf = VotingClassifier(estimators=[('Random Forest', ran),\n",
    "                                             ('Logistic Regression', log),\n",
    "                                             ('XGBoost', xgb),\n",
    "                                             ('Gradient Boosting', gbc),\n",
    "                                             ('Extra Trees', ext),\n",
    "                                             ('AdaBoost', ada),\n",
    "                                             ('Gaussian Process', gpc),\n",
    "                                             ('SVC', svc),\n",
    "                                             ('K Nearest Neighbour', knn),\n",
    "                                             ('Bagging Classifier', bag)], voting='hard')\n",
    "\n",
    "vote_hard = get_model(base_model=vote_hard_clf, hyperparams=dict(), model_name='vote_hard');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vars = ['Pclass','Sex','Age', 'Fare',\n",
    "              'Family_Survival', 'Title', 'Family_Size']\n",
    "\n",
    "X = data_prep.transform(test_data.drop('Survived', axis=1))\n",
    "X = X.reindex(columns=final_vars)\n",
    "y = test_data['Survived']\n",
    "\n",
    "train = data_prep.transform(x_train).reindex(columns = final_vars);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Hard voting en train: {vote_hard.score(train,y_train)*100}\") \n",
    "print(f\"Hard voting en test : {vote_hard.score(X,y)*100}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último se compara con un esquema de votación suave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_soft_clf = VotingClassifier(estimators=[('Random Forest', ran),\n",
    "                                         ('Logistic Regression', log),\n",
    "                                         ('XGBoost', xgb),\n",
    "                                         ('Gradient Boosting', gbc),\n",
    "                                         ('Extra Trees', ext),\n",
    "                                         ('AdaBoost', ada),\n",
    "                                         ('Gaussian Process', gpc),\n",
    "                                         ('SVC', svc),\n",
    "                                         ('K Nearest Neighbour', knn),\n",
    "                                         ('Bagging Classifier', bag)], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_soft = get_model(base_model=vote_soft_clf, hyperparams=dict(), model_name='vote_soft');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Soft voting en train: {vote_soft.score(train,y_train)*100}\") \n",
    "print(f\"Soft voting en test : {vote_soft.score(X,y)*100}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambos modelos concuerdan en sus resultados, se selecciona el sistema de votación suave como modelo final.\n",
    "\n",
    "**Obs:** El flujo de trabajo implementado se basa en la siguiente [fuente](https://www.kaggle.com/josh24990/simple-end-to-end-ml-workflow-top-5-score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Continuación -- Entorno de desarrollo** \n",
    "\n",
    "Ya seleccionado el modelo a trabajar, se procede a traspasar el código a un entorno de desarrollo. La idea es generar una estructura de módulos que permita ejecutar los procesos de preprocesamiento, entrenamiento y predicción desde otras aplicaciones. En este caso la aplicación que hará uso de esta estructura modular será la  API que se desarrollará. \n",
    "\n",
    "Procedemos a modularizar el código generado en el entorno de investigación. Para esto se genera la siguiente estructura de módulos: \n",
    "\n",
    "```\n",
    "config.py\n",
    "preprocessors.py \n",
    "train.py \n",
    "predict.py\n",
    "pipeline.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**config.py**\n",
    "\n",
    "Posee las configuraciones generales a utilizar en los procesos de nuestro módulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ProjectLab/config.py\n",
    "TARGET = 'Survived'\n",
    "MODELS_PATH  = 'tuned_models/'\n",
    "LAST_MODEL_PATH = 'last_models/'\n",
    "LAST_MODEL_NAME = 'last_model'\n",
    "\n",
    "\n",
    "DATA_PATH_TRAIN = 'data/train.csv'\n",
    "\n",
    "FAMILY_DICT ='meta_data/family_dict'\n",
    "TO_DROP_PIPE = ['Name', 'Parch', 'SibSp', 'Ticket', 'Last_Name', 'PassengerId', 'Embarked', 'Cabin']\n",
    "\n",
    "\n",
    "FINAL_VARS = ['Pclass','Sex','Age', 'Fare',\n",
    "              'Family_Survival', 'Title', 'Family_Size']\n",
    "\n",
    "FINAL_MODEL = 'vote_soft'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**preprocessors.py**\n",
    "\n",
    "En este módulo procedemos a encapsular los preprocesadores utilizados en el entorno de investigación. Para ello, se hace uso de herencia múltiple según `BaseEstimator` y `TransformerMixin`. Se importan las librerías necesarias para que estos preprocesdores funcionen. Se traspasan las funciones utilizadas a objetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ProjectLab/preprocesssors.py\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import config\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class NaMeanFiller(BaseEstimator, TransformerMixin):\n",
    "    '''Cambia los datos faltantes de la columna col por la media en df.'''\n",
    "\n",
    "    def __init__(self, col='Fare'):\n",
    "        self.col = col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        data = X.copy()\n",
    "        data[self.col].fillna(data[self.col].mean(), inplace=True)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class LastNameGen(BaseEstimator, TransformerMixin):\n",
    "    '''Genera la columna Last_name a partir de Name en el DataFrame df.'''\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        data = X.copy()\n",
    "        data['Last_Name'] = data['Name'].apply(lambda x: str.split(x, \",\")[0])\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class FamilySurvivalGen(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    '''Recibe observaciones y asigna probabilidad de supervivencia por familia.\n",
    "\n",
    "    Permite generar una variable donde se asigna una proporcion de sobrevivencia\n",
    "    por familia. Para ello toma un DataFrame con las relaciones por familia si\n",
    "    el apellido asociado a la observacion x en df esta en el conjunto  de\n",
    "    familias, le asigna el valor indicado en family_dict. En caso contrario\n",
    "    le asigna la probabilidad default_prob.\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, default_prob=.5, family_dict=None):\n",
    "\n",
    "        self.default_prob = default_prob\n",
    "        self.family_dict = family_dict\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        '''Genera un DataFrame con probabilidades de supervivencia por familia.\n",
    "\n",
    "        El conjunto de datos debe tener las columnas: Survived, Name, Last_Name, \n",
    "        Fare, Ticket, PassengerId, SibSp, Parch, Age y Cabin. Se genera un  \n",
    "        DataFrame con la relación familia -> fecuencia de supervivenca.\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "\n",
    "        data: Pandas.DataFrame\n",
    "            Un conjunto con la relacion familia - frecuencia de supervivencia\n",
    "        '''\n",
    "\n",
    "        if self.family_dict is None:\n",
    "\n",
    "            data = X.copy()\n",
    "\n",
    "            # Utiliza un valor prior para la probabilidad de sobrevivir\n",
    "            data['Family_Survival'] = self.default_prob\n",
    "\n",
    "            # Se agrupa el conjunto de datos por apellido y fare\n",
    "            for grp, grp_df in data[['Survived', 'Name', 'Last_Name', 'Fare',\n",
    "                                     'Ticket', 'PassengerId',\n",
    "                                     'SibSp', 'Parch', 'Age',\n",
    "                                     'Cabin']].groupby(['Last_Name', 'Fare']):\n",
    "\n",
    "                # Si no es igual a 1, se encuentra una familia\n",
    "                # en tal caso se calcula una probabilidad de supervivencia\n",
    "                if (len(grp_df) != 1):\n",
    "                    for ind, row in grp_df.iterrows():\n",
    "                        smax = grp_df.drop(ind)['Survived'].max()\n",
    "                        smin = grp_df.drop(ind)['Survived'].min()\n",
    "                        passID = row['PassengerId']\n",
    "                        if (smax == 1.0):\n",
    "                            data.loc[data['PassengerId'] ==\n",
    "                                     passID, 'Family_Survival'] = 1\n",
    "                        elif (smin == 0.0):\n",
    "                            data.loc[data['PassengerId'] ==\n",
    "                                     passID, 'Family_Survival'] = 0\n",
    "\n",
    "            # Luego se agrupa la informacion por tickets y se procede\n",
    "            for _, grp_df in data.groupby('Ticket'):\n",
    "                if (len(grp_df) != 1):\n",
    "                    for ind, row in grp_df.iterrows():\n",
    "                        if (row['Family_Survival'] == 0) | (row['Family_Survival'] == 0.5):\n",
    "                            smax = grp_df.drop(ind)['Survived'].max()\n",
    "                            smin = grp_df.drop(ind)['Survived'].min()\n",
    "                            passID = row['PassengerId']\n",
    "                            if (smax == 1.0):\n",
    "                                data.loc[data['PassengerId'] ==\n",
    "                                         passID, 'Family_Survival'] = 1\n",
    "                            elif (smin == 0.0):\n",
    "                                data.loc[data['PassengerId'] ==\n",
    "                                         passID, 'Family_Survival'] = 0\n",
    "\n",
    "            self.family_dict = data.groupby('Last_Name').mean()[\n",
    "                'Family_Survival']\n",
    "        else:\n",
    "            return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        data = X.copy()\n",
    "        data['Family_Survival'] = self.default_prob\n",
    "\n",
    "        if self.family_dict is not None:\n",
    "\n",
    "            for x in data.itertuples():\n",
    "                if x.Last_Name in self.family_dict:\n",
    "                    prob = self.family_dict.loc[x.Last_Name]\n",
    "                    data.loc[data['PassengerId'] ==\n",
    "                             x.PassengerId, 'Family_Survival'] = prob\n",
    "            return data\n",
    "\n",
    "        else:\n",
    "            print('Inicializar diccionario de familias antes de transformar!')\n",
    "\n",
    "\n",
    "class ColToCat(BaseEstimator, TransformerMixin):\n",
    "    '''Toma un conjunto de datos df y transfroma la columna col en categorica.\n",
    "\n",
    "    La cantidad de bins viene dada por el parametro cuts.'''\n",
    "\n",
    "    def __init__(self, col='Fare', cuts=4):\n",
    "        self.col = col\n",
    "        self.cuts = cuts\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        lbl = LabelEncoder()\n",
    "        data = X.copy()\n",
    "\n",
    "        data[self.col] = pd.qcut(data[self.col], self.cuts)\n",
    "        data[self.col] = lbl.fit_transform(data[self.col])\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class TitleGen(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def get_title(self, name):\n",
    "        '''Obtiene le titulo asociado a una persona.'''\n",
    "\n",
    "        if '.' in name:\n",
    "            return name.split(',')[1].split('.')[0].strip()\n",
    "        else:\n",
    "            return 'desconocido'\n",
    "\n",
    "    def set_title(self, x):\n",
    "        '''Reduce el titulo a Mr,Mrs o Miss segun corresponda.'''\n",
    "\n",
    "        title = x['Title']\n",
    "        if title in ['Capt', 'Col', 'Don', 'Jonkheer', 'Major', 'Rev', 'Sir']:\n",
    "            return 'Mr'\n",
    "        elif title in ['the Countess', 'Mme', 'Lady', 'Dona']:\n",
    "            return 'Mrs'\n",
    "        elif title in ['Mlle', 'Ms']:\n",
    "            return 'Miss'\n",
    "        elif title == 'Dr':\n",
    "            if x['Sex'] == 'male':\n",
    "                return 'Mr'\n",
    "            else:\n",
    "                return 'Mrs'\n",
    "        else:\n",
    "            return title\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        data = X.copy()\n",
    "        data['Title'] = data['Name'].map(lambda x: self.get_title(str(x)))\n",
    "        data['Title'] = data.apply(self.set_title, axis=1)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class AgeImputer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        ''' Llena la información faltante de la columna Age en df. \n",
    "\n",
    "        Utiliza una agrupacion por titulo y aplica un llenado por mediana de grupo.\n",
    "\n",
    "        Retorna:\n",
    "        -------- \n",
    "\n",
    "        data: Pandas.DataFrame \n",
    "            Conjunto de datos con la variable Age completada.\n",
    "        '''\n",
    "        data = X.copy()\n",
    "        data['Age'] = data.groupby('Title')['Age'].apply(\n",
    "            lambda x: x.fillna(x.median()))\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class ModeFill(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, col):\n",
    "        self.col = col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''Llena los valores faltantes de col en df usando la moda global.\n",
    "\n",
    "        Retorna: \n",
    "        -------- \n",
    "\n",
    "        data: Pandas.DataFrame \n",
    "            Conjunto de datos con la informacion de col completada. \n",
    "        '''\n",
    "\n",
    "        data = X.copy()\n",
    "        data[self.col] = data[self.col].fillna(data[self.col].mode()[0])\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class FillDesconocido(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, col):\n",
    "        self.col = col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        ''' Toma un conjunto de datos df y llena la columna col con el valor\n",
    "        'desconocido'.\n",
    "\n",
    "        Retorna:\n",
    "        -------- \n",
    "\n",
    "        data : Pandas.DataFrame\n",
    "            Conjunto de datos con la variable col completada. \n",
    "\n",
    "        '''\n",
    "        data = X.copy()\n",
    "        data[self.col].fillna('desconocido', inplace=True)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class CabinCat(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def unknown_cabin(self, cabin):\n",
    "        if cabin != 'd':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''Genera dos categorias en df basandose en la columna Cabin.\n",
    "\n",
    "        Retorna:\n",
    "        -------\n",
    "\n",
    "        data : Pandas.DataFrame \n",
    "            Conjunto de datos con la categorizacion creada. \n",
    "        '''\n",
    "\n",
    "        data = X.copy()\n",
    "        data['Cabin'] = data['Cabin'].map(lambda x: x[0])\n",
    "        data['Cabin'] = data['Cabin'].apply(lambda x: self.unknown_cabin(x))\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class FamilySizeGen(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''Genera la columna Family_Size a partir de SibSp y Parch en df. \n",
    "\n",
    "        Retorna:\n",
    "        ------- \n",
    "        data : Pandas.DataFrame \n",
    "            Conjunto de datos con la columna creada. \n",
    "\n",
    "        '''\n",
    "        data = X.copy()\n",
    "        data['Family_Size'] = data['SibSp'] + data['Parch']\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class OneHotEncoderCol(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, col='Sex'):\n",
    "        self.col = col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''Genera codificacion One hot en df basandose en la columna col.\n",
    "\n",
    "        Retorna:\n",
    "        -------\n",
    "\n",
    "        data : Pandas.DataFrame\n",
    "            Conjunto de datos con el encoding creado.\n",
    "        '''\n",
    "        data = X.copy()\n",
    "        ohe = OneHotEncoder(sparse=False)\n",
    "        data[self.col] = ohe.fit_transform(\n",
    "            data[self.col].values.reshape([-1, 1]))\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class OrdinaltEncoderCol(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, col='Title'):\n",
    "        self.col = col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''Genera codificacion Ordinal df basandose en la columna col.\n",
    "\n",
    "        Retorna:\n",
    "        -------\n",
    "\n",
    "        data : Pandas.DataFrame \n",
    "            Conjunto de datos con el encoding creado. \n",
    "        '''\n",
    "\n",
    "        data = X.copy()\n",
    "        ode = OrdinalEncoder()\n",
    "        data[self.col] = ode.fit_transform(\n",
    "            data[self.col].values.reshape([-1, 1]))\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class DropCols(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols=config.TO_DROP_PIPE):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''Elimina las columans cols del DataFrame df.\n",
    "\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        data : Pandas.DataFrame \n",
    "            Conjunto de datos con las columnas borradas. \n",
    "\n",
    "        '''\n",
    "\n",
    "        data = X.copy()\n",
    "        data.drop(self.cols, axis=1, inplace=True)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class StdScaler(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''Toma un DataFrame y lo estandariza, retorna un DataFrame.'''\n",
    "\n",
    "        std_scaler = StandardScaler()\n",
    "\n",
    "        data = X.copy()\n",
    "        cols = data.columns\n",
    "\n",
    "        return pd.DataFrame(std_scaler.fit_transform(data), columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pipeline.py** \n",
    "\n",
    "En este módulo generamos la pipeline de entrenamiento utilizando los preprocedadores construidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ProjectLab/pipeline.py \n",
    "import joblib\n",
    "\n",
    "import config\n",
    "import preprocesssors as pp \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "family_dict = joblib.load(config.FAMILY_DICT)\n",
    "model = joblib.load(config.MODELS_PATH + config.FINAL_MODEL)\n",
    "\n",
    "train_pipe = Pipeline([\n",
    "    ('Fare mean fill',pp.NaMeanFiller()),\n",
    "    ('Last_Name gen',pp.LastNameGen()),\n",
    "    ('Family_Survival gen', pp.FamilySurvivalGen(family_dict=family_dict)),\n",
    "    ('Fare to cat', pp.ColToCat()),\n",
    "    ('Title var gen', pp.TitleGen()),\n",
    "    ('Age imputer', pp.AgeImputer()),\n",
    "    ('Age to cat', pp.ColToCat(col='Age')),\n",
    "    ('Title to ordinal', pp.OrdinaltEncoderCol()),\n",
    "    ('Sex to One Hot Enc', pp.OneHotEncoderCol()),\n",
    "    ('Fill with the mode Embarked', pp.ModeFill(col='Embarked')),\n",
    "    ('Embarked to Ordinal', pp.OrdinaltEncoderCol(col='Embarked')),\n",
    "    ('Fill desconocido Cabin',pp.FillDesconocido(col='Cabin')),\n",
    "    ('Cabin to Cat', pp.CabinCat()),\n",
    "    ('Family size gen', pp.FamilySizeGen()),\n",
    "    ('drop cols',pp.DropCols(cols = config.TO_DROP_PIPE)),\n",
    "    ('Scaler', pp.StdScaler()),\n",
    "    ('clf', model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train.py** \n",
    "\n",
    "Acá se genera un rutina de entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ProjectLab/train.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "import pipeline\n",
    "import config\n",
    "\n",
    "\n",
    "def train(model_name = 'last_model'):\n",
    "    # Lee los datos\n",
    "    data = pd.read_csv(config.DATA_PATH_TRAIN)\n",
    "    test_data = data.sample(frac=0.1, random_state=5)\n",
    "\n",
    "    train_idx = data.index.difference(test_data.index)\n",
    "\n",
    "    y_train = data.loc[train_idx, config.TARGET]\n",
    "    x_train = data.drop(config.TARGET, axis=1).loc[train_idx]\n",
    "    \n",
    "    #Entrena el modelo\n",
    "    pipeline.train_pipe.fit(x_train, y_train)\n",
    "    \n",
    "    #Guarda el modelo \n",
    "    joblib.dump(pipeline.train_pipe, config.LAST_MODEL_PATH + model_name)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La notación anterior nos permite ejecutar el proceso de entrenamiento por medio de la terminal:\n",
    "\n",
    "```\n",
    "(entorno_virtual) user@ruta/a/ProjectLab$ python train.py\n",
    "```\n",
    "\n",
    "Con esto se entrena un modelo de nombre `last_model` en la carpeta `last_models`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**predict.py** \n",
    "\n",
    "Se genera un script que produce predicciones basadas en un modelo a elección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ProjectLab/predict.py \n",
    "import pandas as pd \n",
    "\n",
    "import joblib\n",
    "import config\n",
    "\n",
    "def predict(X):\n",
    "    \n",
    "    model = joblib.load(config.LAST_MODEL_PATH + config.LAST_MODEL_NAME)\n",
    "    \n",
    "    return model.predict(X)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    # Se obtienen los datos\n",
    "    data = pd.read_csv(config.DATA_PATH_TRAIN)\n",
    "\n",
    "    test_data = data.sample(frac=0.1, random_state=5)\n",
    "    train_idx = data.index.difference(test_data.index)\n",
    "        \n",
    "    # Datos de entrenamiento\n",
    "    y_train = data.loc[train_idx, config.TARGET]\n",
    "    x_train = data.drop(config.TARGET, axis=1).loc[train_idx]\n",
    "    \n",
    "    # Datos test\n",
    "    X = test_data.drop(config.TARGET, axis=1)\n",
    "    y = test_data[config.TARGET]\n",
    "    \n",
    "    # Resultados\n",
    "    c_train = classification_report(predict(x_train),y_train)\n",
    "    c_test = classification_report(predict(X),y)\n",
    "    \n",
    "    print('Score en Train:', c_train) \n",
    "    print('Score en Test:', c_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de generados los módulos es necesario crear una estructura librería"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testeo\n",
    "## Manejo de Requests\n",
    "## Construcción de una API\n",
    "## Despliegue en la Nube"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (entorno_virtual)",
   "language": "python",
   "name": "entorno_virtual"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "es",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "es",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
