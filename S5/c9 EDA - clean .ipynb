{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MA6202: Laboratorio de Ciencia de Datos\n",
    "\n",
    "**Profesor: Nicolás Caro**\n",
    "\n",
    "**27/04/2020 - C8 S4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv', index_col = 'Id')\n",
    "object_type_set = [col for col in df.columns if df[col].dtype == 'O']\n",
    "df = df.astype({col:'str' for col in object_type_set})\n",
    "names = ['numeric', 'categorical']\n",
    "# Se crea una lista con las columnas numericas\n",
    "numeric = [\n",
    "    'LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea',\n",
    "    'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF',\n",
    "    '2ndFlrSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n",
    "    'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'MiscVal', 'GarageYrBlt',\n",
    "    'MSSubClass','Fireplaces','SalePrice'\n",
    "]\n",
    "# Se crea una lista con las columnas categoricas\n",
    "categorical = list(set(df.columns) - set(numeric))\n",
    "''' \n",
    "Se generan mappings para el multi indexado del tipo \n",
    "[('numeric', col_if_numeric), ...,('categorical', col_if_categorical),...]\n",
    "'''\n",
    "mapping = [('numeric', col) for col in numeric]\n",
    "mapping.extend([('categorical', col) for col in categorical])\n",
    "'''\n",
    "Se reordenan las columnas del dataframe para que coincidan con el esquema \n",
    "del multi indice\n",
    "'''\n",
    "df = df.reindex(columns=numeric + categorical)\n",
    "df.columns = pd.MultiIndex.from_tuples(mapping)\n",
    "# Grilla de subplots\n",
    "fig, ax = plt.subplots(nrows=6, ncols=4, figsize=[17, 17])\n",
    "# Se remueven el ultimo plot\n",
    "list(map(lambda a : a.remove(), ax[-1,-1:]))\n",
    "# Se ajusta el espaciado exterior de la figura\n",
    "fig.tight_layout()\n",
    "# Se define un titulo y su ubicacion\n",
    "fig.suptitle('Distribuciones Univariadas Numéricas',\n",
    "             fontsize=20,\n",
    "             x=0.5,\n",
    "             y=1.05)\n",
    "'''\n",
    "Se recorre cada axis, para cada columna del dataframe, se genera un grafico \n",
    "distinto en funcion del tipo de dato.\n",
    "'''\n",
    "for axis, col in zip(ax.flatten(), numeric):\n",
    "    # Graficos para datos numericos\n",
    "    sns.distplot(df[('numeric', col)], ax=axis, rug=True)\n",
    "    axis.set_xlabel(col, fontsize=15)\n",
    "# Se ajusta el espaciado interno entre subplots\n",
    "w, h = (.4, .4)\n",
    "plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamiento de datos y Exploración \n",
    "\n",
    "La el manejo de la información contenida en los datos es la razón principal de la construcción de modelos y esquemas de análisis sobre ellos. Tal información se ve afectada por la calidad de los datos, que en segunda instancia, determina el rendimiento de los modelos planteados. Esto hace que sea critico asegurar un preprocesado y una buena examinación de los datasets a trabajar.\n",
    "\n",
    "El contenido de esta cátedra, se centra en las técnicas esenciales para el preprocesado de datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de datos exploratorio\n",
    "\n",
    "El análisis exploratorio de los datos (EDA en inglés) consiste e utilizar técnicas de sumarización o agregación, con el fin de conocer la distribución de los datos, confirmar hipótesis y contrastar información. Existen muchas maneras de explorar los datos, por ejemplo, se pueden generan visualizaciones, descripciones del conjunto de datos, se pueden también generar agrupaciones y obtener patrones de tales agrupaciones. \n",
    "\n",
    "Un concepto recurrente en el análisis exploratorio de datos consiste en el *perfilamiento* de datos. Este hace referencia a la sumarización por medio de estadística descriptiva, aquí existe una variedad de herramientas que pueden ayudar a comprender mejor los datos disponibles. La meta del perfilamiento de datos consiste en generar respuestas y conocimiento en torno al fenómeno que los datos reflejan. En función de los perfiles generados, se puede tener una idea de la calidad del dataset con lo cual es posible decidir como transformar las variables a disposición. A continuación se da una guía a seguir al momento de explorar los datos\n",
    "\n",
    "### Perfilamiento Univariado\n",
    "\n",
    "El punto inicial para comprender la naturaleza de una variable, pasa por caracterizar la forma de su distribución, un histograma permite obtener ideas sobre tal faceta.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se cargan las librerías iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se carga el dataset de 'house pricing', este consiste en 80 variables (79 variables explicativas más una variable objetivo), describiendo aspectos fundamentales de hogares residenciales en la ciudad de Ames, Iowa. Este dataset está centrado en la regresión sobre el precio final de cada hogar. A continuación se procede a explorar tal dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El conjunto a trabajar es el de entrenamiento\n",
    "df = pd.read_csv('data/train.csv', index_col = 'Id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cantidad de observaciones corresponde a 1460, por otra parte, posee 79 variables explicativas más un índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se estudia el tipo de valor y cantidad de información faltante para cada columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observado lo anterior y la estructura de la base, se aprecia que los datos de tipo `object` hacen referencia a 'strings' o categorias del dataset. Se crea una lista con aquellas columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_type_set = [col for col in df.columns if df[col].dtype == 'O']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa la estructura del dataset en tales columnas, en este caso se decide transformarlas a formato 'str' para obtener visualizaciones sobre sus valores. El proceso de transformar los tipos de datos en un dataframe se conoce como *typecasting*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se transoforman las columnas anteriores a 'str'\n",
    "df = df.astype({col:'str' for col in object_type_set})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sumado a lo anterior, se agregan niveles de multi indexado a las columnas para indicar si son del tipo numérico o categórico. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['numeric', 'categorical']\n",
    "\n",
    "# Se crea una lista con las columnas numericas\n",
    "numeric = [\n",
    "    'LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea',\n",
    "    'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF',\n",
    "    '2ndFlrSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n",
    "    'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'MiscVal', 'GarageYrBlt',\n",
    "    'MSSubClass','Fireplaces','SalePrice'\n",
    "]\n",
    "\n",
    "# Se crea una lista con las columnas categoricas\n",
    "categorical = list(set(df.columns) - set(numeric))\n",
    "''' \n",
    "Se generan mappings para el multi indexado del tipo \n",
    "\n",
    "[('numeric', col_if_numeric), ...,('categorical', col_if_categorical),...]\n",
    "'''\n",
    "\n",
    "mapping = [('numeric', col) for col in numeric]\n",
    "mapping.extend([('categorical', col) for col in categorical])\n",
    "'''\n",
    "Se reordenan las columnas del dataframe para que coincidan con el esquema \n",
    "del multi indice\n",
    "'''\n",
    "\n",
    "df = df.reindex(columns=numeric + categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalemente se asocia el multi indexado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Se reasignan las columnas\n",
    "df.columns = pd.MultiIndex.from_tuples(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como observación, se agrega que la columna 'MSSubClass' se clasifica como categórica pues representa un tipo de sector asociado a la propiedad. \n",
    "\n",
    "A continuación, se genera una visualización para entender la geometría de cada distribución, en el caso de las variables continuas, se calcula un estimado de la distribución por medio de `kernel density estimation`, este procedimiento consiste en elegir un tipo de función base (en este caso, una gaussiana con media en cada punto y de varianza constante) posteriormente, se calcula el promedio de las funciones base y se obtiene una función representante de la distribución denominada como 'density estimator', esto se hace por medio del método `sns.displot` de la librería `seaborn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grilla de subplots\n",
    "fig, ax = plt.subplots(nrows=6, ncols=4, figsize=[17, 17])\n",
    "\n",
    "# Se remueven el ultimo plot\n",
    "list(map(lambda a : a.remove(), ax[-1,-1:]))\n",
    "\n",
    "# Se ajusta el espaciado exterior de la figura\n",
    "fig.tight_layout()\n",
    "\n",
    "# Se define un titulo y su ubicacion\n",
    "fig.suptitle('Distribuciones Univariadas Numéricas',\n",
    "             fontsize=20,\n",
    "             x=0.5,\n",
    "             y=1.05)\n",
    "'''\n",
    "Se recorre cada axis, para cada columna del dataframe, se genera un grafico \n",
    "distinto en funcion del tipo de dato.\n",
    "\n",
    "'''\n",
    "for axis, col in zip(ax.flatten(), numeric):\n",
    "\n",
    "    # Graficos para datos numericos\n",
    "    sns.distplot(df[('numeric', col)], ax=axis, rug=True)\n",
    "    axis.set_xlabel(col, fontsize=15)\n",
    "\n",
    "# Se ajusta el espaciado interno entre subplots\n",
    "w, h = (.4, .4)\n",
    "plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las variables categóricas, se genera un conteo de valores únicos. Dado que se buscan las distribuciones de forma visual, se elimina información referente a las escalas, que dada la cantidad de gráficos a obtener, solo entorpece el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Grilla de subplots\n",
    "fig, ax = plt.subplots(nrows=10, ncols=6, figsize=[17, 17])\n",
    "\n",
    "# Se remueven los ultimos 3 plots\n",
    "list(map(lambda a : a.remove(), ax[-1,-3:]))\n",
    "\n",
    "# Se ajusta el espaciado exterior de la figura\n",
    "fig.tight_layout()\n",
    "\n",
    "# Se define un titulo y su ubicacion\n",
    "fig.suptitle('Distribuciones Univariadas Categóricas',\n",
    "             fontsize=20,\n",
    "             x=0.5,\n",
    "             y=1.05)\n",
    "'''\n",
    "Se recorre cada axis, para cada columna del dataframe, se genera un grafico \n",
    "distinto en funcion del tipo de dato.\n",
    "\n",
    "'''\n",
    "for axis, col in zip(ax.flatten(), categorical):\n",
    "\n",
    "    # Graficos para datos tipos str\n",
    "    sns.countplot(df[('categorical',col)], ax=axis)\n",
    "    axis.set_axis_off()\n",
    "    axis.set_title(col, fontsize=15)\n",
    "  \n",
    "    \n",
    "# Se ajusta el espaciado interno entre subplots\n",
    "h, w = (.4, .1)\n",
    "plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al observar las distribuciones, es importante buscar si existe variabilidad dentro de estas, pues por lo general, una variable con un único valor casi seguro, no aporta información a la dinámica de los datos.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se observa la variable 'Heating' (categórica) y se compara con la variable de interés 'SalePrice'. Para ello se usa un gráfico de categórias tipo violín"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sirve para fija el tamaño de lasetiquetas del plot\n",
    "fontdict = {'fontsize':20}\n",
    "\n",
    "# Estrucutra de figura y axes\n",
    "fig, ax = plt.subplots(2,1,figsize=[12,13])\n",
    "\n",
    "# violin plot --> equivalente a catplot(kind = 'violin')\n",
    "\n",
    "sns.violinplot(('categorical', 'OverallQual'),\n",
    "            y=('numeric', 'SalePrice'),\n",
    "            data=df,\n",
    "            kind='violin',\n",
    "            ax=ax[0])\n",
    "\n",
    "sns.countplot(df[('categorical','OverallQual')], ax=ax[1])\n",
    "\n",
    "ax[0].set_xlabel('OverallQual', fontdict)\n",
    "ax[1].set_xlabel('OverallQual', fontdict)\n",
    "\n",
    "ax[0].set_ylabel('SalePrice', fontdict)\n",
    "ax[0].set_title('Violin plot OverallQuall vs SalePrice', fontdict)\n",
    "ax[1].set_title('Frecuencias OverallQuall', fontdict)\n",
    "\n",
    "h, w = (.3, .1)\n",
    "plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un gráfico de violín permite sumarizar y observar características de un dataset. Este se comporta como un gráfico de cajas (boxplot), mostrando la mediana, el rango intercuantílico IQR (percentil 75 - percentil 25, o Q3 - Q1) y el rango 1.5 intercuantílico (Q3 +- 1.5 IQR). Además de lo anterior, se suma una estimación de la densidad por kernel a cada lado. Esto quiere decir, que zonas con mayor densidad, se verán como 'montes' horizontales. \n",
    "\n",
    "En el caso de 'OverallQuall', se ve una clara relación entre los distintos niveles de está variable en contraste con difierentes distribuciones de 'SalePrice'. Junto con una distribción que presenta variabilidad, se podría considerar como una de interés. \n",
    "\n",
    "\n",
    "Por otra parte, analizando las gráficas univariadas, se puede observar que para 'LandSlope', se tiene poca variablidad y no genera diferencias en distribución para 'SalePrice' en ninguna de sus categorias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sirve para fija el tamaño de lasetiquetas del plot\n",
    "fontdict = {'fontsize':20}\n",
    "\n",
    "# Estrucutra de figura y axes\n",
    "fig, ax = plt.subplots(2,1,figsize=[12,13])\n",
    "\n",
    "# violin plot --> equivalente a catplot(kind = 'violin')\n",
    "\n",
    "sns.violinplot(('categorical', 'LandSlope'),\n",
    "            y=('numeric', 'SalePrice'),\n",
    "            data=df,\n",
    "            kind='violin',\n",
    "            ax=ax[0])\n",
    "\n",
    "sns.countplot(df[('categorical','LandSlope')], ax=ax[1])\n",
    "\n",
    "ax[0].set_xlabel('LandSlope', fontdict)\n",
    "ax[1].set_xlabel('LandSlope', fontdict)\n",
    "\n",
    "ax[0].set_ylabel('SalePrice', fontdict)\n",
    "ax[0].set_title('Violin plot LandSlope vs SalePrice', fontdict)\n",
    "ax[1].set_title('Frecuencias LandSlope', fontdict)\n",
    "\n",
    "h, w = (.3, .1)\n",
    "plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios**\n",
    "\n",
    "1. Los gráficos generados anteriormente siguen exactamente el mismo patrón de generación, lo único que cambia es la columna a analizar. Esto es una mala práctica pues siempre se debe buscar reutilizar código o 'no repetirse' esto se conoce como principio DRY (don't repeat yourself). Construya una función que permita visualizar columnas categóricas del dataset y compararlas con 'SalePrice'. \n",
    "\n",
    "2. En función de las visualizaciones construidas, discuta que variables categóricas pueden ser de interés para predecir 'SalePrice'. Busque variabilidad y separación en la distribución de precios. ¿Qué ocurre si una variable categórica posee poca variablidad pero genera buenas separaciones en  'SalePrice'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comparar las variables numéricas, se pueden utilizar gráficos de dispersión contra 'SalePrice'. En este caso, se buscan variabilidad en el histograma univariado y a la vez, se buscan relaciones funcionales (del tipo lineal, exponencial, cuadrático, etc..) con 'SalePrice'. \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Debido a su distribución, se estudia la variable 'GrLivArea', en este caso se define una función para gráficar variables numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_dists(col, df=df, h=.3, w=.1, fontdict={'fontsize': 20}, reg=True):\n",
    "    ''' Recibe una columna numerica y genera una visualizacion comparativa.\n",
    "    \n",
    "    Genera una figura por sobre el dataframe HousePricing (por defecto), recibe \n",
    "    parametros extra como el espaciado entre subfigura.\n",
    "    \n",
    "    Args:\n",
    "    ----------\n",
    "    \n",
    "    col: String\n",
    "         El nombre de la columna numerica a visualizar\n",
    "    \n",
    "    h,w: float\n",
    "        Espaciado entre subplot h -> vertical, w -> horizontal\n",
    "    \n",
    "    fontdict: dict\n",
    "             Permite configurar las fuentes de los subplots\n",
    "    reg: bool\n",
    "         Permite graficar una regresion lineal sobre los datos (if True)\n",
    "        \n",
    "    Returns: None\n",
    "        Se muestra una figura en pantalla    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Estrucutra de figura y axes\n",
    "    fig, ax = plt.subplots(2, 1, figsize=[12, 13])\n",
    "\n",
    "    # violin plot --> equivalente a catplot(kind = 'violin')\n",
    "\n",
    "    if reg:\n",
    "        sns.regplot(x=df[('numeric', col)],\n",
    "                    y=df[('numeric', 'SalePrice')],\n",
    "                    ax=ax[0])\n",
    "        ax[0].set_title('Regplot plot {} vs SalePrice'.format(col), fontdict)\n",
    "    else:\n",
    "        sns.scatterplot(('numeric', col),\n",
    "                        y=('numeric', 'SalePrice'),\n",
    "                        data=df,\n",
    "                        ax=ax[0])\n",
    "        ax[0].set_title('Scatter plot {} vs SalePrice'.format(col), fontdict)\n",
    "\n",
    "    \n",
    "    # Distribucion univariada\n",
    "    sns.distplot(df[('numeric', col)], ax=ax[1])\n",
    "\n",
    "    ax[0].set_xlabel(col, fontdict)\n",
    "    ax[1].set_xlabel(col, fontdict)\n",
    "\n",
    "    ax[0].set_ylabel('SalePrice', fontdict)\n",
    "    ax[1].set_title('Frecuencias {}'.format(col), fontdict)\n",
    "\n",
    "    plt.subplots_adjust(wspace=w, hspace=h)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_dists('GrLivArea')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, se puede observar una distrbución univariada bien definida y un comportamiento lineal aunque ruidoso. Esto hace que 'GrLivArea' sea una variable de interés. De la misma manera, '1stFlrSF', parece reflejar las mismas buenas características. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_dists('1stFlrSF') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de 'TotalBsmtSF' se tiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_dists('TotalBsmtSF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una relación menos lineal con un poco más de ruido pero una buena distribución en e dataset. Esta variable puede ser de interés pero esto se puede estudiar a posteriori. \n",
    "\n",
    "Finalmente para 'MasVnrArea', se tiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_dists('MasVnrArea', reg = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aprecia una distribución altamente concentrada y poco relacionada con la variable a predecir, a priori, se puede considerar como una variable de poco interés en el análisis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Estudie las siguientes proposiciones: \n",
    "\n",
    "    1.'OverallQual' y 'YearBuilt' parecen relacionadas con 'SalePrice'. \n",
    "    2. En el caso de 'OverallQual', esta relación es bastante débil.\n",
    "    3. En el caso de 'YearBuilt', esta relación es bastante débil.\n",
    "    4. Los gráficos de caja para 'OverallQual contra  'SalePrice' muestran cierta linealidad con respecto a 'SalePrice'.\n",
    "\n",
    "2. Estudie la distribución univariada de 'SalePrice', a continuación ejecute el test K^2 de D’Agostino usando `normaltest` del módulo `stats` de SciPy. Compare para una significancia de 5%. ¿ Se puede tratar esta variable como distribuida de manera normal, tomando en cuenta su comportamiento estadístico?\n",
    "\n",
    "3. Las distribuciones de 'TotalBsmtSF' y '1stFlrSF' parecen bastante similares, más aún sus relaciones con 'SalePrice' comparten una tendencia. Ejecute el [test de Kolmogorov-Smirnov ](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_2samp.html) por medio de `ks_2samp` para explorar la hipótesis:\n",
    "\n",
    " ```'TotalBsmtSF' y '1stFlrSF' vienen de la misma distribución```\n",
    " \n",
    "4. Estudie algunos estadísticos de interés según el tipo de dato.\n",
    "    \n",
    "    1. Para las variables numéricas estudie promedios, desviaciones estándar y rangos intercuartílicos. Utilice los rangos calculados para tener una idea del porcentaje de valores fuera de tales rangos por columna. \n",
    "    \n",
    "    2. Para variables catégoricas calcule frecuencias, proporciones y modas. Utilice lo anterior para obtener alguna idea de la variabilidad de los datos.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perfilamiento Bivariado\n",
    "\n",
    "Basándose en el perfilamiento anterior, es de utilidad observar relaciones entre variables de interés. Para esto se pueden emplear visualizaciones a pares. \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se selecciona un conjunto de variables de interés y se investigan sus relaciones bivariadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se genera una función auxiliar\n",
    "\n",
    "def indexer(cols, t_c = df.columns):\n",
    "    '''Genera columnas multinivel a partir de nombres de columna planos.'''\n",
    "    \n",
    "    set_to_tuple = set(*[cols])\n",
    "\n",
    "    tuples = [\n",
    "        i for i in t_c if set_to_tuple.intersection(set(i))\n",
    "    ]\n",
    "    \n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se selecciona un conjunto de variables a examinar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest = [\n",
    "    'SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF',\n",
    "    'FullBath', 'YearBuilt'\n",
    "]\n",
    "\n",
    "idxs = indexer(interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[idxs].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a observar el comportamiento bivariado de las columnas seleccionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pone SalePrice al final de la lista\n",
    "idxs.sort()\n",
    "idxs.remove(('numeric', 'SalePrice'))\n",
    "idxs.append(('numeric', 'SalePrice'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Seaborn presenta problemas para multi indices en columnas, se \n",
    "procede a eliminar el nivel exterior y a obtener la visualización\n",
    "correspondiente.\n",
    "'''\n",
    "data = df.reindex(idxs, axis=1).droplevel(0,axis=1)\n",
    "sns.pairplot(data = data, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La última fila de la visualización anterior entrega una idea de la relación entre 'SalePrice' y las demás variables de interés. Dentro de estas relaciones, se observan ciertos comportamientos lineales y en particular para 'OveralQuall' y 'YearBuilt' se observa cierta exponencialidad. Dentro de las interacciones entre variables, se observa que 'GrLivArea' y 'TotalBsmtSf' se comportan de manera similar contra 'OverllQuall', esperandose cierta tendencia creciente en ambos casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los análsis iniciales basados en visualizaciones sirven para comprender a grandes rasgos la estructura del dataset. Este tipo de exploración debe ser acompañada de tests estadísticos como los vistos en los ejercicios anteriores. En el caso del perfilamiento bivariado se puede usar una técnica mixta, basada en el análisis de las correlaciones.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se construye una matriz de correlaciones y se visualiza para todo el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se muestran las dos variables más correlacionadas (positivamente) con 'SalePrice'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = indexer(['SalePrice'])\n",
    "corrmat[col].nlargest(3,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a correlación negativa, no se ven relaciones lineales inversas de mayor fortaleza "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat[col].nsmallest(3,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a visualizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Se inserta 'SalePrice' como primera fila x columna de la matriz de correlacion\n",
    "'''\n",
    "\n",
    "unsorted = list(corrmat.columns)\n",
    "unsorted.remove(*col)\n",
    "unsorted.insert(0, *col)\n",
    "\n",
    "sortd = pd.MultiIndex.from_tuples(unsorted)\n",
    "corrmat = corrmat.reindex(index = sortd, columns = sortd)\n",
    "'''\n",
    "Dado lo anterior, se ajusta el anchor de colores con maximo en .9\n",
    "y -0.5, para tener una perspectiva entorno a los valores maximos \n",
    "de correlacion (negativa y positiva)\n",
    "'''\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[16, 14])\n",
    "\n",
    "sns.heatmap(corrmat, vmin=-.5, vmax=.9, linewidths=.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según el esquema de valores, se buscan los puntos más claros y más oscuros fuera de la diagonal. En primera instancia, las variables  'TotalBsmtSF' y '1stFlrSF' parece bastante correlacionadas, lo mismo ocurre con la variables 'GarageCars' y 'GarageArea', esto puede indicar multicolinearidad que implica información duplicada o relacionada de manera trivial en el dataset. \n",
    "\n",
    "Las correlaciones con 'SalePrice' deben ser analizadas con más detenimiento, aquí se ve que  'GrLivArea', 'TotalBsmtSF', y 'OverallQual' juegan un papel preponderante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Obtenga las 15 correlaciones más altas (positiva o negativa) con 'SalePrice'. Reindexe la matriz de correlaciones, de manera tal que contenga 1's en la diagonal y 'SalePrice' sea la primera fila - columna. \n",
    "\n",
    "2. Muestre los coeficiente de correlación dentro de cada casilla del gráfico de correlaciones. Utilice esa información en conjunción con el perfilamiento univariado para filtrar variables de interés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las correlaciones pueden ser interpretadas con datos mixtos pero se recomienda analizar sus valores cuando se trabaja con valores continuos (comparación variable continua vs continua). Para analizar valores categóricos (categórico vs categórico) existen herramientas especializadas una de ellas es por medio de tablas de dos tratamientos o de contingencia (2 way tables). \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se construye una tabla para analizar 'OverallQual' vs 'GarageCars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_compare =['OverallQual','GarageCars']\n",
    "data_cat = df['categorical']\n",
    "\n",
    "kwargs = {'index': data_cat[to_compare[0]], 'columns': data_cat[to_compare[1]]}\n",
    "\n",
    "# Se construye la tabla\n",
    "tabla = pd.crosstab(**kwargs, margins=True, margins_name='Total')\n",
    "tabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso anterior, la función `pd.crosstab(**kwargs, margins=True, margins_name='Total')` es equivalente a\n",
    "\n",
    "```python\n",
    "data_cat.pivot_table(**kwargs, values = 'OverallQual',aggfunc='count', fill_value=0)\n",
    "```\n",
    "y permite calcular el numero de ocurrencias de una variable para cada una de sus categorías en comparación con los valores de otra variable. Se añaden los totales como margenes de la tabla. En este caso, podemos deducir las interacciones entre las variables, de manera similar como actúa la correlación en variables continuas. \n",
    "\n",
    "Para el caso de  'OverallQual' y 'GarageCars' vemos que tienden a acumularse dentro de una rango reducido, se puede concluir que a medida que 'OverallQual' crece entre 4 y 6, aparece un aumento considerable en la categoría 'GarageCars' hasta que esta última llega al valor 2, valores superiores paracieran ser independientes de 'OverallQual'. \n",
    "\n",
    "**Ejercicios**\n",
    "\n",
    "1. Compare variables categóricas usando este método, ¿ se puede encontrar alguna relación entre categórias?\n",
    "\n",
    "2. Es posible aplicar este método para comparar variables categóricas y continuas, para esto se necesita categorizar la variable continua objetivo. Categoríce la variable 'SalePrice' en 5 tramos y compare con 'OverallQual' ¿Se observa alguna tendencia?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de comparar variables categóricas es por medio de un test $\\chi^2$. Este permite obtener un indicador de significancia estadística entre variables. Se basa en una tabla de contingencia y proporciona la probabilidad de que dos variables categóricas sean independientes basádandose en el estádistico $\\chi^2$, entrega también un arreglo con frecuencias esperadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Se debe trabajar la tabla sin margenes\n",
    "tabla = pd.crosstab(**kwargs, margins=False)\n",
    "\n",
    "chi2, p, dof, ex =chi2_contingency(tabla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tabla de frecuencias esperadas se puede interpretar de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_freq = pd.DataFrame(ex, index=range(1,11))\n",
    "expected_freq.index.name = 'OverallQual'\n",
    "expected_freq.columns.name = 'GarageCars'\n",
    "expected_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí, la frecuencia esperada para la la categoría 1 'OverallQual' de estar en la categoría 0 de 'GarageCars' es 0.11. Se puede decir que esta configuración es muy poco probable en comparación a otras como pertenecer a la categoría 6 de 'OverallQual' y 2 de 'GarageCars'. Este tipo de tablas permite clasificar las relaciones entre variables categóricas y obtener *insights* sobre las dinámicas que el dataset refleja. \n",
    "\n",
    "El valor $p$ entregado por el cáculo corresponde a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si para este test llamamos $\\alpha$ al valor de significancia, se puede resumir:\n",
    "\n",
    "1. Si $p > \\alpha$ no hay evidencia para rechazar la hipótesis nula por lo que se pueden considerar independientes.\n",
    "\n",
    "2. Si $p \\leq \\alpha$ hay evidencia para rechazar la hipótesis nula por lo que se puede decir que existe una dependencia estadística entre las variables. \n",
    "\n",
    "\n",
    "Para una significancia del 5% , hay evidencia para rechazar la hipótesis de independencia entre 'OverallQual' y 'GarageCars', luego puede existir un factor latente que las relaciona (¿será 'SalePrice'?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p <= 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como consideración general, para que este test sea consistente estadísticamente, se deben observar frecuencias (esperadas y observadas) mayores a 5. \n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "1. Utilice el gráfico de correlaciones para escojer dos variables categóricas de interés. Verifique si existen relaciones estadísticas entre ellas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, para comparar variables numéricas y categóricas, es posible utilizar técnicas especializadas como lo son los tests Z y T. Estos tests se utilizan de manera simultanea con gráficos de caja (o violín), donde cada caja representa una categoría. \n",
    "\n",
    "Tanto el test Z como el T permiten verificar si las medias de dos grupos son estadísticamente diferentes entre si. aquí, el estadístico Z se define por\n",
    "\n",
    "\\begin{equation}z=\\frac{\\left|\\bar{x}_{1}-\\bar{x}_{2}\\right|}{\\sqrt{\\frac{s_{1}^{2}}{n_{1}}+\\frac{s_{2}^{2}}{n_{2}}}}\\end{equation}\n",
    "\n",
    "Si su probabilidad asociada es pequeña, entonces **la diferencia de las medias** es significativa. \n",
    "\n",
    "Por otra parte, el estadístico T es más robusto a tamaños de observaciones pequeños (menores que 30 por ejemplo), este viene dado por \n",
    "\n",
    "\\begin{equation}\n",
    "t=\\frac{\\bar{X}_{1}-\\bar{X}_{2}}{\\sqrt{S^{2}\\left(\\frac{1}{N_{1}}+\\frac{1}{N_{2}}\\right)}}\n",
    "\\end{equation}\n",
    "\n",
    "Donde \n",
    "\n",
    "\\begin{equation}\n",
    "S^{2}=\\frac{\\left(N_{1}-1\\right) S_{1}^{2}+\\left(N_{2}-1\\right) S_{2}^{2}}{N_{1}+N_{2}-2}\n",
    "\\end{equation}\n",
    "\n",
    "Aquí , $\\bar{X}_{1}, \\bar{X}_{2}$ son las medias, $S_{1}^{2}, S_{2}^{2}$ varianzas y $N_1$ , $N_2$ los totales de cada grupo a testear. \n",
    "\n",
    "**Ejercicios**\n",
    "\n",
    "1. Utilice el test de independencia $t$ (o 2 - sample $t$-test) para comparar 2 variables continuas de interés. [*Hint*](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html)\n",
    "\n",
    "2. Observe que el caso categórico vs continuo, cada categoría representa un grupo de valores continuos asociados. Por ejemplo, si la variable categórica `A` tiene las categorías `c_1` y `c_2`, al compararla con la variable continua `B`, es necesario agrupar los valores de `B` para `c_1` y para `c_2` para luego estudiar su independencia. Utilice el test anterior para medir independencias de grupos entre una variable categórica vs 'SalePrice'. **Obs**: La variable categórica debe ser bivariada.\n",
    "\n",
    "3. Utilice el test Z con las variables anteriores y compare. ¿Qué restricciones extra posee este test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se puede hacer uso de un test F o ANOVA. Este test permite comparar más de una media al mismo tiempo, una manera simple de aplicar este test consiste en método conocido como **one way ANOVA**, aquí, se testea si más de 2 grupos son similares basados en sus medias. En este caso, la hipótesis nula es \n",
    "\n",
    "`No hay diferencia significativa entre los grupos`\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se selecciona la variable 'GarageCars' y se compara con 'SalePrice'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = indexer(['SalePrice','GarageCars'])\n",
    "grouped = df[idx].groupby(idx[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la variable 'GarageCars' se distinguen 5 categorías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[idx[1]].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a partir de la agrupación anterior, se forman entonces 5 grupos de valores para 'SalePrice'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grouped.groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtienen los grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_groups = len(grouped.groups)\n",
    "groups = [grouped.get_group(i) for i in range(total_groups)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se muestra el grupo correspondiente a la categoría 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se limpia el formato de cada grupo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_cleaner(group):\n",
    "    ''' Limpia un grupo.\n",
    "    Reconoce la categoria del grupo, en la posicion [:,1], \n",
    "    guarda ese nombre y elimina la columna de categoria, \n",
    "    posteriormente renombra la columna.\n",
    "    \n",
    "    Args:\n",
    "    ----------\n",
    "    \n",
    "    group: pandas Groupby object\n",
    "          Recibe una agrupacion para categorias\n",
    "          \n",
    "    Returns:\n",
    "    ----------\n",
    "        pandas Grppuby object\n",
    "        Entrega el grupo ordenado.\n",
    "    '''\n",
    "    group_0 = group.copy()\n",
    "    name = group_0.iloc[0,1]\n",
    "    group_0.drop(indexer(['GarageCars']), axis=1, inplace=True)\n",
    "    group_0.columns  = ('cat_{}'.format(name),)\n",
    "    \n",
    "    return group_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se procede a limpiar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_to_test = list(map(group_cleaner, groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se muestra el grupo correspondiente a la categoria 0 post limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_to_test[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a testear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "F,p = f_oneway(*groups_to_test)\n",
    "\n",
    "print('Estadistico F:',F)\n",
    "print('p valor :', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "probando para una significancia del 5% se tiene hay evidencia para rechazar la hipótesis nula y por tanto hay una diferencia significativa entre los grupos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "p <= alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Compruebe el resultado del test ANOVA anterior con un analísis visual por medio de gráficos de violín."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El problema de los datos faltantes \n",
    "\n",
    "Los métodos estándar de manejo de datos han sido desarrollados para para analizar arreglos tabulares. Por lo general las filas de tal arreglo representan observaciones y las columnas sus características asociadas. Cada entrada en este arreglo puede ser modelada como un número, siendo este ligado a un proceso subyacente continuo o discreto. Para comprender tal proceso, es de utilidad sumarizar y observar los valores faltantes con el fin de obtener patrones y seleccionar estrategias para tratarlos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración de valores faltantes\n",
    "\n",
    "Cuando los datos faltantes se encuentran en variables que no son de interés, se pueden obviar y pasar a trabajar directamente en ingeniería de *features* e implementación de modelos de aprendizaje automático, por tal motivo, el análisis exploratorio y visualización se considera como primer paso en un procedimiento de análsis de datos. Sin embargo, una exploración preliminar de valores faltantes puede ser útil en conjunto con los perfilamientos visuales y estadísticos realizados.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Observe que en los perfilamientos anteriores, las variables categóricas:\n",
    "\n",
    "```python\n",
    "var_missing = ['GarageQual', 'GarageCond', 'BsmtFinType1','BsmtCond', 'GarageFinish', 'Fence', 'BsmtExposure',  'BsmtQual', 'MiscFeature', 'GarageType', 'Electrical', 'FireplaceQu', 'BsmtFinType2','MasVnrType']\n",
    "```\n",
    "Parecen no tener valores faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_missing = [\n",
    "    'GarageQual', 'GarageCond', 'BsmtFinType1', 'BsmtCond', 'GarageFinish',\n",
    "    'Fence', 'BsmtExposure', 'BsmtQual', 'MiscFeature', 'GarageType',\n",
    "    'Electrical', 'FireplaceQu', 'BsmtFinType2', 'MasVnrType'\n",
    "]\n",
    "\n",
    "var_missing = indexer(var_missing)\n",
    "\n",
    "df[var_missing].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, basta observar las columnas para comprender que tales variables si poseen valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[var_missing].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tal motivo es necesario realizar una exploración inicial de los datos faltantes en conjunción con los análisis de distribución iniciales. \n",
    "\n",
    "**Ejercicios**\n",
    "\n",
    "1. Estudie la distribución de los valores faltantes en las variables numéricas.\n",
    "\n",
    "2. Considerando que para las variables categóricas las variables con valor 'nan' son consideradas como una nueva categoría. ¿Se ven afectados los análisis anteriores sobre sus distribuciones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo**\n",
    "\n",
    "Para estudiar en mayor profundidad la distribución de los valores faltantes, se procede a transformarlos en formato `np.nan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('nan',np.nan, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En términos generales, los valores perdidos de este dataset se encuentran relativamente limpios pues están estandarizados con la categoría 'nan'.\n",
    "\n",
    "\n",
    "Dado que sumarizar valores faltantes genera una estructura de datos, vale la pena explorarla visualmente, para facilitar tal tarea, existe la librería `missingno`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las visualizaciones de generadas por medio de esta librería pueden ser utilizadas para discutir el problema de valores faltantes y generan una estrategia para su tratamiento.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se genera una visualización sobre la distribución de valores perdidos en el dataset. En primer lugar, se confirma que la conversión 'nan' $\\mapsto$ `np.nan` sea reconocida en el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que en efecto aparecen los valores faltantes antes ignorados. En este apartado se observa que dentro de las variables categóricas se encuntra la mayor cantidad de información perdida. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().nlargest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mediante la libreria `missingno` es posible ver el panorama completo de los valores faltanes en el dataset de manera sencilla "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = [15, 10])\n",
    "msno.matrix(df,ax = ax, sparkline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Genere un subconjunto con las 10 columnas con mayor información faltante y genere el gráfico anterior sin usar un objeto `axes` y con la opción `sparkline=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta visualización muestra que exiten columnas practicamente sin información, según la agregación anterior, estas corresponden a 'PoolQC', 'MiscFeature' y 'Alley'. \n",
    "\n",
    "Por medio de correlaciones entre valores faltantes, es posible obtener un análisis bivariado análogo al anteriormente generado. Para ello se puede utilizar un mapa de calor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = [15, 10])\n",
    "msno.heatmap(df, ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este gráfico muestra correlaciones de nulidad entre pares de variables, estas varian desde -1 a 1, donde -1 significa que las variables son excluyentes, es decir, la aparición de una hace que la otra este ausente. Por otra parte el valor 1 corresponde inclusión, esto quiere decir, que la aparición de una hace que la otra aparezca. Valores cercanos a 0 (sin valor numérico en el gráfico) indican ausencia de relación de nulidad entre las variables.\n",
    "\n",
    "En el gráfico recien generado, no se observen relaciones de nulidad negativa, por otra parte, existen variables fuertemente relacionadas en cuanto a su información como lo son 'MasVnrType' y 'MasVnrArea', el comportamiento general es que la información esta fuertemente relacionada (en el sentido de inclusión de información) o simplemente no lo está. \n",
    "\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "1. El gráfico de correlaciones de nulidad permite tener una idea de como se relaciona la información faltante en pares de variables. Para comparar más de dos variables es posible utilizar un *dendograma*. Utilice las 20 variables con mayor cantidad de valores faltanes visualice su dendograma por medio de `msno.dendogram`. Interprete los resultados.[*Hint*](https://github.com/ResidentMario/missingno)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Una perspectiva teórica\n",
    "\n",
    "Para dar un contexto teórico al problema de valores faltantes, se define la matriz de datos $Y=\\left(y_{i j}\\right)$, la cual representa un arreglo rectangular de datos. Para cada elemento de esa matriz, se asocia una variable indicadora, que da lugar a una *matriz indicadora de información faltante* definida por  \n",
    "$R=\\left(r_{i j}\\right)$. \n",
    "\n",
    "Por simplicidad, se puede asumir que las filas $\\left(y_{i}, r_{i}\\right)$ son i.i.d sobre $i$. Es posible así, modelar la existencia de un proceso (o fenómeno) que causa la perdida de información sobre $Y$ por medio de una distribución condicional de $R$  dado $Y$, denotada por $\\mathcal{P} \\left(R \\mid Y_{obs}, Y_{mis} ,\\phi \\right)$, donde $\\phi$ denota un conjunto de parámetros que modelan el proceso de perdida de información e $Y_{obs}$, $Y_{mis}$, denotan aquellas entradas de la matriz de datos $Y$ con información observada y faltante respectivamente. Si para tal proceso no se aprecia una relación con los valores faltantes en $Y$, es decir, \n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{eq:1}\n",
    "\\label{eq:1}\n",
    "\\mathcal{P} \\left(R \\mid Y_{obs}, Y_{mis} ,\\phi \\right)= \n",
    "\\mathcal{P} \\left(R \\mid \\phi \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Se dice entonces que el mecanismo de perdida de información es del tipo *información faltante completamente aleatoria* o MCAR (missing completely at random). Por otra parte, cuando la probabilidad de la información faltante en $Y$ se relaciona con variables observadas, así, $R$ depende de $Y_{obs}$ pero no de $Y_{mis}$, por lo que la distribución pasa a ser\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{eq:2}\n",
    "\\label{eq:2}\n",
    "\\mathcal{P} \\left(R \\mid Y_{obs}, Y_{mis} ,\\phi \\right)=\n",
    "\\mathcal{P} \\left(R \\mid Y_{obs}, \\phi \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Entonces, el proceso de perdida de información se denomina como, *información faltante aleatoria* MAR (missing at random). El proceso es llamado *información faltante no aleatoria* MNAR, si la distribución de $R$ depende de las componentes faltantes de $Y_{mis}$, es decir, la ecuación inicial ($\\ref{eq:2}$) no se cumple para algunas filas de $Y$ y algunos valores de las componentes faltantes.\n",
    "\n",
    "En los métodos discutidos en esta cátedra $R$ e $Y$ serán modelados por medio de distribuciones conjuntas, es decir, son tratadas como variables aleatorias. \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "La estructura de datos con información faltante más simple, ocurre en el caso univariado. Acá, $Y$ y $R$ son vectores, luego \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{P}(Y=y, R=m | \\theta, \\phi)=\\prod_{i=1}^{n} f_{Y}\\left(y_{i} | \\theta\\right) \\prod_{i=1}^{n} f_{R \\mid Y} \\left(r_{i} | y_{i}, \\phi\\right)\n",
    "\\end{equation}\n",
    "\n",
    "donde $f_{Y}\\left(y_{i} | \\theta\\right)$ denota la densidad de la componente $i$-sima de $Y$, $y_{i}$, parametrizada por $\\theta$, y $ f_{R \\mid Y} \\left(r_{i} | y_{i}, \\phi\\right)$ es la densidad de una variable aleatoria Bernoulli para el indicador binario $r_{i}$, con probabilidad $\\mathcal{Pr}\\left(r_{i}=1 | y_{i}, \\phi\\right)$ para $y_{i}$ valor faltante. Si el proceso de perdida de información es independiente de $Y$, es decir, $\\mathcal{Pr}\\left(r_{i} = 1 | y_{i}, \\phi\\right)=\\phi$ , constante que no depende de $y_{i}$, entonces el proceso de perdida de información es MCAR. Si tal mecanismo depende de $y_{i}$, entonces es MNAR pues pasa a depender de los valores perdidios de $y_{i}$.\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "Si se supone que $Y$ es una variable $n$ dimensional con posibles valores faltantes, $R$ es el indicador de pérdida de información para $Y$ y $X$ es una variable  $n$ dimensional relacionada al mismo dataset pero con valores completamente observados. En este contexto, si el dataset consiste de $n$ observaciones, donde para $r < n$  fijo, se tiene que $i=1, \\ldots, r$ $X_i$ e $Y_i$ son observados, mientras que para $j = r+1, \\ldots, n$ se tiene $X_j$ observado pero $Y_j$ faltante. \n",
    "\n",
    "1. Si para $i = 1, \\ldots, n$ se asume que $y_i$ es independiente de $r_i$ dado $x_i$. Aplique el supuesto MAR sobre este conjunto de datos y deduzca una expresión para $\\mathcal{P}(R \\mid X, Y , \\phi)$ que no dependa de $Y$. \n",
    "\n",
    "2. Utilice la expresión anterior para deducir que $R$ e $Y$ son independientes dado $X$. \n",
    "\n",
    "3. Utilice lo anterior para deducir que la distribución condicional de $Y$ dado $X$ y $R$ no depende de $R$. \n",
    "\n",
    "4. Deduzca que la distribución condicional de $Y$ dado $X$ puede ser estimada para componentes con $Y$ observado ($r_i = 0$) para luego ser utilizada para predecir valores faltantes de $Y$ ($r_i = 1$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprender el significado de lo anterior, considere el siguiente ejemplo\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se realiza un experimento en cual se examinan ciertas variables relacionadas a la salud, dentro de estas variables, se consideran algunos comportamientos como *beber alcohol*, *fumar*, *consumo de drogas* y *actividad sexual*. El experimento se realiza sobre una población menor de 18 años, luego, por regulaciones gubernamentales, no se pueden realizar preguntas sobre actividad sexual a menores de 14 años. Observe\n",
    "\n",
    "1. La variable asociada al comportamiento sexual cumple la hipótesis MAR, pues en efecto, al observar la variable de edad (variable observada $Y_{obs}$), se puede comprender la falta de información subyacente.\n",
    "\n",
    "2. Por otra parte, suponga que la variable de edad y comportamiento sexual están altamente correlacionadas. Asuma además que en el estudio existe un valor de *índice de salud* asociado a cada participante. Los investigadores del experimento deciden utilizar regresión lineal sobre el valor del *índice de salud* para estudiar el peso de cada variable en el modelo. Al hacer esto, eliminan la variable de edad por producir multicolinearidad. \n",
    "Si los investigadores relacionan los valores del *índice de salud* con la probabilidad de que la variable sexual tenga valores faltantes, se está en la hipótesis MNAR, pues la variable edad pasa a ser no observada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interludio: Introducción a Scikit Learn\n",
    "\n",
    "Dentro la extensa variedad de librerías para el manejo de datos e implementacion de modelos, se encuentra [Scikit-learn](https://scikit-learn.org/stable/). Está librería es estándar en flujos de trabajo con datos y provee herramientas clásicas de aprendizaje automático implemententadas de manera eficiente. Sus APIs permite generar código limpio y está provista de una extensa documentación.\n",
    "\n",
    "\n",
    "Una gran ventaja de Scikit-learn consiste en su estructura transversal de clases, construida sobre una lista simple de APIs y patrones de diseño. Las Apis más representativas son:\n",
    "\n",
    "* *transformers*: Permite transformar datos input antes de utilizar algoritmos de aprendizaje sobre ellos. Con esto, se pueden realizar imputaciones de valores faltantes, estandrización de variables, escalamientos y seleccion de caracterísiticas por medio de algoritmos especializados.\n",
    "\n",
    "* *estimators*: La interfaz de esimadores es uno de los componentes más importantes. Los algoritmos de aprendizaje automático están implementados aquí. El proceso de aprendizaje de tales algoritmos es manejado según la inicialización de un objeto alojado en el módulo, esto consiste proporcionar los hiperparámetros que definen el modelo a entrenar, antes de proporcionar datos. El segundo paso corresponde a utilizar el método `.fit()` sobre los datos a utilizar, aquí se aprenden los parámetros y se encapsulan sus valores como atributos públicos para fácil inspección. \n",
    "\n",
    "* *predictors*: Esta interfaz permite generar predicciones usando un estimador previamente entrenado sobre datos (a priori) desconocidos. \n",
    "\n",
    "El método usual de importación se basa en seleccionar un submódulo de la librería indicando (de manera opcional) el objeto que se utilizará, por ejemplo, si se desea utilizar el escalador de datos Min-Máx del submódulo `preprocessing`, se haría de la manera usual, por medio de:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "```\n",
    "\n",
    "**Obs**: No se recomienda importar la librería completa `import sklearn as sk` pues su estructura de submódulos es suficientemente grande, como para considerar cada uno como una librería. \n",
    "\n",
    "\n",
    "A lo largo del curso se estudiarán distintos componentes de esta librería, durante la siguiente sección nos centraremos en los móduloz `preprocessing`, `compose` y `pipeline`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento de datos con Scikit-Learn\n",
    "\n",
    "El módulo `sklearn.preprocessing` entrega funciones de manejo de datos ampliamente utilizadas en la práctica. Hace uso de *transformers* con lo que facilita la transición de datos 'crudos' a un formato estándar para el entrenamiento de algoritmos. \n",
    "\n",
    "Dentro de tales algoritmos encontramos:\n",
    "\n",
    "#### Estandarización\n",
    "\n",
    "La estándarización es el primer tipo de transformación a tener presente, esto pues, una gran cantidad de algoritmos de aprendizaje automático / estadístico, asumen que los datos a operar se encuentran distribuidos de manera normal. \n",
    "\n",
    "En la práctica, se ignora la forma de la distribución a trabjar y simplemente e transforma removiendo la media y escalando por la desviación estándar.\n",
    "\n",
    "**Obs**:Esto **no** es recomendaddo si el histograma de la variable objetivo dista mucho de ser gaussiana, para tal caso se revisarán transformaciones alternativas.\n",
    "\n",
    "El objeto `StandarScaler` permite estandarizar datos.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se generan 3 distribuiones a estandarizar:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = pd.DataFrame({\n",
    "    'x1': np.random.normal(0, 2, 1460),\n",
    "    'x2': np.random.normal(10, 4, 1460),\n",
    "    'x3': np.random.normal(-15, 6, 1460)\n",
    "})\n",
    "df_0.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importa el escalador y se inicializa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se generan datos escalados, el método `.fit_transform` es transversal en los transformadores de `preprocessing`, lo que hace es obtener los parámetros de transformación de los datos proporcionados y transformar (todo en un paso). Es una abreviación de los métodos `.fit` para obtención de parámetros y `.transform()` para aplicar la transformación a nuevos datos. \n",
    "\n",
    "Por lo anterior, al aplicar `.fit_transform()` se obtienen los parámetros de media `.mean_` y desviación estándar `.scale_` para cada columna del dataframe operado. Observe que tales atributos del objeto tipo `StandardScaler` son públicos, observe además que es posible crear objetos que hereden de tal clase y por tanto, anular sus métodos utilizando métodos propios, recuerde que Python soporta Duck typing.\n",
    "\n",
    "Se obtienen los parámetros y se transforman las columnas generadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = scaler.fit_transform(df_0)\n",
    "df_1 = pd.DataFrame(df_1, columns=['x1','x2','x3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se implementa la función `scaler_test` que simplifica el proceso anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler_test(df, scaler, dat=False):\n",
    "    ''' Simplifica el proceso de testear transformadores de datos.'''\n",
    "    \n",
    "    data = df.copy()\n",
    "    data = scaler.fit_transform(data)\n",
    "    data = pd.DataFrame(data, columns=df.columns)\n",
    "    data.plot.kde()\n",
    "\n",
    "    if dat:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escalamiento mínimo-máximo\n",
    "\n",
    "Una buena alternativa al método anterior, es el escalamiento por rango, este tiene la forma:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{x_{i} - \\min(x)}{\\max (x)-\\min (x)}\n",
    "\\end{equation}\n",
    "\n",
    "para $x$ columna a tratar, $x_i$ elemento a transformar. Esta transformación permite hacer que los datos se muevan entre 0 y 1 y puede ser utilizado y la distribución de los datos a tratar no cumple la hipotesis de normalidad (recordar test Z). Observe que este transformador se ve afectado por la presencia de outliers. \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se generan 3 distribuciones y se estudia la transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = pd.DataFrame({\n",
    "    'x1': np.random.beta(8, 2, 1460)*100,\n",
    "    'x2': np.random.chisquare(10, 1460),\n",
    "    'x3': np.random.normal(50, 3, 1460)\n",
    "})\n",
    "\n",
    "df_0.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importa el objeto `MinMaxScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se prueba la transformación,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data = scaler_test(df_0, scaler, dat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comprueban minimos y máximos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data min:', data.min())\n",
    "print('data max:', data.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se desea escalar por rengo, la mejor práctica es comprender los mínimos y máximos *absolutos* para cada columna. Esto se refiere, a las cotas superiores e inferiores que posee la columna **por definición**, a modo de ejemplo, considere un dataframe con las notas de una asginatura donde se enzeña análisis de datos, se sabe que la nota máxima en cierto ítem se codifica en una columna y su máximo es en efecto es 7.0, sin embargo el mínimo en dicha columna es 1.5, que es distinto al mínimo natural para dicho item que es 1.0. Esto puede acarrear problemas con datos nuevos, sobretodo si aparece una nota inferior a 1.5. \n",
    "\n",
    "**Ejercicios**\n",
    "\n",
    "1. Investigue los parámetros que se deben usar para proporcionar escalamiento por rango con valores máximos y mínimos proporcionados explícitamente. \n",
    "\n",
    "2. Estudie el transformador `MaxAbsScaler`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación robusta\n",
    "\n",
    "Cuando se trabaja con columnas que poseen valores fuera de rango (outliers) las transformaciones anteriores pueden fallar. En este caso, se recomienda utilizar una transformación de la forma\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{x_i - Q_1(x)}{IQR(x)}\n",
    "\\end{equation}\n",
    "\n",
    "Donde $IQR = Q_3(x) - Q_1(x)$ es el rango intercuartílico de la columna $x$. \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se generan datos con outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'x1': np.concatenate([np.random.normal(20, 1, 1460), np.random.normal(1, 1, 25)]),\n",
    "    'x2': np.concatenate([np.random.normal(-10, 1, 1460), np.random.normal(50, 1, 25)]),\n",
    "})\n",
    "df.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importa el objeto `RobustScaler` y se aplica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "scaler_test(df, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar como los datos son centrados pero se mantienen los outliers generados.\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "1. Sea $k(x,y)$ un kernel definido por $\\langle \\phi(x), \\phi(y) \\rangle_{\\mathcal{H}}$, donde $\\phi: \\mathcal{X} \\rightarrow \\mathcal{H}$ es una fución que opera elementos entre espacio de datos $\\mathcal{X}$ y un espacio de Hilbert $\\mathcal{H}$. Se sabe por el *truco del kernel* que algoritmos kernelizados obvian el tratamiento de *features* no lineales representadas por transformaciones del tipo $\\phi(\\cdot)$. Sin embargo, es posible centrar datos en el espacio de carácteristicas $\\mathcal{H}$ utilizando la matriz de Gramm asociada a $k(x,y)$. Para efectuar tal procedimiento, estudie la clase `KernelCenterer`. ¿Que ventaja tiene usar este método?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapeo a distribuciones gaussianas \n",
    "\n",
    "Como mencionó anteriormente, no siempre se cumple la hipótesis de normalidad en las columnas de un dataset, en tal caso, no es una buena idea estandarizar los datos pues puede llevar a problemas al momento de operar con algoritmos que requieren normalidad en su formulación. Existe una familia de transformaciones paramétrica que busca aproximar una distribución arbitraria a una gaussiana, se accede a este tipo de transformaciones por medio de la clase `PowerTransformer`, en esta clase se encuentran 2 transformaciones:\n",
    "\n",
    "* Yeo-Johnson dada por: \n",
    "\n",
    "\\begin{equation}\n",
    "x_{i}^{(\\lambda)}=\n",
    "\\begin{cases}\n",
    "\\left[\\left(x_{i} + 1\\right)^{\\lambda}-1 \\right] / \\lambda & \\text { si } \\lambda \\neq 0, x_{i} \\geq 0 \\\\\n",
    "\\ln \\left(x_{i}+1\\right) & \\text { si } \\lambda=0, x_{i} \\geq 0 \\\\\n",
    "-\\left[\\left(-x_{i}+1\\right)^{2-\\lambda}-1\\right] /(2-\\lambda) & \\text { si } \\lambda \\neq 2, x_{i}<0 \\\\\n",
    "-\\ln \\left(-x_{i}+1\\right) & \\text { si } \\lambda=2, x_{i}<0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "* Box-Cox: Solo puede ser utilizada en datos extrictamente positivos. Viene dada por:\n",
    "\n",
    "\\begin{equation}\n",
    "x_{i}^{(\\lambda)} =\n",
    "\\begin{cases}\n",
    "\\frac{x_{i}^{\\lambda}-1}{\\lambda} & \\text { si } \\lambda \\neq 0 \\\\\n",
    "\\ln \\left(x_{i}\\right) & \\text { si } \\lambda=0\n",
    "\\end{cases}.\n",
    "\\end{equation}\n",
    "\n",
    "En ambos casos, el parámetro $\\lambda$ es estimado por máxima verosimilitud.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "La técnica de transformación por medio de potencias (power transform) permite estabilizar la varianza y ahcer que los datos se distribuyan de manera más similar a la distribución normal. Por lo general, se recomienda el uso de este tipo de transformaciones en datasets con pocas observaciones pues generalizan de manera rápida, además son fácilmente interpretables por medio del valor $\\lambda$. Se definen distintas distribuciones de datos y se implementan las transformaciones anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "n_sample = 1460\n",
    "\n",
    "transformer_bc = PowerTransformer(method='box-cox')\n",
    "transformer_yj = PowerTransformer(method='yeo-johnson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se genera un dataset con diferentes distribuciones por columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'x_lognormal': np.random.lognormal(size = n_sample,),\n",
    "    'x_chisq': np.random.chisquare(5, n_sample),\n",
    "    'x_weibull': np.random.weibull(30, n_sample),\n",
    "    'x_gaussian': np.random.normal(loc = 25, size = n_sample),\n",
    "    'x_uniform': np.random.uniform(0, 1, n_sample)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan las distribuciones univariadas para cada columna en la diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se transforman los datos usando los objetos anteriores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bc = transformer_bc.fit_transform(df)\n",
    "df_bc = pd.DataFrame(df_bc, columns=df.columns)\n",
    "\n",
    "df_yj = transformer_yj.fit_transform(df)\n",
    "df_yj = pd.DataFrame(df_yj, columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan los resultados para el método de Box - Cox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_bc, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan los resultados para Yeo-Johnson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_yj, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Obtenga los valores de lambda para cada uno de los de métodos revisados.\n",
    "\n",
    "2. El preprocesamiento por transformación de cuantiles es un método robusto que permite transformar una distribución de datos en una variable uniforme o normal. Permite el reducir el impacto de outliers. Investigue su formulación, ventajas y desventajas, aplique el transformer `QuantileTransformer` en los datos recientemente generados para observar su comportamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalización\n",
    "\n",
    "Otro método de transformación de datos es la normalización de estos. Esto conisite en un mapeo a la bola cerrada según una norma a elección. Para la norma 'l2' en el espacio euclidiano de 3 dimensiones se tiene el normalizador\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{x_{i}}{\\sqrt{x_{i}^{2}+y_{i}^{2}+z_{i}^{2}}}\n",
    "\\end{equation}\n",
    "\n",
    "**Ejemplo** \n",
    "\n",
    "Se estudia como opera este transformador de datos en una visualización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'x': np.random.randint(-50, 50, 1460),\n",
    "    'y': np.random.randint(-60, 60, 1460),\n",
    "    'z': np.random.randint(-70, 70, 1460)\n",
    "})\n",
    "\n",
    "# Permite generar graficos 3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=[12,9])\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter3D(df.x, df.y, df.z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se estudia el impacto de la transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "scaler = Normalizer()\n",
    "df_1 = scaler.fit_transform(df)\n",
    "df_1 = pd.DataFrame(df_1, columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente se visualiza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12,9])\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.scatter3D(df_1.x, df_1.y, df_1.z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codificación de varaibles categóricas\n",
    "\n",
    "Para el manejo de adecuado de variables categóricas,  se recomienda expresar sus valores en función de códigos númericos. El transformer `OrdinalEncoder` permite transformar características categóricas en códigos enteros (números enteros)\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se define una variable categórica y se preprocesa según `OrdinalEncoder`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv', index_col='Id')\n",
    "\n",
    "# Esta variable posee las categorias 'RL', 'RM', 'C (all)', 'FV' y 'RH'\n",
    "data = df['MSZoning'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se inicializa el codificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrena el codificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se le da forma a los datos\n",
    "X = data.values.reshape([-1,1])\n",
    "\n",
    "# Se entrena el codificador\n",
    "enc.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtienen las categorías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = data.unique().reshape([-1,1])\n",
    "\n",
    "print('Codigos:')\n",
    "enc.transform(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comprueban los códigos anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = pd.Series(enc.transform(X).flatten(), name = 'codes', index=data.index)\n",
    "data_codes = pd.concat([data,codes], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa por ejemplo que el código 2, corresponde a 'RH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_codes.groupby('codes').get_group(2).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codificación por variables dummy\n",
    "\n",
    "Un problema común con la códificación ordinal es que las variables pasan a ser consideradas continuas por algoritmos de machine learning (en especial por la API *estimators* de scikit-learn). Para evitar esto es posible convertir cada categoría en una columna por si sola y asignar un 1 cuando esté presente. \n",
    "\n",
    "Esto se puede llevar a cabo por medio del transformador ` OneHotEncoder`.\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "1. Utilice este transformador en los datos categóricos anteriores. Los resultados serán entregados en formato *sparse* por  lo que tendrá que hacer uso del método `.toarray()` de los arreglos de NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers definidos \n",
    "\n",
    "Si se desea realizar una transformación no estándar en los datos, es posible utilizar la clase `FunctionTransformer` y proporcionar una función de transformación.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se define un dataset y se opera con un transformador propio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'cat_1': np.random.randint(30,80,size = 5),\n",
    "    'cat_2':range(5),\n",
    "     })\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define un transformador propio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def mapping(x):\n",
    "    x.copy()\n",
    "    x['cat_1'] += 5\n",
    "    x['cat_2'] = x['cat_2'].apply(str) + '+ 1'\n",
    "    \n",
    "    return x\n",
    "\n",
    "# se inicializa el transformador\n",
    "trasformer = FunctionTransformer(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplica a los datos anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trasformer.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flujos de transformación con Pipelines y Compose\n",
    "\n",
    "Las transformaciones en un dataset son combinadas entre si, hasta obtener una versión ordenada de los datos, posteriormente, estas se combinan con estimadores para formar un flujo de trabajo *input-output*. En Sckit-Learn el flujo antes nombrado de denomina *composite estimator* y se construye por medio de objetos tipo `Pipeline`.\n",
    "\n",
    "\n",
    "Los objetos `Pipeline` se pueden utilizar para mezclar múltiples transformadores y estimadores generando un único tratamiento, su utilidad se expresa en la simplificación de etapas fijas en el tratamiento de datos, manteniendo un lenguaje sencillo e intuitivo. Se importan desde el módulo `pipeline` por medio de la convención\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "```\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se utiliza una pipeline para transformar datos. En primera instancia se define un dataset por medio de las variables 'MSZoning' y 'LotArea' del dataset 'HousePricing'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv',\n",
    "                 usecols=['Id', 'MSZoning', 'LotArea'],\n",
    "                 index_col='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre el dataset cargado, se efectúa una transformación ordinal sobre 'MSZoning', la variable 'LotArea' se escala según rango y se le aplica una transformación de Yeo-Johnson. El procedimiento mencionado, se lleva  a cabo mediante *pipelines*.\n",
    "\n",
    "En primer lugar, las series de pandas, se consideran objetos con dimensión del tipo `(n,)` al traspasarlos a formato NumPy. Para poder trabajar con transformers de sklearn, sobre series de pandas, podemos transormar la dimensión de la serie por meido dl método `.reshape([-1,1])` para que pase a ser considerada como arreglo de NumPy de dimensión `(n,1)`. Para automtizar tal proceso, se define la función `data_1d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_1d(df_column):\n",
    "    return df_column.values.reshape([-1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego se definen los transformadores que deseamos utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#reshape\n",
    "rs = FunctionTransformer(data_1d)\n",
    "\n",
    "#categorico\n",
    "ordinal = OrdinalEncoder()\n",
    "\n",
    "# Numericos\n",
    "numeric_1 =  MinMaxScaler() #minmax\n",
    "numeric_2 =  PowerTransformer(method='yeo-johnson') #yeo-johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se quisieran llevar a cabo los procesos de transformación, estos se harían de la forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Caso categorico'''\n",
    "\n",
    "data_cat = df.MSZoning.copy()\n",
    "data_cat = rs.fit_transform(data_cat)\n",
    "data_cat = ordinal.fit_transform(data_cat)\n",
    "data_cat = pd.Series(data_cat.flatten(), name = 'MSZoning')\n",
    "data_cat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Caso numerico'''\n",
    "\n",
    "data_num = df.LotArea.copy()\n",
    "data_num = rs.fit_transform(data_num)\n",
    "data_num = numeric_1.fit_transform(data_num)\n",
    "data_num = numeric_2.fit_transform(data_num)\n",
    "data_num = pd.Series(data_num.flatten(), name = 'LotArea')\n",
    "data_num.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La manera anterior es bastante clara de comprender, sin embargo, es redundante y repite muchos patrones de asiganción tediosos. Utilizando pipelines, es posible reducir la cantidad de código, sin reducir simpleza.\n",
    "\n",
    "Para definir la *pipeline* se debe definir una lista de tuplas, donde cada una posee un identificador (*nombre definido por el usuario*) y una operación a realizar sobre el dataset. en el caso de la variable 'MSZoning' se puede definir según el siguiente esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "cat_pipe = Pipeline([('reshape', rs), ('ordinal', OrdinalEncoder())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otra parte, para la variable 'MSZoning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipe = Pipeline([('reshape', rs), ('scaler', MinMaxScaler()),\n",
    "                     ('yeo-johnson', PowerTransformer(method='yeo-johnson'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los objetos tipo `Pipeline` permiten encadenar operaciones siguiendo el orden de la lsita de tuplas proporcionadas. En el caso de las pipelines proporcionadas, se efectúa primero (en ambas) la transformación `rs` que permite alterar la diemnsión de la series a operar. \n",
    "\n",
    "Si se desea solo obtener parámetros de transformación, se puede utilizar el método `.fit` sobre un objeto tipo `Pipeline`. Este actúa como un `map` aplicado a cada objeto transformador en cada tupla proporcioanada, de la misma forma, se pueden aplicar los métodos de transformación `transform` y de obteneción de parámetros con transformación directa `fit_transform`. \n",
    "\n",
    "En el caso anterior, esto corresponde a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe.fit(df.MSZoning)\n",
    "cat_pipe.transform(df.MSZoning)\n",
    "\n",
    "# Alternativamente cat_pipe.fit_transform(df.MSZoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipe.fit(df.LotArea)\n",
    "num_pipe.transform(df.LotArea)\n",
    "\n",
    "# Alternaitvamente num_pipe.fit_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pueden utilizar los identificadores de cada paso (atributo `.steps`) para comprender las transformaciones de un objeto tipo `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[pipe[0] for pipe in num_pipe.steps]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, se observa que las transformaciones numéricas corresponden a un reshape, luego a escalar los datos y finalmente a transformarlos según el método de Yeo-Johnson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Genere un trasformador personalizado que permita transformar los datos de una pipeline diseñana sobre arreglos unidimensionales en series de Pandas. La función a implementar debe recibir un arreglo y un nombre de columna, debe entregar una serie con los datos transformados cuyo nombre es el nombre de la columna procesada. Añada este último transformador a las pipelines `cat_pipe` y `num_pipe`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es muy frecuente, es que los datos sea heterogéneos por ejemplo, es normal encontrar datasets con variables ordinales, categoricas, y numéricas. Para utilizar *pipelines* en este contexto, se necesitaria definir una por cada variable, repitiendo varios componentes de código entre variables que son del mismo tipo, esto resulta en una redundancia excesiva que se puede atacar por medio \n",
    "de objetos tipo `ColumnTransformer`, miembros del módulo `compose`. Estos objetos permite separar flujos de preprocesamiento, permitiendo seleccionar por columna o grupos de columna dentro de una *pipeline*.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se utiliza un objeto tipo `ColumnTransformer` para tratar datos heterogéneos. En primera ligar se importan las transformaciones a realizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para trabajar en este ejemplo, se cargará la base 'Titanic', esta consiste en una base pequeña con características de pasajeros del Titanic y una variable categórica 1-0 que indica si sobreviveron o no al choque hundimiento. Se carga la base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/titanic_train.csv', index_col='PassengerId')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos las columnas en numéricas y categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['Age','Fare']\n",
    "cat_cols = ['Embarked','Sex','Pclass']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya sabemos llenar valores faltantes con `.fillna()`, una alternativa es el uso de objetos `SingleImputer` del módulo `impute`. Este tipo de objetos permiten llenar valores faltantes de acuerdo a un método de agregación sobre los valores con información completa en las columnas de un dataset. Se usará la estrategia de llenado 'constant' que es equivalente a `.fillna()` pues llena todos los datos faltantes con el mismo valor proporcionado. \n",
    "\n",
    "Se procede a inicializar el objeto `SimpleImputer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se generan pipelines de preprocesamiento sobre las variables categóricas. Esta pipeline cambia todos los valores faltantes por 'NA' y luego genera una codificación Dummy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='NA')),\n",
    "    ('encoding', OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las variables numéricas se genera la siguiente pipeline. esta llena los valores faltantes usando la mediana y luego estandariza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaling',StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se combinan los procesos por medio de `ColumnTransformer`, este objeto se inicializa entregando una lista de tuplas, donde la primera componente es un identificador, la segunda una pipeline y la tercera un lista de strings contenedora de las columnas a tratar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipe, num_cols),\n",
    "        ('cat', cat_pipe, cat_cols)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente se aplican los procedimientos planificados en la variable `prep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene los datos transformados en formato NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "Las pipelines pueden manejar transformadores de todo tipo y conectarlos entre si, observe que `ColumnTransformer` es un transformador por si solo, luego puede conectarlo con otro objeto por medio de una nueva pipeline. En particular, se pueden conectar transformadores con estimadores, estos ultimos son contenedores de rutinas de aprendizaje automático, que se entrenan por medio del método (transversal en la API *estimators*)`.fit()`. El objetivo de este ejercicio es que comience a utilizar estimadores en pipelines, para ello:\n",
    "\n",
    "1. Importe la clase `RandomForestClassifier` del submódulo `ensemble` de Scikit-Learn.\n",
    "2. Importe la función `train_test_split` del submódulo `model_selection` de  Scikit-Learn.\n",
    "3. Estudie la documentación de ambas clases, luego utilice `train_test_split` para generar conjuntos de entrenamiento y test, denominados `X_train`, `X_test`, `y_train`, `y_test`. En este caso la variable de respuesta `y` es la columna `'Survived'`. ¿Que porcentaje del conjunto de datos pasa a ser *test* por defecto?\n",
    "4. Inicialice un clasificador de *bosque aleatorio* (Random Forest) con 10 estimadores.\n",
    "5. Genere una pipeline que preprocese los datos según el esquema viston anteriormente y que como paso final entrene el clasificador Random Forest inicializado por usted. Esto se debe llavar a cabo por medio del método `.fit()`\n",
    "6. Estudie el porcentaje de aciertos del clasificador en datos test por medio del método `.score()` aplicado a la pipeline producida.\n",
    "\n",
    "**Obs**: Observe que para de pueden construir transformadores personalizados, haciendo uso de la clase `FunctionTransformer`. Si por otra parte se desea generar un transformador *desde 0* se puede crear una clase derivada de `BaseEstimator` y `TransformerMixin` (herencia múltiple, si). Para que la clase pueda actuar dentro de una pipeline, hace falta anular los métodos `fit`, `.transform` y `.fit_transform` (este último es opcional). Es un **buen** ejercicio generar un transformador usando este último método."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manejo de valores faltantes\n",
    "\n",
    "Por lo general, existen razones prácticas y conceptuales a tener en cuenta cuando se trabaja con valores faltantes. \n",
    "\n",
    "En primer lugar, la falta de información introduce sesgos en los modelos de datos, pues hace que las muestras obtenidas no sean representativas del fenómeno que se desea estudiar, esto genera conclusiones sesgadas y puede llevar a tomar malas decisiones. \n",
    "\n",
    "En cuanto al componente práctico, los valores faltantes son incompatibles con algunos modelos de aprendizaje automático, debido a que estos modelos son parte de la razón fundamental de analizar un fenómeno por medio de datos,es que se necesita comprender bien los mecanismos de manejo de este tipo de valores. \n",
    "\n",
    "Según el contexto teórico anterior, el mecanismo de pérdida de información MCAR es el más sencillo en términos de modelación, pues solo requiere parametrizar la matriz indicadora de valores faltantes, sin considerar información fuera dentro de dataset. El test *MCAR de Little* sirve para probar si la información faltante en un dataset sigue la hipótesis MCAR. \n",
    "\n",
    "El test de Little evalúa diferencias en media entre subgrupos de datos con valores faltantes. Es una generalización del test-$t$ mencionado anteriormente. El estadístico de test es una suma ponderada según la ecuación:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{eq:3}\n",
    "\\label{eq:3}\n",
    "d^{2}=\\sum_{j=1}^{J} n_{i}\\left(\\hat{\\boldsymbol{\\mu}}_{j}-\\hat{\\boldsymbol{\\mu}}_{j}^{(\\mathrm{ML})}\\right)^{\\mathrm{T}} \\hat{\\mathbf{\\Sigma}}_{j}^{-1}\\left(\\hat{\\boldsymbol{\\mu}}_{j}-\\hat{\\boldsymbol{\\mu}}_{j}^{(\\mathrm{ML})}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Donde $n_j$ representa el numero de valores faltantes de la columna $j$. Dentro de la columna $j$ se generan grupos de valores faltantes en función de su relación con los valores faltantes de las demás columnas, así por ejemplo, el grupo 1 puede contabilizar solo aquellos valores faltantes para aquellas componentes (filas) presentes unicamente en la columna $j$ y que presentan información completa para todo $i \\neq j$, por otra parte, el grupo 2, puede poseer aquellos valores faltantes para cuya componente posee información faltante en la columna $i \\neq j$, pero inormación completa para todo $k \\neq i,j$. Se generan grupos hasta agotar las combinaciones. Luego, $\\hat{\\boldsymbol{\\mu}}_{j}$ representa un vector contenedor de medias para cada grupo, donde estas se medias, se calculan para las variables con información presente.\n",
    "$\\hat{\\boldsymbol{\\mu}}_{j}^{(\\mathrm{ML})}$ representa un contenedor de estimadores de medias para cada grupo por medio de máxima verosimilitud. Finalmente $\\hat{\\mathbf{\\Sigma}}_j$ represeta la matriz de covarianza entre cada grupo de la variable $j$.\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "En el módulo `mcar` Se implementa el test de Little para cuantificar si un conjunto de datos posee un mecanismo de información faltante MCAR. Revise el código de la función `little_mcar`, comprenda su implementación.\n",
    "\n",
    "\n",
    "#### Eliminación\n",
    "\n",
    "Es el método más sencillo, se conoce tambien como 'list-wise deletion' y consiste en eliminar filas o columnas de un dataset que presenten datos faltantes. Se puede acceder a este tipo de tratamiento por medio de `.dropna()` objetos de Pandas.\n",
    "\n",
    "Otra forma de eliminación de datos se conoce como 'pair-wise deletion' que consiste en generar subconjuntos de un dataset en función de sus patrones de perdida de información. (Vea la implementación del test de Little). Posteriormente, se conducen análisis separados por patrón de perdida de información y se llegan a distintos modelos para cada uno.\n",
    "\n",
    "Ambos métodos son simples de implementar y asumen que el mecanismo de perdida de información en el dataset es del tipo MCAR. En casos distintos (MAR o MNAR) su uso es contraindicado. Se recomiendan cuando el patrón de perdida de información observada (por ejemplo por medio de `mssingno`) es claramente aleatorio, y si además las variables con información faltante son 'pocas' y con 'pocos' valores faltantes. La definición de 'poco' varia en función del problema, pero una buena huerística puede ser inferior al 15% en variables de poca importancia.\n",
    "\n",
    "#### Imputación\n",
    "\n",
    "Corresponde al llenado de información faltante por medio de estimaciones. Para efectuar este tipo de operaciones es importante tener en cuenta los mecanismos de pérdida de información latentes en los datos. Cabe destacar que el principal objetivo de la imputación no es maximizar la precisión (o maximizar metricas de similitud), sino que más bien, busca preservar las características del dataset inicial, generando uno con información completa que permita dilucidar las dinámicas implicadas en el fenómeno estudiado. \n",
    "\n",
    "Como resumen, los métodos de eliminación tienen más sentido bajo la hipótesis MCAR, luego si por ejemplo bajo el test de Little se adquiere un valor p > 0.05, cobra sentido evaluar tales estrategias.\n",
    "\n",
    "##### Imputación singular\n",
    "\n",
    "Como se ha venido aplicando a lo largo de este curso, corresponde a llenar valores faltantes con un valor único basado en la información observada, se requiere por tanto, tener evidencia de que el mécanismo de pérdida de información sigue la hipótesis MAR. \n",
    "\n",
    "Por lo general este tipo de imputación presenta un buen rendimiento empírico en tareas de ciencia de datos y es ampliamente recomendado, sin embargo, aplicar este tipo de métodos puede afectar el calculo de varianzas y covarianzas. \n",
    "\n",
    "En pandas podemos acceder a este tipo de imputación por medio del método `.fillna()` ya sea entregando un valor precalculado (media, mediana, moda, etc...) o utilizando los argumentos `ffill` y `bfill`. \n",
    "\n",
    "\n",
    "Existen directrices a tener en cuenta al momento de tratar valores faltantes:\n",
    "\n",
    "1. Valores faltantes en variables categóricas / ordinales:\n",
    "    * Transformar valores faltantes en una nueva categoría.\n",
    "    * Utilizar códificación Dummy en variables categóricas\n",
    "    * Agrear la categoría de valor faltante como orden inicial o final en categorias ordinales.\n",
    "\n",
    "\n",
    "2. Valores faltantes continuos:\n",
    "    * Probar métodos de imputación o eliminación.\n",
    "    * En mecanismos MCAR, utilizar media / mediana.\n",
    "    * En MAR o MNAR hay que tener en cuenta que la varianza no será representativa.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se desea aplicar el test de Little sobre el dataset 'HousePricing'. Para ello, se agrupan las variables de la base según el tipo de columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train_0.csv', index_col='Id')\n",
    "\n",
    "cat_cols = [\n",
    "    'MSZoning', 'Street', 'Alley', 'LandContour', 'LotConfig', 'Neighborhood',\n",
    "    'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle',\n",
    "    'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation',\n",
    "    'Heating', 'CentralAir', 'Electrical', 'GarageType', 'PavedDrive',\n",
    "    'MiscFeature', 'SaleType', 'SaleCondition'\n",
    "]\n",
    "\n",
    "ordinal_cols = [\n",
    "    'LotShape', 'Utilities', 'LandSlope', 'ExterQual', 'ExterCond', 'BsmtQual',\n",
    "    'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC',\n",
    "    'KitchenQual', 'Functional', 'FireplaceQu', 'GarageFinish', 'GarageQual',\n",
    "    'GarageCond', 'PoolQC', 'Fence'\n",
    "]\n",
    "\n",
    "# Adquieren las categorias de cada variable\n",
    "ordinal_cat = [['Reg', 'IR1', 'IR2', 'IR3'],\n",
    "               ['AllPub', 'NoSewr', 'NoSeWa', 'ELO'], ['Gtl', 'Mod', 'Sev'],\n",
    "               ['Po', 'Fa', 'TA', 'Gd', 'Ex'], ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "               ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "               ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "               ['NA', 'No', 'Mn', 'Av', 'Gd'],\n",
    "               ['NA', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n",
    "               ['NA', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n",
    "               ['Po', 'Fa', 'TA', 'Gd', 'Ex'], ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "               ['Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'],\n",
    "               ['NA', 'Po', 'Fa', 'TA', 'Gd',\n",
    "                'Ex'], ['NA', 'Unf', 'RFn', 'Fin'],\n",
    "               ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "               ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "               ['NA', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "               ['NA', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv']]\n",
    "\n",
    "num_cols = [\n",
    "    'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n",
    "    'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n",
    "    'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n",
    "    'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n",
    "    'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n",
    "    'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n",
    "    'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n",
    "    'MoSold', 'YrSold'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores perdidos en las variables categóricas serán completados con la nueva categoría 'NA', luego se procesan según una codificación Dummy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Pipeline categorica\n",
    "cat_pipe = Pipeline(\n",
    "    steps=[('imputer_cat', SimpleImputer(strategy='constant', fill_value='missing')), \n",
    "           ('onehot',OneHotEncoder(sparse=False, handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos numéricos serán estandarizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Numerica\n",
    "num_pipe = Pipeline(steps=[('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores ordinales serán tratados de la misma manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Ordinal\n",
    "ord_pipe = Pipeline(\n",
    "    steps=[('imputer_ord', SimpleImputer(strategy='constant', fill_value='NA')),\n",
    "           ('ordinal', OrdinalEncoder(categories = ordinal_cat))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se componen las pipelines generadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocesador Compuesto\n",
    "prep = ColumnTransformer(\n",
    "    transformers=[('num', num_pipe, num_cols), \n",
    "                  ('cat', cat_pipe, cat_cols), \n",
    "                  ('ord', ord_pipe, ordinal_cols)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que bajo este tratamiento, las variables ordinales y categóricas tendrán una categoría extra que aportará información al modelo y que por tanto no será contabilizada por el test de Little.\n",
    "\n",
    "Es **importante** que observe que el preprocesamiento fue realizado sobre las variables regresoras (no la dependiente), pues al momento de recibir nuevos datos, serán las columnas asociadas a variables regresoras,las que vendrán con información.\n",
    "\n",
    "Se preparan los datos según las transformaciones anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables regresoras\n",
    "X = df.drop('SalePrice', axis=1).copy()\n",
    "\n",
    "# Variable dependiente\n",
    "y = df['SalePrice'].copy()\n",
    "\n",
    "# Se preparan los datos\n",
    "X_prep = prep.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que para las variables categóricas, se generan nuevas columnas asociadas a sus categorías. Se puede acceder a tales nuevas columnas por medio del atributo publico `.named_transformers_` del preprocesador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se obtienen las variables categoricas transformadas\n",
    "post_cat = prep.named_transformers_['cat'][-1]\n",
    "cat_cols_fit = post_cat.get_feature_names(cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las variables numéricas y ordinales, no hay cambios en las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columnas del datase luego de transformarlo\n",
    "post_cols = list(num_cols) +list( cat_cols_fit) + list(ordinal_cols)\n",
    "len(post_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se construye el dataset transformado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post = pd.DataFrame(data=X_prep, columns=post_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dimension del nuevo dataset es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se Aplica el test de Little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcar import little_mcar\n",
    "little_mcar(df_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el valor $p$ obtenido se tiene que las diferencias en medias producidas por los patrones de datos perdidos, son **inconsistentes** con mecanismo MCAR. Por lo tanto, no se recomienda utilizar metodos de eliminación de valores faltantes, a menos que la la columna donde se encuentren estos valores sea de poca significancia en comparación a la variable dependiente.\n",
    "\n",
    "#### Imputación múltiple\n",
    "\n",
    "Un problema de la imputación singular es que modela los datos como uno completo, sin considerar la incertidumbre inherente de los datos. Los métodos de imputación múltiple permite generar estadísticos insesgados para imputar y considerar incertidumbre en la imputación.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
