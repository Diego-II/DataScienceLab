{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MA6202: Laboratorio de Ciencia de Datos\n",
    "\n",
    "**Profesor: Nicolás Caro**\n",
    "\n",
    "**10/06/2020 - C11 S7**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingeniería de Características y Algoritmos de Aprendizaje Automático\n",
    "\n",
    "La práctica de la ciencia de datos se fundamenta en la obtención, exploración, limpieza, tratamiento y transformación de la información contenida en distintas fuentes de datos. Tales procedimientos constituyen gran parte del desarrollo de proyectos basados datos y su finalidad es la facilitar la obtención de conocimiento. Este último paso se lleva a cabo por medio de sistemas de aprendizaje automático. En general esto se puede resumir en las siguientes etapas:\n",
    "\n",
    "1. **Obtención de la información**: Corresponde a la recolección de datos referentes al fenómeno de interés. Tal información puede ser tanto estructurada como no estructurada.\n",
    "\n",
    "2. **Preprocesamiento y exploración**: Una vez reunida una porción de información significativa, se procede a explorarla y procesarla. En este apartado se aplican métodos de reducción de dimensionalidad, escalamiento, muestreo (conjuntos de entrenamiento y test), selección de características y transformación. \n",
    "\n",
    "3. **Aprendizaje**: Cuando los datos han sido estudiados y transformados de buena manera, se procede a la aplicación de modelos de aprendizaje de máquinas. Acá se estudian métricas de rendimiento y se optimizan hiperparámetros. \n",
    "\n",
    "4. **Evaluación**: Seleccionado un modelo de optimo, se generan esquemas de evaluación sobre conjuntos de datos no observados pero representativos del fenómeno estudiado, esto permite adquirir nociones sobre el comportamiento del sistema modelado fuera del conjunto de entrenamiento. \n",
    "\n",
    "5. **Producción**: Finalmente se genera un entorno en cual el modelo obtenido es utilizado para la obtención de información, elaboración de predicciones, clasificaciones o perfilamientos en función de la materia estudiada. \n",
    "\n",
    "Estos procesos son interdependientes y no presentan un orden lineal, más bien, se entrelazan circulando de manera natural entre los pasos 2 y 4. También se relacionan los pasos 1 y 6 a medida que llegan nuevos datos al sistema. \n",
    "\n",
    "Hasta el momento nos hemos centrado en las etapas de obtención, preprocesamiento y exploración de datos. El paso siguiente corresponde a un *refinamiento* de la información, con el fin de entrenar modelos de aprendizaje de alto rendimiento sobre los datos que se poseen. Este refinamiento ve su primera etapa en los procesos exploratorios (perfilamiento de datos y preprocesamiento inicial), para luego generar transformaciones y selecciones sobre las variables disponible. Dichas transformaciones y selecciones logran un mejor rendimiento en la medida que se obtienen por medio de *conocimiento del área* (*domain kwnlegde*), es decir, utilizando el conocimiento existente, con respecto al problema que se desea modelar. Sin embargo, existen ciertas técnicas *estándar* y algunas *automáticas* que permiten depurar los datos para obtener atributos y transformaciones de interés. \n",
    "\n",
    "En general, el proceso de refinamiento de datos se conoce como **ingeniería de características**, donde las características o *features* hacen referencia a los atributos y sus transformaciones. El proceso de ingeniería de características pasa generalmente por:\n",
    "\n",
    "Explorar los datos y ajustar las variables disponibles, de manera tal, que se gane compatibilidad con las hipótesis de ciertos modelos de aprendizaje automático.\n",
    "\n",
    "Posteriormente, se derivan características de interés, ya sea por medio de composición de atributos o transformaciones sobre estos. En esta parte, es una buena práctica, buscar transformaciones que hagan sentido con el problema modelado, esto no siempre se cumple (ej: una transformación polinomial sobre los datos no necesariamente es interpretable en ciertos problemas, pero puede aumentar considerablemente el rendimiento). \n",
    "\n",
    "Se continua, seleccionando caraterísticas, esto se puede hacer por medio de análisis estadísticos y algoritmos puntuación. Es necesario entender, que aunque en primera instancia más variables deberían llevarnos a mejores resultados, en la realidad, pueden aparecer caraterísticas que añadan ruido al sistema, ya sea por no estar relacionadas con el fenómeno estudiado o por presentarse problemas en su recolección. En tal contexto, cobra importancia la exlusión de dichas variables o de manera reciproca, la selección de aquellas que aseguren el mejor desempeño. \n",
    "\n",
    "Finalmente, se procede a entrenar y evaluar modelos sobre las métricas relevantes con el problema a trabajar, esta etapa se comunica constantemente con la de generación y selección de caraterísticas, pues las transformaciones que se decidan utilizar dependen en cierta medida de los modelos que se implementarán. \n",
    "\n",
    "A continuación se estudian ciertas técnicas de ingeniería de características estándar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones de Atributos\n",
    "\n",
    "Ya hemos estudiado técnicas de transformación por medio de preprocesamiento. A continuación se añaden y estudian nuevas metodologias además de aplicar algunas técnicas previas en la base de datos `HousePricing`. \n",
    "\n",
    "### Preprocesamiento inicial\n",
    "\n",
    "Se cargan los datos y se agrupan sus columnas según tipo de dato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../S5/data/train.csv', index_col = 'Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las técnicas más básicas de ingeniería de características es la **modificación de tipo de dato**, en este caso se transforman los datos reconocido como tipo `object` a tipo `str`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_type_set = [col for col in df.columns if df[col].dtype == 'O']\n",
    "df = df.astype({col:'str' for col in object_type_set})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para trabajar de manera más sencilla con los datos se procede a agrupar los atributos por tipo de dato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    'MSZoning', 'Street', 'Alley', 'LandContour', 'LotConfig', 'Neighborhood',\n",
    "    'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle',\n",
    "    'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation',\n",
    "    'Heating', 'CentralAir', 'Electrical', 'GarageType', 'PavedDrive',\n",
    "    'MiscFeature', 'SaleType', 'SaleCondition'\n",
    "]\n",
    "\n",
    "ordinal_cols = [\n",
    "    'LotShape', 'Utilities', 'LandSlope', 'ExterQual', 'ExterCond', 'BsmtQual',\n",
    "    'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC',\n",
    "    'KitchenQual', 'Functional', 'FireplaceQu', 'GarageFinish', 'GarageQual',\n",
    "    'GarageCond', 'PoolQC', 'Fence'\n",
    "]\n",
    "\n",
    "\n",
    "num_cols = [\n",
    "    'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n",
    "    'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n",
    "    'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n",
    "    'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n",
    "    'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n",
    "    'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n",
    "    'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n",
    "    'MoSold', 'YrSold','SalePrice'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se genera un *mapping* para obtener un datset multindexado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = [('numeric', col) for col in num_cols]\n",
    "mapping.extend([('categorical', col) for col in cat_cols])\n",
    "mapping.extend([('ordinal', col) for col in ordinal_cols])\n",
    "\n",
    "df = df.reindex(columns=num_cols + cat_cols + ordinal_cols)\n",
    "df.columns = pd.MultiIndex.from_tuples(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos visualizaciones según tipo de variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_plotter(df, dtype, nrows=6):\n",
    "    '''Permite graficar subconjuntos de variables en un dataframe multindexado\n",
    "    \n",
    "    Toma como argumento un dataframe de pandas df multi-indexado y un nivel \n",
    "    de multi - indice asociado a un subconjunto de columnas. Obtiene graficos\n",
    "    univariados asociados a dicho subconjunto de varaibles.\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    \n",
    "    df: DataFrame\n",
    "        El conjunto de datos a explorar.\n",
    "    \n",
    "    dtype: String\n",
    "        El nombre del nivel a visualizar\n",
    "    \n",
    "    nrwos: int\n",
    "        La cantidad de filas a generar en la matriz de graficos\n",
    "    \n",
    "    Returns: \n",
    "    -------        \n",
    "        None\n",
    "        Se genera una visualizacion de matplotlib.\n",
    "    '''\n",
    "\n",
    "    cols = df[dtype].shape[1] // nrows\n",
    "    resto = df[dtype].shape[1] % nrows\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=nrows + 1, ncols=cols, figsize=[17, 17])\n",
    "\n",
    "    # Se remueve el resto de plots\n",
    "    list(map(lambda a: a.remove(), ax[-1, -(cols - resto):]))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    # Se define un titulo y su ubicacion\n",
    "    fig.suptitle('Distribuciones Univariadas typo: ' + dtype,\n",
    "                 fontsize=20,\n",
    "                 x=0.5,\n",
    "                 y=1.05)\n",
    "    '''\n",
    "    Se recorre cada axis, para cada columna del dataframe, se genera un grafico \n",
    "    distinto en funcion del tipo de dato.\n",
    "\n",
    "    '''\n",
    "    df_cols = df[dtype].columns\n",
    "    for axis, col in zip(ax.flatten(), df_cols):\n",
    "        try:\n",
    "            # Graficos para datos numericos\n",
    "            sns.distplot(df[(dtype, col)], ax=axis, rug=True)\n",
    "\n",
    "        except RuntimeError:\n",
    "            sns.distplot(df[(dtype, col)], ax=axis, rug=True, kde=False)\n",
    "\n",
    "        except ValueError:\n",
    "            sns.countplot(df[(dtype, col)], ax=axis)\n",
    "\n",
    "        axis.set_xlabel(col, fontsize=15)\n",
    "\n",
    "    # Se ajusta el espaciado interno entre subplots\n",
    "    w, h = (.4, .4)\n",
    "    plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos visualizaciones para las variables numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_plotter(df,'numeric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El manejo de valores faltantes corresponde a una técnica de preprocesamiento ya estudiada y previa a las transformaciones de atributos buscadas en ingeniería de características. Por tal motivo, se hace un tratamiento rápido de los valores faltantes, sin ahondar en detalles. En este apartado, se reemplazan las variables con texto 'nan' por su objeto equivalente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('nan',np.nan, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa la distribución porcentual de los valores perdidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_per(df):\n",
    "    na_percent = (df.isnull().sum()/len(df))[(df.isnull().sum()/len(df))>0].sort_values(ascending=False)\n",
    "\n",
    "    return pd.DataFrame({'Missing Percentage':na_percent*100})\n",
    "\n",
    "find_missing_per(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se genera una función auxiliar para tratar con multi indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexer(cols, t_c = df.columns):\n",
    "    '''Genera columnas multinivel a partir de nombres de columna planos.'''\n",
    "    \n",
    "    set_to_tuple = set(*[cols])\n",
    "\n",
    "    tuples = [\n",
    "        i for i in t_c if set_to_tuple.intersection(set(i))\n",
    "    ]\n",
    "    \n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se llenan los valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se transforma nan -> None\n",
    "to_none = [\n",
    "    'FireplaceQu', 'Fence', 'Alley', 'MiscFeature', 'PoolQC', 'MSSubClass',\n",
    "    'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual',\n",
    "    'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'\n",
    "]\n",
    "\n",
    "# Fill mapper\n",
    "to_fill = {key: 'None' for key in indexer(to_none)}\n",
    "\n",
    "# Se transforma nan -> 0\n",
    "to_0 = [\n",
    "    'GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2',\n",
    "    'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'\n",
    "]\n",
    "to_fill.update({key: 0 for key in indexer(to_0)})\n",
    "\n",
    "# Se llenan los valores faltantes\n",
    "df.fillna(to_fill, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se estudia el dataset luego de llenar los valores anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_missing_per(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se tiene que las variables `LotFrontage` y `Electrical` presentan valores faltantes. Para la variable `Electrical` esto representa una cantidad muy baja de observaciones, por lo que simplemente se pueden eliminar, mientras que para la variable `LotFrontage` se escoge llenar los valores faltantes por medio de una subagrupación en función de la variable 'Neighborhood', con esto, se asigna el valor de la mediana de 'LotFrontage' en para cada subgrupo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_median = lambda x: x.fillna(x.median())\n",
    "\n",
    "df.dropna(axis=0, subset=indexer(['Electrical']), inplace= True)\n",
    "\n",
    "df[indexer(['LotFrontage'])] = df.groupby(indexer(['Neighborhood']))[indexer(['LotFrontage'])].transform(\n",
    "    group_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finalmente no se tienen valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(find_missing_per(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es trabajar con los valores anómalos. Nuevamente, este paso es previo a la transformación de atributos por lo que no se estudia en profundidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varlist = [\n",
    "    'GrLivArea', 'TotalBsmtSF', '1stFlrSF', 'MasVnrArea', 'GarageArea',\n",
    "    'TotRmsAbvGrd'\n",
    "]\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "def scatter_vs_price(df=df,\n",
    "                     varlist=varlist,\n",
    "                     nrows=3,\n",
    "                     db_min_samples=5,\n",
    "                     db_eps=0.99):\n",
    "    \n",
    "    '''Grafica una lista de variables contra SalePrice.\n",
    "    \n",
    "    Acepta como argumento una lista de variables, cada una de estas es \n",
    "    escalada de manera robusta, posteriormente se entrena una aglomeracion\n",
    "    usando dbscan para resaltar los posible outliers.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    df: Pandas DataFrame\n",
    "        El conjunto de datos de HousePricing\n",
    "    \n",
    "    varlist : List\n",
    "              Lista de variables a estudiar\n",
    "    \n",
    "    nrows: int\n",
    "           Cantidad de filas a graficar (argumento de subplots)\n",
    "    \n",
    "    db_min, db_eps: int, float\n",
    "                    Hiperparametros de DBSCAN\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "        Visualizacion: None\n",
    "                       Visualizacion de las variables estudiadas contra el precio\n",
    "        \n",
    "        Outliers: Dict\n",
    "                  Diccionario de posibles outliers\n",
    "    '''\n",
    "    outliers = dict()\n",
    "    \n",
    "    ncols = len(varlist) // nrows\n",
    "    resto = len(varlist) % nrows\n",
    "\n",
    "    fig, ax = plt.subplots(nrows + 1, ncols, figsize=[13, 10])\n",
    "    # Se borran las axis innecesarias\n",
    "    for a in ax[-1, -(ncols - resto):]:\n",
    "        a.remove()\n",
    "    ax = ax.ravel()\n",
    "\n",
    "    for a, var in zip(ax, varlist):\n",
    "        \n",
    "        X = df[indexer([var, 'SalePrice'])]\n",
    "        scaler = RobustScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "        outlier_detection = DBSCAN(min_samples=db_min_samples, eps=db_eps)\n",
    "        C = outlier_detection.fit_predict(X)\n",
    "\n",
    "        a.scatter(x=X[:, 0], y=X[:, 1], c=C, alpha=0.5)\n",
    "\n",
    "        #a.axvline(x=4600, color='r', linestyle='-')\n",
    "        title = 'Scaled' + var + '- Price scatter plot'\n",
    "        a.set_title(title, fontsize=15, weight='bold')\n",
    "        outliers.update({var:C})\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se estudian las variables seleccionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = scatter_vs_price()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por simplicidad, se eliminan los valores anómalos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se recolectan los indices de valores anomalos\n",
    "outlier_idx = pd.Index({})\n",
    "for key, val in outliers.items():\n",
    "\n",
    "    outlier_locs = [*map(bool, val)]\n",
    "    columns = indexer([key])\n",
    "\n",
    "    outlier_idx = outlier_idx.union(df.loc[outlier_locs, columns].index)\n",
    "    \n",
    "# Se eliminan las filas con valores anomalos\n",
    "df.drop(index=outlier_idx, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien, luego de este preprocesamiento se puede trabajar en transformaciones sobre los datos, se debe recordar, que las etapas de preprocesamiento y manejo de caraterísticas se comunican constantemente, generando ciclos en el flujo de trabajo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manejo de Carácteristicas Básico\n",
    "\n",
    "\n",
    "A continuación se procede a producir características relevantes, una primera aproximación a este procedimiento es comenzar por introducir transformaciones sobre las variables y modelar la interacción entre estas. En este apartado, es necesario discutir el término de **Data Leakage**. \n",
    "\n",
    "**Data Leakage** hace referencia a la *fuga* de información entre conjuntos destinados para entrenamiento y test. Si se produce esta fuga, significa que se esta utilizando información no valida en procesos de entrenamiento, lo cual genera modelos sobre optimistas en sus métricas de evaluación. En la práctica, un modelo con fuga de información puede ser incluso inútil en producción. Para evitar este fenómeno, basta realizar las transformaciones de datos pertinentes dentro de los procesos de validación y entrenamiento. Otra buena práctica en este aspecto, es generar un conjunto de validación adicional para comprobar métricas de rendimiento luego de realizar selección de modelo.\n",
    "\n",
    "A modo de ejemplo: si en un problema de modelamiento se normalizan ciertas variables utilizando la información total del dataset y luego se procede a generar particiones de entrenamiento y validación, entonces los elementos ubicados en los conjuntos de validación habrán aportado información al proceso de normalización pues influyeron en los calculos de media y desviación estándar. Esto puede mejorar de cierta manera el rendimiento en predicción, dando como resultado un modelo sesgado.\n",
    "\n",
    "Lo primero que se hará es generar un conjunto de validación utilizando el 10% de los datos, este conjunto se utiliza al final del proceso de selección de modelos para verificar la capacidad de generalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_val = train_test_split(df, test_size = .1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se procede a estudiar ciertos atributos del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_color_codes(palette='deep')\n",
    "\n",
    "\n",
    "sns.distplot(df_train['numeric']['SalePrice'], color=\"b\");\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Frecuancia\")\n",
    "ax.set(xlabel=\"SalePrice\")\n",
    "ax.set(title=\"Distribucion de SalePrice\")\n",
    "\n",
    "sns.despine(trim=True, left=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tiene similitud a una distribución normal, para ello se hace un test de normalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def norm_test(data = df_train['numeric']['SalePrice'] ):\n",
    "\n",
    "    k, p = stats.normaltest(data)\n",
    "\n",
    "    alpha = 1e-3\n",
    "    print(\"p =\",p)\n",
    "\n",
    "    if p < alpha: \n",
    "        print(\"Se puede rechazar la hipotesis nula\")\n",
    "    else:\n",
    "        print(\"No se puede rechazar la hipotesis nula\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego la variable `SalePrice` no se comporta como una normal. En estos casos. Una técnica de manejo de características básica es la transformación de Box-Cox (ya vista), Podemos transformar esta variable en función de tal método"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Se utiliza una funcion auxiliar para transformar la serie\n",
    "to_array = lambda pd_series: pd_series.values.reshape(-1, 1)\n",
    "y = to_array(df_train['numeric']['SalePrice'])\n",
    "\n",
    "transformer_bc = PowerTransformer(method='box-cox')\n",
    "\n",
    "y = transformer_bc.fit_transform(y)\n",
    "\n",
    "# Se aplica el test de normalidad a la variable de interes\n",
    "norm_test(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego la transformación de Box - Cox permite normalizar de manera sencilla la distribución de la variable `SalePrice`. En general, todas las técnicas de preprocesamiento vistas anteriormente (escalar, normalizar,códificación dummy, etc...) entran en el manejo de básico de características. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Visualice las variables numéricas, calcule su asimetría estadística (skewness) por medio del método `.skew()`. Defina un valor umbral (ej: 0.6). Luego aplique una transformación de potencia sobre las variables más asimétricas. \n",
    "\n",
    "**Obs**: Lo anterior permite normalizar de manera más robusta aquellas variables que por naturaleza se alejan de ser normales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra manera de generar características de interés es por medio de interacciones. En este caso se hace uso del conocimiento del área que se trabaja. \n",
    "\n",
    "Por ejemplo, las variables `TotalBsmtSF`, `1stFlrSF` y `2ndFlrSF` representan unidades de área en pies cuadrados para el subterraneo, primer y segundo piso. Con esta información se puede crear la variable `TotalSF` que representa la superficie total de una vivienda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:, ('numeric','TotalSF')] = df_train['numeric']['TotalBsmtSF'] + \\\n",
    "                                         df_train['numeric']['1stFlrSF'] + \\\n",
    "                                         df_train['numeric']['2ndFlrSF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede también general la variable `YearsSinceRem` que permite codificar la cantidad de años que han pasado desde la remodelación de una vivienda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:,('numeric','YearsSinceRemodel')] = df_train.loc[:,('numeric','YrSold')]\\\n",
    "                                          - df_train.loc[:,('numeric','YearRemodAdd')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma manera, se puede agregar la variable `TotalQual` que representa el puntaje total de calidad asociado a una vivienda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:,('numeric','TotalQual')]  = df_train.loc[:,('numeric','OverallQual')]+\\\n",
    "                                           df_train.loc[:,('numeric','OverallCond')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, se pueden aplicar transformaciones exponenciales (como en el caso de Box-Cox), lineales (como los dos casos anteriores) o de cualquier otro tipo (trigonometricas, polinomiales) en función del tipo de problema que se trabaje, siempre y cuando la transformación tenga un trasfondo en el fenómeno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Para recuperar multi indexado: \n",
    "df_train = pd.read_csv('df_train_v1.csv', header=[0,1], index_col=0)\n",
    "'''\n",
    "\n",
    "df_train.to_csv('df_train_v1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "Otra forma de inducir interacciones es por medio de variables dummy, en este caso se procede a generar codificaciones one hot. En este contexto, si se posee una variable codificada de esta forma y se multiplica por otra, ocurre que la nueva variable generada modela la interesección de eventos entre esas variables. \n",
    "\n",
    "1. Visualice las variables categóricas, genere un esquema de codificación y modele interacciones entre variables. Cuantifique estadísticamente el efecto de las variables de interacción con la variable de respuesta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de características \n",
    "\n",
    "En conjunción con el estudio, perfilamiento y aplicación de transformaciones iniciales a los datos, se hace necesario utilizar técnicas de selección de característcas. Estas permiten mejorar métricas de rendimiento en predicción. \n",
    "\n",
    "Las técnicas de selección de característcas se pueden clasificar como **no supervisadas** que no usan la variable de respuesta (análisis de correlaciones en contextos de multicolinealidad). Otra categroría son las técnicas **supervisadas** , estas hacen uso de la variable de respuesta, y pueden separarse en técnicas *envolventes* o *wrappers*, las cuales seleccionan variables en función de los resultados provistos por algoritmos de aprendizaje. Se observan también los métodos de *filtrado*, que seleccionan subconjuntos de variables en función de ciertos estadísticos, valores umbrales o scoring  (pueden ser no supervisados). Además, se encuentran los métodos de selección supervisada *implícitos*, estos corresponden a metodologías naturales dentro de ciertos algoritmos de aprendizaje (ej: selección de variables por medio de regresión LASSO). Por último se pueden utilizar técnicas de **reducción de dimensionalidad** que permiten proyectar los datos input a dimensiones más bajas. \n",
    "\n",
    "Sci-kit Learn provee de la API `sklearn.feature_selection ` que proporciona métodos de ayuda en este apartado.\n",
    "\n",
    "A continuación se estudian métodos de **selección de caraterísticas univariadas**.\n",
    "\n",
    "### Umbral de varianza \n",
    "\n",
    "Este método permite seleccionar variables por *filtrado* no supervisado y es un acercamiento simple que se basa en la premisa de que aquellas características con muy poca varianza no aportan información al modelo. \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se observan las características ordinales del conjunto de entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_plotter(df_train,dtype='ordinal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se observa que las variables `Functional` y `PoolQc` poseen una baja varianza (entre otras variables). Se importa el método de seleccion de caraterísticas por medio de umbral de varianza y se estudia su efecto en el conjunto de entrenamiento, para ello, es necesario codificar las variables estudiadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se generan pipelines sobre las variables ordinales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adquieren las categorias de cada variable\n",
    "ordinals = df_train['ordinal']\n",
    "\n",
    "ordinal_cat = [[*ordinals[col].unique()] for col in ordinals.columns]\n",
    "\n",
    "ord_pipe = Pipeline(\n",
    "    steps=[('ordinal', OrdinalEncoder(categories = ordinal_cat))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(('numeric','SalePrice'), axis=1).dropna(how = 'all').copy()\n",
    "\n",
    "# Variable dependiente\n",
    "y_train = df_train['numeric']['SalePrice'].copy()\n",
    "\n",
    "# Se preparan los datos\n",
    "X_prep = ord_pipe.fit_transform(X_train['ordinal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc = pd.DataFrame(X_prep, columns=X_train['ordinal'].columns).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las varianzas asociadas al conjunto de características anterior viene dado por:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importa el método de umbral de varianza y se aplica a las características del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "sel = VarianceThreshold(threshold=.47)\n",
    "X_filtered = sel.fit_transform(X_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lo que se filtran 9 caracteristicas, para obtener las caraterísticas seleccionadas se hace uso del método `.get_support()` para obtener una mascara de los indices seleccionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['ordinal'].columns[sel.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos Univariados para Regresión \n",
    "\n",
    "Dentro de las técnicas clásicas de selección de variables en contextos de **regresión** encontramos los métodos:\n",
    "\n",
    "**Input numérico u Ordinal**\n",
    "\n",
    "* Coeficiente de correlación de Pearson (scoring lineal)\n",
    "\n",
    "Este coeficiente mide la relación lineal entre dos variables aleatorias gaussinas y ha sido utilizado de manera frecuente durante el curso. Para seleccionar características utilizando este coeficiente se puede hacer uso de la función `f_regression` del módulo `feature_selection` de Scikit-learn.\n",
    "\n",
    "\n",
    "* Coeficiente de Spearman (scoring no lineal) \n",
    "\n",
    "El coeficiente de correlación de Spearman corresponde a una medida de correlación de *rango*, esto se refiere a que permite cuantificar relaciones entre variables ordinales. En este caso, si se trabaja con variables numéricas, es necesario categorizarlas en intervalos ordenados para luego compararlas. La ventaja de esto es que no se requiere especificar distribuciones para los datos, por lo que es un test no paramétrico. Este coeficiente permite describir si la relación entre dos variables aleatorias es *monotona*.\n",
    "\n",
    "Si se poseen $N$ $X_i$ e $Y_i$ de las variables aleatorias $X$ e $Y$, es posible calcular el coeficiente de correlación de Spearman $r_s$ generando transformaciones de rango $\\pi_X$ y $\\pi_Y$ sobre los valores $X_i$ e $Y_i$. Lo que esta transformación hace es ordenar las muestras asociadas a cada variable aleatoria asociandoles una posición.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se define una variable aleatoria y se calcula su transformación de rango."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(10)\n",
    "\n",
    "def pi_x(X):\n",
    "    X_ = X.copy()\n",
    "    X_.sort()\n",
    "    return [*enumerate(X_,start=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_x(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando dos o más valores se repiten, se debe definir una regla de desempate, esta consiste en asignar rangos promedio, es decir, si el valor $X_i$ se repite $l$ veces, entonces se les asigna el promedio de sus rangos, es decir $\\pi_X(X_i^{(k)}) = \\frac{1}{l}\\sum_{j=1}^l \\pi_X(X_i^{(j)})$ para todo $k = 1,...,l$ donde $X_i^{(k)}$ representa cada copia de $X_i$.\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "1. Modifique la función de asignación de rangos para que implemente esta regla de desempate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haciendo uso de las transformaciones $\\pi_X$ y $\\pi_Y$ se calcula \n",
    "$r_s$ por medio de:\n",
    "$$\n",
    "r_{s}=\\frac{\\operatorname{cov}\\left(\\pi_{X}, \\pi_{Y}\\right)}{\\sigma_{\\pi_{X}} \\sigma_{\\pi_{Y}}}\n",
    "$$\n",
    "Donde $\\sigma_{\\pi_{X}}$ y $\\sigma_{\\pi_{Y}}$ corresponden a las desviaciones estándar sobre los rangos de las variables. El calculo anterior corresponde por tanto al coeficiente de correlación de Pearson sobre la categorización ordinal de las variables de interés. Para calcular el coeficiente de correlación de Spearman entre las variables de numéricas se puede hacer uso del método de pandas `.corr()`.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se calcula el coeficiente de correlación de Spearman sobre las variables numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman = df_train['numeric'].corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman.nlargest(5,columns='SalePrice')['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman.nsmallest(5,columns='SalePrice')['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "un coeficiente de Spearman muy cercano a 1 o a -1 indica relaciones monotonas entre las variables, por otra parte un coeficiente cercano a 0 sugiere ausencia de tal tipo de relaciones. Con esto es posible hacer un filtrado de variables poniendo un umbral de correlación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. La función `spearmanr` de `scipy.stats` permite tener acceso a un valor de significancia para contrarrestar la hipotesis nula _\" No hay relación entre las caracteristicas\"_. Seleccione ciertas variables numéricas y utilizando dicha función."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Coeficiente de correlación de Kendall (scoring no lineal) \n",
    "\n",
    "Este coeficiente mide la *concordancia* entre los rangos  de dos variables aleatorias . Los rangos asociados a cada valor son calculados de la misma manera que para el coeficiente de correlación de Spearman. Si se obtienen dichos rangos, es posible calcular el *número de pares concordantes* $C$ y *número de pares no concordantes* $D$, para ello, si se poseen $N$ muestras $X_i$ e $Y_i$ de las variables aleatorias $X$ e $Y$, se generan sus transformaciones de rango correspondientes $x_i = \\pi_X(X_i)$ y $y_i = \\pi_Y(Y_i)$, luego se generan pares ordenados $(x_i, y_i)$. Para finalizar, el coeficiente de correlación $\\tau$ de Kendall se obtiene mediante la formula:\n",
    "$$\n",
    "\\tau=\\frac{2}{n(n-1)} \\sum_{i<j} \\operatorname{sgn}\\left(x_{i}-x_{j}\\right) \\operatorname{sgn}\\left(y_{i}-y_{j}\\right)\n",
    "$$\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se utiliza el coeficiente de correlación de Kendall con pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kendall = df_train['numeric'].corr(method='kendall')\n",
    "kendall.nsmallest(5,columns='SalePrice')['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kendall.nlargest(5,columns='SalePrice')['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, un valor cercano a 1 indica alta concordancia, cercano a -1 indica alta discordancia. Un valor cercano a 0 indica la ausencia de relación. \n",
    "\n",
    "**Ejercicios**\n",
    "\n",
    "1. Utilice la función `kendalltau` del módulo `scipy.stats` para realizar pruebas de hipótesis sobre la relación entre variables numéricas y la variable predictiva.\n",
    "\n",
    "2. Scikit-learn ofrece la función `f_regression` del módulo `feature_selection`. Esta permite realizar un test F univariado. Con este test se captura la dependencia lineal entre dos vectores. ¿Es posible interpretar el estadístico F como un puntaje de selección de variables? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Información Mutua\n",
    "\n",
    "La información mutua entre dos variables $X$ e $Y$ busca medir dependecia. Así, si ambas son independientes, no existe información de $Y$ que se pueda obtener por medio de $X$ (lo mismo en el sentido contrario) luego el valor asociado de información mutua para estas variables será 0. Si por otra parte $X$ es una función determinista de $Y$ se tendrá un valor de información mutua máximo, en general, mayores valores de este valor indican mayor dependencia (puede ser usado como un scoring). En términos matemáticos, si $(X,Y)$ es un par de variables aleatorias, si su distribución conjunta se denota por $P_{(X,Y)}$ y sus marginales por $P_X$ y $P_Y$ se define la información mutua $I(\\cdot,\\cdot)$ como:\n",
    "\n",
    "$$\n",
    "I(X;Y) = D_{KL}(P_{(X,Y)} || P_X \\otimes P_Y))\n",
    "$$\n",
    "\n",
    "Donde $D_{KL}$ es la divergencia de Kullback-Leibler. Esta se define para dos medidas de probabilidad $P, Q$ sobre $\\mathcal{X}$, con $P$ absolutamente continua con respecto a $Q$ según la ecuación:\n",
    "$$\n",
    "D_{\\mathrm{KL}}(P \\| Q)=\\int_{\\mathcal{X}} \\log \\left(\\frac{d P}{d Q}\\right) d P\n",
    "$$\n",
    "\n",
    "En el caso discreto esto equivale a\n",
    "$$\n",
    "D_{\\mathrm{KL}}(P \\| Q)=\\sum_{x \\in \\mathcal{X}} P(x) \\log \\left(\\frac{P(x)}{Q(x)}\\right)\n",
    "$$\n",
    "Con lo que la información mutua se puede escribir como:\n",
    "\n",
    "$$\n",
    "\\mathrm{I}(X ; Y)=\\sum_{y \\in \\mathcal{Y}} \\sum_{x \\in \\mathcal{X}} P_{(X, Y)}(x, y) \\log \\left(\\frac{P_{(X, Y)}(x, y)}{P_{X}(x) P_{Y}(y)}\\right)\n",
    "$$\n",
    "\n",
    "La ventaja de esta medida de dependencia recae en que puede cuantificar relaciones de dependencia no lineales.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se aplica el criterio de información mutua para filtrar características, para ello se importa el módulo correspondiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "Y = df_train['numeric'].SalePrice.values.reshape([-1,])\n",
    "X = df_train['numeric'].GrLivArea.values.reshape([-1,])\n",
    "\n",
    "I = mutual_info_regression(df_train['numeric'].drop('SalePrice', axis=1),Y)\n",
    "I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que hay variables indicadas como independientes de `SalePrice` por lo que puede ser beficioso para el modelo extraerlas.\n",
    "\n",
    "**Ejercicios**\n",
    "\n",
    "1. Genere un conjunto de datos `D` de 3 dimensiones y 1000 registros utilizando `np.random.rand`.\n",
    "\n",
    "2. Genere una variable `y` dependiente de `D`, para ello, `y` debe depender de manera lineal de la primera columna de `D` y de manera no lineal de la segunda columna de `D` (use una transformación trigonométrica por ejemplo). Añada ruido aleatorio normal. \n",
    "\n",
    "3. Haga un test F para medir la relación entre las columnas de `D` (características) e `y`. \n",
    "\n",
    "4. Haga calcule la información mutual entre las características de `D` e `y`. Compare con los resultados de del test F. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos Univariados para Clasificación\n",
    "\n",
    "Cuando la variable de respuesta es discreta se pueden aplicar las siguientes técnicas.\n",
    "\n",
    "**Input Númerico**\n",
    "\n",
    "* Test F ANOVA \n",
    "\n",
    "Este método ya fue estudiado. Permite detectar si la variable de respuesta (discreta) genera grupos significativamente diferenciables en la variable numérica input. Scikit-learn ofrece la función `f_classif` del módulo `feature_selection` para realizar la versión one-way de este test. Observe que se puede utilizar para inputs numéricos en problemas de regresión. Este test requiere que los grupos inducidos por las categorías sean distribuidos de manera normal, lo cual dificilmente se cumple en la práctica.\n",
    "\n",
    "* Información Mutua\n",
    "\n",
    "Este método es aplicable a problemas de clasificación con inputs numéricos. \n",
    "\n",
    "**Inputs discretos**\n",
    "\n",
    "* Test Chi cuadrado\n",
    "\n",
    "El test de independencia $\\chi^2$ permite determinar si existen relaciones de dependencia significativas entre dos variables discretas. El estadístico $\\chi^2$ comprueba la existencia de diferencias significativas entre las frecuencias observadas y las frecuencias esperadas para las variables estudiadas. Acá la hipótesis nula es `No exsite una asociación entre las variables`. Para la construcción de este test se genera una tabla de contingencia, esta consiste en una matriz con entradas del tipo $O_{ij}$, cada una de las cuales representa la cantidad de veces que se observa la categoría (feature) $x_i$ versus la categoría $y_j$ (respuesta). De esta forma, se pueden obtener totales por fila $N_{i,.}$ y por columna $N_{.,j}$. Con estos valores se obtienen los *conteos esperados* $\\hat{E}_{ij}$ por medio de:\n",
    "$$\n",
    "\\hat{E}_{i, j}=\\frac{\\left(N_{i, .}\\right)\\left(N_{., j}\\right)}{\\sum_{i}N_{i,.} + \\sum_{j}N_{.,j}}\n",
    "$$\n",
    "\n",
    "Así, el estadístico asociado a la relación de categorías $i,j$ viene dado por:\n",
    "\n",
    "$$\n",
    "\\chi^{2}=\\sum_{i, j} \\frac{\\left(O_{i j}-\\hat{E}_{i, j}\\right)^{2}}{\\hat{E}_{i, j}}\n",
    "$$\n",
    "\n",
    "Tal valor permite hacer pruebas de hipótesis o ser considerado como un puntaje para selección de características.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se aplica el test chi cuadrado sobre las variables categóricas, para ello, se transforma la variable `SalePrice` a una variable ordinal y se procesa por medio de encoders de Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "# Features\n",
    "X_train = df_train['categorical'].copy()\n",
    "X_train.dropna(how='all', inplace=True)\n",
    "\n",
    "# Variable dependiente\n",
    "y_train = df_train['numeric']['SalePrice'].copy()\n",
    "y_train.dropna(how='all', inplace=True)\n",
    "y_train = pd.cut(y_train, bins=5)\n",
    "y_train = np.array(y_train).reshape([-1,1])\n",
    "\n",
    "# Se preparan los datos\n",
    "encoder_one_hot = OneHotEncoder(sparse=False)\n",
    "encoder_ordinal = OrdinalEncoder()\n",
    "\n",
    "X_cat_enc = encoder_one_hot.fit_transform(X_train)\n",
    "\n",
    "X_cat_enc = pd.DataFrame(\n",
    "    X_cat_enc, columns=encoder_one_hot.get_feature_names()).astype(int)\n",
    "\n",
    "y_train_enc = encoder_ordinal.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente se obtiene un arreglo de puntajes y valores de significancia para cada categoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "chi2(X_cat_enc,y_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. El método de información mutua funciona para variables discretas en problemas de clasificación. Investigue como implementarlo en selección de características en tal contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, los métodos univariados de selección de caractrísticas permiten realizar selecciones basándose en valores interpretables como *puntajes*. Si se ve este proceso de selección como un filtrado de caracteŕisticas, entonces es sencillo implementarlo dentro de una *pipeline* de preprocesamiento, para esto, Scikit-learn ofrece los objetos `SelectKBest` para filtrar características según un número de dimensiones objetivo, `SelectPercentile` que permite remover variables según un porcentaje objetivo de dimensiones.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se seleccionan 5 variables categóricas según el resultado del test chi cuadrado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Se entrena y transfroma el selector\n",
    "selector = SelectKBest(\n",
    "    k=5,\n",
    "    score_func=chi2,\n",
    ")\n",
    "selected_data = selector.fit_transform(X_cat_enc, y_train_enc)\n",
    "\n",
    "# Se obtienen las categorias\n",
    "selected_cats = selector.get_support()\n",
    "selected_cats = X_cat_enc.columns[selected_cats]\n",
    "\n",
    "print(\n",
    "    'Categorias seleccioandas: \\n',\n",
    "    *[str(i + 1) + '.- ' + str(h) + '\\n' for i, h in enumerate(selected_cats)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios**\n",
    "\n",
    "1. Utilice `SelectPercentile` del módulo `feature_selection` para seleccionar el 10% de las categrías con mayor puntuación según el estadístico chi cuadrado.\n",
    "\n",
    "2. Lo métodos de proporcion de falso positivo (FPR), proporción de descubrimientos falso (FDR) y error por familias (FWE) permiten seleccionar caracteríssticas utilizando un valor de significacia versus p-valores en tests estdísticos. Investigue su formulación y aplique selección de variables utilizando los métodos `SelectFpr`, `SelectFdr` y `SelectFwe` respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interludio: Métodos de Estimación basados en Árboles\n",
    "\n",
    "Los métodos basados en arboles operan particionando el espacio de características en un conjunto de rectangulos, luego se ajusta un modelo simple (cómo una constante $c$) en cada uno de estos rectangulos, con el fin de predecir la respuesta de los datos agrupados en esta región del espacio.\n",
    "\n",
    "En concreto este proceso se reduce a encontrar cuales son las variables que mejor segmentan el conjunto de datos (con una desigualdad simple), en el caso binario para la primera etapa del algoritmo el conjunto de entrenamiento original es particionado en 2 subconjuntos. Este proceso se repite de forma recursiva en cada una de las separaciones. De esta forma se segmenta el conjunto de entrenamiento quedando asociada cada muestra a un nodo terminal del árbol ensamblado.\n",
    "\n",
    "Después del proceso de crecimiento del arbol recién descrito dependiendo del contexto del problema abordado, puede seguir una etapa de podado del arbol, lo cual permite manejar el tamaño y problemas con **sobre-ajuste** en el conjunto de entrenamiento.\n",
    "\n",
    "#### CART: Arboles de Regresión y Clasificación\n",
    "\n",
    "En primer lugar se aborda la construcción para el problema de regresión, luego se revisará un método para construir clasificadores modificando algunos aspectos de la construcción que antecede.\n",
    "\n",
    "##### Regresión:\n",
    "\n",
    "Sea un conjunto de datos $\\{ (x_i,y_i) \\}_{i=1}^N$, donde cada $x_i \\in \\mathbb{R}^p$ e $y_i \\in \\mathbb{R}$ corresponde a la respuesta. Se define $X = (x_1,\\ldots, x_N)^T \\in \\mathbb{R}^{N \\times p}$. Esto corresponde al escenario clásico que se da en un problema de regresión.\n",
    "\n",
    "La idea del algoritmo es generar reglas de desición automáticas para segmentar los puntos del conjuntos de entrenamiento y la topología de las regiones de desición. Para este desarrollo usaremos un esquema de partición binario, es decir el árbol resultante será un árbol binario.\n",
    "\n",
    "Supongamos que ya se tiene una partición con $M$ elementos $R_1, \\ldots, R_M$; donde se modela una respuesta constante $c_m$ en cada región, es decir:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\sum_{m=1}^M c_m I{(x \\in R_m)}\n",
    "\\end{equation}\n",
    "\n",
    "Donde $I$ corresponde a la función indicatríz.\n",
    "\n",
    "Al adoptar como criterio de perdida la minimización de la suma de los errores cuadráticos ($\\sum(y_i - f(x_i))^2)$ es simple demostrar que el $c_m$ óptimo en cada región $R_m$ corresponde al promedio entre las respuestas, es decir:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{c}_m = \\frac{1}{|R_m|}\\sum_{i:x_i \\in R_m} y_i\n",
    "\\end{equation}\n",
    "\n",
    "Encontrar la mejor partición en término de los errores cuadráticos es computacionalmente costoso, incluso prohibitivo. Por esta razón se emplea un esquema de optimización **glotón**, partiendo con todos los datos, considerando una variable $j$ para partición y un punto $s$, se difinen los siguiente semi planos:\n",
    "\n",
    "\\begin{align}\n",
    "R_1(j,s) &= \\{ x \\in X | x_j \\geq s\\} \\\\\n",
    "R_2(j,s) &= \\{ x \\in X | x_j < s\\}\n",
    "\\end{align}\n",
    "\n",
    "Luego para encontrar las variables $j,s$, se resuelve el siguiente problema:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{j,s} \\left \\{ \\min_{c_1} \\sum_{i:x_i \\in R_1(j,s)} (y_i-c_1)^2 + \\min_{c_2} \\sum_{i:x_i \\in R_2(j,s)} (y_i-c_2)^2 \\right \\}\n",
    "\\end{equation}\n",
    "\n",
    "para cualquier $j,s$ la elección óptima de $\\hat{c}_1$ y $\\hat{c}_2$ corresponderá al promedio los $y_i$ en las regiones $R_1$ y $R_2$ respctivamente.\n",
    "\n",
    "Considerando que para cada $j$ variable de segmentación es facil determinar el punto $s$ óptimo. Es factible recorrer todas las características para encontrar el par óptimo $(j,s)$.\n",
    "\n",
    "Habiendo obtenido el primer corte de los datos $R_1$ y $R_2$, el proceso de segmentación se realiza de forma recursiva en cada una de las regiones encontradas de forma sucesiva.\n",
    "\n",
    "La pregunta que aparece de forma natural en esta etapa del proceso es que tan profundo debe ser extendido el arbol. Claramente una arbol demasiado grande puede conllevar a un **sobre-ajuste** del conjunto de entrenamiento, generando malas propiedades de generalización del modelo.\n",
    "\n",
    "La estrategía que suele ser empleada es definir un parámetro $n$ que representa el tamaño mínimo que puede tener una hoja del arbol. De esta forma el arbol crece hasta que cada hoja de este tiene un tamaño a lo más $n$. Este arbol producto del proceso de separar sucesivamente los datos se define cómo $T_0$.\n",
    "\n",
    "Para evitar lo mencionado en el parrafo anterior despues de crecer el arbol, se lleva a cabo un proceso de poda, es decir colapsar nodos del arbol (no terminales, hojas). Esto será explicado y se realiza en base a un criterio que considera la función de costo y la complejidad del arbol (tamaño), un criterio de **costo-complejidad**.\n",
    "\n",
    "Se define el sub arbol $T \\subset T_0$ cómo cualquier arbol que puede ser obtenido a partir de $T_0$ colapsando cualquier cantidad de nodos internos (no terminales) del arbol. Se indexan los nodos terminales de $T$ por $m$, cada uno representando una región $R_m$, adicionalmente $|T|$ representa la cantidad de nodos terminales de $T$.\n",
    "\n",
    "Se define:\n",
    "\n",
    "\\begin{align}\n",
    "N_m =& |R_m|\\\\\n",
    "\\hat{x}_m =& \\frac{1}{N_m} \\sum_{i:x_i \\in R_m} y_i \\\\\n",
    "Q_m(T) =& \\frac{1}{N_m} \\sum_{i:x_i \\in R_m} (y_i - \\hat{c}_m)^2\n",
    "\\end{align}\n",
    "\n",
    "Con esto se define el criterio de **costo-complejidad** cómo:\n",
    "\n",
    "\\begin{equation}\n",
    "C_{\\alpha}(T) = \\sum_{m=1}^{|T|} N_m Q_m(T) + \\alpha |T| \n",
    "\\end{equation}\n",
    "\n",
    "La idea es encontrar para cada $\\alpha$ el sub arbol $T_\\alpha \\subset T_0$ que minimice $C_\\alpha(T)$. Se observa que $\\alpha \\geq 0$ funciona cómo un termino de penalización sobre el tamaño del arbol, valores grandes de $\\alpha$ inducen arboles más pequeños. Si $\\alpha= 0$ entonces se recupera $T_0$.\n",
    "\n",
    "Para cada $\\alpha$ se puede demostrar que existe un único $T_\\alpha$ de tamaño mínimo que minimiza $C_\\alpha$. Para encontrar $T_\\alpha$ se usa la estrategía de podar el nodo que produce el menor incremento posible en $\\sum_{m=1}^{|T|} N_m Q_m(T)$ hasta llegar a la raíz. Esto genera una secuencia finita de sub arboles, es posible demostrar que esta secuencia contine necesariamente a $T_\\alpha$.\n",
    "\n",
    "Para estimar el valor de $\\alpha$ es posible recurrir a un esquema de validación cruzada (se recomienda 5-10) sobre un conjunto de validación. Finalmente se elige $\\hat{\\alpha}$ que minimíce la suma de errores cuadráticos en el esquema de validación cruzada. Finalmente el arbol óptimo será $T_\\hat{\\alpha}$.\n",
    "\n",
    "Las **ventajas** de los este método se consisten en su capacidad para manejar variables numéricas y categóricas de manera nativa. Además requieren de un procesamiento casi nulo de los datos, son además modelos no paramétricos y se comporta bien en altas dimensiones. Por otra parte, las **desventajas** o limitaciones de esta familia de modelos recae en su alta propensión al sobreajuste y alta fragilidad en cuanto a cambios en el conjunto de datos. Por lo general, al trabajar con clases no balanceadas se producen problemas de sesgo de manera muy notoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Observando de la formulación del modelo, ¿Qué significa que una variable sea seleccionada al comienzo para particionar? investigue sobre el atributo `feature_importances_` que poseen los modelos basados en arboles de Scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo**\n",
    "\n",
    "Se genera un modelo basado en arboles de para estimar el precio en la base `HousePricing` en función de ciertas variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest = [\n",
    "    'SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF',\n",
    "    'FullBath', 'YearBuilt'\n",
    "]\n",
    "\n",
    "interest = indexer(interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se importa la clase `DecisionTreeRegressor` para hacer regresión sobre el precio de venta. Se genera además un conjunto de entrenamiento en con las variables de interés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse(y,y_pred):\n",
    "    return np.sqrt(mean_squared_error(y,y_pred))\n",
    "    \n",
    "tree_reg = DecisionTreeRegressor()\n",
    "\n",
    "# Conjuto de entrenamiento\n",
    "df_train_tree = df_train[interest].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se entrena un modelo sobre los datos sin optimizar hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separacion de datos en train y test usando pandas\n",
    "X_train_tree = df_train_tree[interest[:-1]].sample(frac=.7)\n",
    "Y_train_tree = df_train_tree.loc[X_train_tree.index,interest[-1]]\n",
    "\n",
    "test_idx = df_train_tree.index.difference(X_train_tree.index)\n",
    "\n",
    "X_test_tree = df_train_tree.loc[test_idx, interest[:-1]]\n",
    "Y_test_tree = df_train_tree.loc[test_idx,interest[-1]]\n",
    "\n",
    "# Se entrena el modelo\n",
    "tree_reg.fit(X_train_tree, Y_train_tree)\n",
    "\n",
    "# Se obtienen metricas de rendimiento en train\n",
    "\n",
    "train_res = { 'R2': tree_reg.score(X_train_tree,Y_train_tree), \n",
    "              'rmse':  rmse(tree_reg.predict(X_train_tree),Y_train_tree)}\n",
    "train_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se obtienen el métricas de rendimiento sobre test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = { 'R2': tree_reg.score(X_test_tree,Y_test_tree), \n",
    "            'rmse':  rmse(tree_reg.predict(X_test_tree),Y_test_tree)}\n",
    "test_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "si bien los resultados en entrenamiento parecen prometedores, en test se observa un baja capacidad de generalización, lo cual es una clara señal de sobreajuste, como era de esperar en este tipo de modelos. Para mejorar la capacidad de generalización haremos uso de un esquema de búsqueda por grilla según validación cruzada, para esto se importa el módulo `model_selection` especificando la función `GridSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esta función toma como argumento un estimador, una grilla de hiperparámetros especificada como un diccionario, además se puede especificar la cantidad de conjuntos train / test que se quiere generar por medio de `cv` y la cantidad de procesos en paralelo que se realizan `n_jobs` entre otros parámetros. Se aplica el esquema de grilla sobre un regresor basado en arboles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg = DecisionTreeRegressor(random_state=6202)\n",
    "\n",
    "grid = {\n",
    "    'max_depth': range(2, 11),\n",
    "    'ccp_alpha': np.power(2, np.linspace(-5,7,num = 12)),\n",
    "    'min_samples_split': range(2, 6),\n",
    "    'min_samples_leaf': range(2, 6)\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(tree_reg, param_grid=grid, cv=5, n_jobs=-1)\n",
    "\n",
    "cv.fit(X_train_tree, Y_train_tree)\n",
    "\n",
    "tree_reg_best = cv.best_estimator_\n",
    "tree_reg_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se almacena el modelo obtenido en un formato accesible, esto se denomina **persistencia de modelos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tree_model.pkl','bw') as handler:\n",
    "    pickle.dump(tree_reg_best,handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se recolectan las métricas de rendimiento correspondientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res_gs = { 'R2': tree_reg_best.score(X_train_tree, Y_train_tree), \n",
    "               'rmse': rmse(tree_reg_best.predict(X_train_tree),Y_train_tree)}\n",
    "\n",
    "print('Resultados train: \\n', train_res_gs , '\\n')\n",
    "\n",
    "test_res_gs = { 'R2': tree_reg_best.score(X_test_tree,Y_test_tree), \n",
    "              'rmse': rmse(tree_reg_best.predict(X_test_tree),Y_test_tree)}\n",
    "\n",
    "print('Resultados test: \\n', test_res_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con lo que se observa cierto sobreajuste pero a la vez una mejora en la capacidad de generalización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. ¿Cómo se producen las particiones train / test en el esquema automatizado por `GridSearchCV` para `cv = 5`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clasificación\n",
    "\n",
    "El esquema es homólogo al de regresión, solamente se deben ajustar los criterios de separación de nodos y el podado de estos.\n",
    "\n",
    "Cómo se trata de un problema de clasificación, los valores de la respuesta $y_i$ toma valores en un conjuntos discretos $1,\\ldots, K$.\n",
    "\n",
    "En el problema anterior se tomó como criterio de **pureza** dentro de los nodos la suma de errores cuadráticos, esto no es apropiado para clasificación.\n",
    "\n",
    "En un nodo $m$ del árbol, que representa la región $R_m$ con $N_m$ observaciones, se define:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{p}_{mk} = \\frac{1}{N_m} \\sum_{i: x_i \\in R_m} I(y_i = k)\n",
    "\\end{equation}\n",
    "\n",
    "como la proporción de observaciones de la clase $k$ en el nodo $m$. Se clasifican los puntos en el nodo $m$ según:\n",
    "\n",
    "\\begin{equation}\n",
    "k(m) = \\arg \\max_k \\hat{p}_{mk}\n",
    "\\end{equation}\n",
    "\n",
    "es decir, se asignan los puntos en el nodo $m$ a la clase que mayor presencia tenga en el nodo.\n",
    "\n",
    "Para el problema de clasificación se suele usar distintas forma de $Q_m(T)$ (pureza del nodo), los más comones son:\n",
    "\n",
    "- **Error de clasificación:**\n",
    "\\begin{align}\n",
    "Q_m^1(T) =& \\frac{1}{N_m} \\sum{i \\in R_m} I(y_i \\neq k(m))\\\\\n",
    "=& 1- \\hat{p}_{mk(m)}\n",
    "\\end{align}\n",
    "\n",
    "- **Coeficiente de Gini:**\n",
    "\\begin{align}\n",
    "Q_m^2(T) =& \\sum_{k \\neq k'} \\hat{p}_{mk} \\hat{p}_{mk'}\\\\\n",
    "=& \\sum_{k=1}^K \\hat{p}_{mk}(1 - \\hat{p}_{mk})\\\\\n",
    "\\end{align}\n",
    "\n",
    "- **Entropía:**\n",
    "\\begin{equation}\n",
    "Q_m^3(T) = -\\sum_{k=1}^K \\hat{p}_{mk}\\log\\hat{p}_{mk}\n",
    "\\end{equation}\n",
    "\n",
    "Las ventajas de $Q_m^2$ y $Q_m^3$ sobre $Q_m^1$, es que estos son diferenciables, lo cual puede ser fundamental para esquemas de optimización numéricos.\n",
    "\n",
    "Cabe mencionar que que los semi planos utilizados para generar las bifurcaciones en cada nodo del arbol no es necesario que sean de la forma $\\{ x \\in X | x_j \\geq s\\}$, tambien es posible considerar combinaciones lineales de las características. Esto por lo general puede mejorar la precisión pero se pierde por otro lado interpretabilidad del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Implemente un modelo basado en árboles para predecir una separación ordinal de `SalePrice` en la base `HousePricing` usando las variables de interés utilizadas en la sección anterior. Recuerde generar un esquema que evite el sobreajuste del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos envolventes de Selección de Características\n",
    "\n",
    "Esta clase métodos de selección de caráterísticas utilizan un estimador externo y utilizan estrategias de búsqueda sobre suconjuntos de variables optimizando las métricas de rendimiento del estimador externo. Se puede utilizar cualquier método de aprendizaje en conjunción a este tipo de métodos. Dentro de las estrategias de búsqueda se encuentran:\n",
    "\n",
    "* **Selección hacia adelante**: Se comienza con un arreglo vacío, se entrenan modelos de predicción usando conjuntos de una característica para seleccionar aquella que con mayor rendimiento. El proceso se repite de manera secuencial.\n",
    "\n",
    "Para implementar este tipo de métodos se puede hacer uso de la librería `mlxtend` de Python, esta librería permite extender ciertas funcionalidades de Scikit-learn y por tanto es completamente compatible con sus clases. Se importa el método de selección hacia adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('df_train_v1.csv', header=[0,1], index_col=0)\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer, RobustScaler\n",
    "from sklearn.preprocessing import PowerTransformer, PolynomialFeatures\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se define un objeto `Pipeline` para transformación de columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trabajaremos con un modelo basado en árboles sobre la base de datos de `HousePricing`. En primera instancia se tratan las variables numéricas, estas se estandarizan de manera robusta y se filtran según asimetria estadística. Las variables con mayor asimetria son transformadas según una transformación de potencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables numericas\n",
    "def custom_scaler(df):\n",
    "    '''Escala y retorna objeto DataFrame'''\n",
    "    scaler = RobustScaler()\n",
    "    return pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "\n",
    "def skew_selector(df, skew_val=0.75):\n",
    "    '''Reconoce columnas asimetricas.\n",
    "    \n",
    "    Obs: recibe solo valores de df_train[numeric]\n",
    "    \n",
    "    '''\n",
    "    return [\n",
    "        df.loc[:, df.columns[df.skew() > skew_val]],\n",
    "        df.loc[:, df.columns[df.skew() <= skew_val]]\n",
    "    ]\n",
    "\n",
    "\n",
    "def custom_power_transform(df_list):\n",
    "    '''Escala solo aquellas columnas con alta asimetria.'''\n",
    "\n",
    "    scaler = PowerTransformer(method='yeo-johnson')\n",
    "    transformed = pd.DataFrame(scaler.fit_transform(df_list[0]),\n",
    "                               columns=df_list[0].columns)\n",
    "    intact = df_list[1]\n",
    "\n",
    "    return pd.concat((transformed, intact),axis=1, sort=False)\n",
    "\n",
    "\n",
    "scaler = FunctionTransformer(func=custom_scaler)\n",
    "to_yj = FunctionTransformer(func=skew_selector)\n",
    "num_transform = FunctionTransformer(func=custom_power_transform)\n",
    "\n",
    "num_pipe = Pipeline(steps=[('estandariza',\n",
    "                            scaler), ('select by skew',\n",
    "                                      to_yj), ('normaliza', num_transform)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables categóricas y ordinales son codificadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables ordinales\n",
    "ordinals = df_train['ordinal']\n",
    "ordinal_cat = [[*ordinals[col].unique()] for col in ordinals.columns]\n",
    "\n",
    "ord_pipe = Pipeline(\n",
    "    steps=[('ordinal', OrdinalEncoder(categories = ordinal_cat))])\n",
    "\n",
    "# Variables categoricas\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "cat_pipe = Pipeline(steps=[('OneHot', enc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se agregar caracteristicas por interacción de grado 2 usando `PolynomialFeatures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = PolynomialFeatures(degree=2,interaction_only=False ,include_bias=False)\n",
    "extra_features = Pipeline(steps=[('poli_interactions', interactions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se unen los objetos `Pipeline` definidos para producir un dataset unificado, para ello se utiliza `ColumnTransformer` del módulo `compose`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion auxiliar\n",
    "def train_indexer(cols, t_c=df_train.columns):\n",
    "    '''Genera columnas multinivel a partir de nombres de columna planos.'''\n",
    "\n",
    "    set_to_tuple = set(*[cols])\n",
    "\n",
    "    tuples = [i for i in t_c if set_to_tuple.intersection(set(i))]\n",
    "\n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define el proceso de preprocesamiento y obtención de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = train_indexer(df_train['numeric'].columns)\n",
    "num_cols.remove(('numeric','SalePrice'))\n",
    "\n",
    "cat_cols = train_indexer(df_train['categorical'].columns)\n",
    "ord_cols = train_indexer(df_train['ordinal'].columns)\n",
    "\n",
    "# Preprocesamiento segun tipo de dato\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('prepr_num', num_pipe, num_cols), \n",
    "                  ('prepr_cat', cat_pipe, cat_cols), \n",
    "                  ('prepr_ord', ord_pipe, ord_cols)], n_jobs = -1)\n",
    "\n",
    "# Se agregan las interacciones\n",
    "trans_process = Pipeline(\n",
    "    steps=[('prepr', preprocessor), ('interactions', extra_features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que el número de carácteristicas que genera nuestro flujo es casi el más del doble que la cantidad de observaciones que se poseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(trans_process.fit_transform(df_train))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se diseña un esquema de filtrado de caracterísitcas. En primera instancia se filtran eliminando las variables con menor varianza cuya varianza acumulada sea inferior al 1% de la varianza total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_sorted = X.var(axis=1).sort_values()\n",
    "variance_thr = vars_sorted[vars_sorted.cumsum() <= vars_sorted.sum()*.01].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a generar una selector de características basado en el umbral definido anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "var_selector = VarianceThreshold(threshold=variance_thr)\n",
    "\n",
    "var_selector.fit_transform(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. ¿Se puede hacer lo anterior usando `SelectPercentile`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente, se obtiene que de las 26334 variables generadas, solo 2778 (el 10% aproximadamente) aportan el 99% de la varianza total. A continuación se filtran características según distintos coeficientes de correlación al comparar con la variable de respuesta, se filtran además según información mutua. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, mutual_info_regression\n",
    "from scipy.stats import kendalltau, spearmanr, pearsonr\n",
    "\n",
    "# Se define un decorador para que las metricas de scipy sean compatibles\n",
    "def mask(func):\n",
    "    '''Decorador especfico para compatibilizar scipy.stats con dataframes.'''\n",
    "    def wrapper(X, y, func=func):\n",
    "        '''Toma la funcion func y le permite operar sobre la columnas de X.'''\n",
    "\n",
    "        res = [func(X[:, i], y)[0] for i in range(X.shape[1])]\n",
    "        return np.array(res)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "# Se decoran las funciones de scipy\n",
    "kendalltau = mask(kendalltau)\n",
    "pearsonr = mask(pearsonr)\n",
    "spearmanr = mask(spearmanr)\n",
    "\n",
    "# Se opera sobre X e y\n",
    "y_train = df_train['numeric'].SalePrice.copy()\n",
    "\n",
    "# Se declaran los selectores\n",
    "mi_selector = SelectPercentile(score_func=mutual_info_regression,\n",
    "                               percentile=20)\n",
    "\n",
    "kendall_selector = SelectPercentile(score_func=kendalltau, percentile=20)\n",
    "\n",
    "spearman_selector = SelectPercentile(score_func=spearmanr, percentile=20)\n",
    "\n",
    "pearson_selector = SelectPercentile(score_func=pearsonr, percentile=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los selectores definidos anteriormente se genera un objeto `FeatureUnion`, este permite concatenar los resultados de cada selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "selectors = FeatureUnion(\n",
    "    transformer_list=[('mutual info', mi_selector), \n",
    "                      ('Kendall corr', kendall_selector), \n",
    "                      ('Spearman corr',spearman_selector),\n",
    "                      ('pearson corr',pearson_selector)]\n",
    "                      , n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "el proceso anterior se unifica en un objeto `Pipeline` que incorpora el proceso de filtrado por varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_pipe = Pipeline(steps=[('var filter', var_selector),\n",
    "                              ('corr mi selectors', selectors)]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ajusta el proceso de filtrado y se remueven las columnas duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered = pd.DataFrame(filter_pipe.fit_transform(X,y_train))\n",
    "X_filtered = X_filtered.T.drop_duplicates().T\n",
    "X_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered = pd.read_csv('X_filtered.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered.to_csv('X_filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde se observa que han ido seleccionadas 916 variables del total. Sobre esta cantidad de variables se realiza un esquema de **selección hacia adelante** utilizando `mlextend`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pickle\n",
    "\n",
    "n_cols = (7,20)\n",
    "\n",
    "# Se inicializa el regresor\n",
    "with open('tree_model.pkl', 'br') as handler:\n",
    "    tree_reg_sfs = pickle.load(handler)\n",
    "\n",
    "# Se inicializa el buscador\n",
    "forward_selector = sfs(tree_reg_sfs,\n",
    "                       k_features=n_cols,\n",
    "                       forward=True,\n",
    "                       scoring='r2',\n",
    "                       cv=15,\n",
    "                       n_jobs=-1,\n",
    "                       verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a hacer la selección utilizando como métrica de rendimiento el puntaje `R2` y haciendo validación cruzada de 5 folds. Se inicializa el regresor con los hiperparametros encontrados anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lento\n",
    "forward_selector.fit(X_filtered, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtienen las caractetisticas seleccionadas por medio del método `.k_features_names_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered.columns[f_selector.k_feature_names_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abre el modelo generado -> sobreescribe f_selector\n",
    "with open('forward_selector.pkl', 'br') as handler:\n",
    "    f_selector = pickle.load(handler)\n",
    "\n",
    "X_filtered.loc[:,map(str,f_selector.k_feature_names_)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda el modelo generado -> sobreescribe f_selector\n",
    "import pickle\n",
    "\n",
    "with open('forward_selector.pkl','bw') as handler:\n",
    "    pickle.dump(forward_selector, handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se repite el proceso de validación cruzada utilizando un modelo de arboles de desición, esta vez, sobre los datos con caracterísitcas filtradas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg = DecisionTreeRegressor(random_state=6202)\n",
    "\n",
    "X_train_tree, X_test_tree, Y_train_tree, Y_test_tree = train_test_split(\n",
    "    X_filtered.loc[:,map(str,f_selector.k_feature_names_)],\n",
    "    y_train,\n",
    "    test_size=.2)\n",
    "#\n",
    "grid = {\n",
    "    'max_depth': range(7,13),\n",
    "    'ccp_alpha': np.power(2, np.linspace(14, 16, num=2)),\n",
    "    'min_samples_leaf': range(3, 10),\n",
    "    'min_impurity_decrease': np.power(2, np.linspace(12.5, 13, num=3))\n",
    "}\n",
    "\n",
    "\n",
    "cv = GridSearchCV(tree_reg, param_grid=grid, cv=5, n_jobs=7)\n",
    "\n",
    "cv.fit(X_train_tree, Y_train_tree)\n",
    "\n",
    "tree_reg_best = cv.best_estimator_\n",
    "tree_reg_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comparan los resultados de test y train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_res_gs = {\n",
    "    'R2': tree_reg_best.score(X_train_tree, Y_train_tree),\n",
    "    'rmse': rmse(tree_reg_best.predict(X_train_tree), Y_train_tree)\n",
    "}\n",
    "\n",
    "print('Resultados train: \\n', train_res_gs, '\\n')\n",
    "\n",
    "test_res_gs = {\n",
    "    'R2': tree_reg_best.score(X_test_tree, Y_test_tree),\n",
    "    'rmse': rmse(tree_reg_best.predict(X_test_tree), Y_test_tree)\n",
    "}\n",
    "\n",
    "print('Resultados test: \\n', test_res_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene una mejora notoria, con claros signos de una reducción en el sobre ajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se abre el mejor modelo\n",
    "with open('sfs_tree_reg', 'br') as handler:\n",
    "    tree_reg_best = pickle.load(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se guarda el mejor modelo\n",
    "with open('sfs_tree_reg','bw') as handler:\n",
    "    pickle.dump(tree_reg_best,handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Selección hacia atras\n",
    "\n",
    "Es análogo a la selección hacia adelante, en este procedimiento se comienza con todas las características y se elimina de manera secuancial aquellas con menores métricas de rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Utilice `SequentialFeatureSelector` para hacer una selección de variables hacia atrás.\n",
    "\n",
    "2. Utilice la función `export_graphviz` para exportar un modelo entrenado de árbol de desición a un archivo en el archivo `tree.dot`. Abra el archivo usando el context manager `open(tree.dot)` en conjunción con la orden `read()`, almacene el resultado en la variable `tree_viz`. Utilice la clase `Source` de la librería `graphviz`  con la variable `tree_viz` como argumento. El resultado de estas acciones generan una visualización del árbol elegido. ¿Se puede obtener información sobre el comportamiento de las características?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Selección por eliminiación reursiva (RFE)\n",
    "\n",
    "Este método recibe un estimado externo, a diferencia de las técnicas anteriores, este estimador debe asociar pesos a sus características (ej: modelo lineal) o importancia de características (*feature importance*, ej: modelos basados en árboles). El procedimiento de selección de características consiste en *seleccionar hacia atrás* dado un conjunto inicial de características, a cada característica se le asocia un puntaje basándose en su coeficiente o importancia (proporcionados por el modelo externo) . Las caracteísticas con menor puntaje son eliminadas, el proceso se repite de manera recurrente. \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se efectúa una selección de caraterísticas por RFE sobre el datset `HousePricing` transformado, para ello se utiliza un modelo basado en árboles con los hiperparámetros ya encontrados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "n_cols = 20\n",
    "\n",
    "# Se inicializa el regresor\n",
    "with open('tree_model.pkl', 'br') as handler:\n",
    "    tree_reg_rfe = pickle.load(handler)\n",
    "\n",
    "# Se inicializa el buscador\n",
    "rfe_selector = RFECV(tree_reg_rfe,\n",
    "                     min_features_to_select=n_cols,\n",
    "                     scoring='r2',\n",
    "                     cv=5,\n",
    "                     n_jobs=7,\n",
    "                     verbose=1,\n",
    "                     step=129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_selector.fit(X_filtered, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se guarda la selección generada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se abre --> reescribe rfe_selector\n",
    "with open('rfe_selection.pkl','br') as handler:\n",
    "    rfe_selector = pickle.load(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se guarda --> reescribe selector_rfe.pkl\n",
    "with open('rfe_selection.pkl','bw') as handler:\n",
    "    pickle.dump(rfe_selector,handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg_rfe = DecisionTreeRegressor(random_state=6202)\n",
    "\n",
    "X_train_tree, X_test_tree, Y_train_tree, Y_test_tree = train_test_split(\n",
    "    X_filtered.loc[:, rfe_selector.support_],\n",
    "    y_train,\n",
    "    test_size=.2)\n",
    "\n",
    "grid = {\n",
    "    'max_depth': range(7,13),\n",
    "    'ccp_alpha': np.power(2, np.linspace(14, 16, num=2)),\n",
    "    'min_samples_leaf': range(3, 10),\n",
    "    'min_impurity_decrease': np.power(2, np.linspace(12.5, 13, num=3))\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(estimator = tree_reg_rfe, param_grid=grid, cv=5, n_jobs=-1, verbose = 1)\n",
    "\n",
    "cv.fit(X_train_tree, Y_train_tree)\n",
    "\n",
    "tree_reg_rfe_best = cv.best_estimator_\n",
    "tree_reg_rfe_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res_gs = {\n",
    "    'R2': tree_reg_rfe_best.score(X_train_tree, Y_train_tree),\n",
    "    'rmse': rmse(tree_reg_rfe_best.predict(X_train_tree), Y_train_tree)\n",
    "}\n",
    "\n",
    "print('Resultados train: \\n', train_res_gs, '\\n')\n",
    "\n",
    "test_res_gs = {\n",
    "    'R2': tree_reg_rfe_best.score(X_test_tree, Y_test_tree),\n",
    "    'rmse': rmse(tree_reg_rfe_best.predict(X_test_tree), Y_test_tree)\n",
    "}\n",
    "\n",
    "print('Resultados test: \\n', test_res_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde se obtienen resultados similares a los métodos de selección anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se abre el mejor modelo\n",
    "with open('tree_reg_rfe.pkl','br') as handler:\n",
    "    tree_reg_rfe_best = pickle.load(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se guarda el mejor modelo para las características seleccionadas\n",
    "with open('tree_reg_rfe.pkl','bw') as handler:\n",
    "    pickle.dump(tree_reg_rfe_best, handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "La clase `TransformedTargetRegressor` del módulo `compose` permite entrenar regresores para los cuales se transforma el output según una función proporcionada (ej: `PowerTransformer`). Si se desea trabajar con búsqueda por grilla utilizando validación cruzada, es necesario entregar un diccionario de la forma:\n",
    "\n",
    "```python\n",
    "\n",
    "param_grid = {'regressor__HIPERPARAMETRO_1':value_range_1,\n",
    "              'regresor__HIPERPARAMETRO_2':value_range_2,\n",
    "              ...\n",
    "              'regressor__HIPERPARAMETRO_n':value_range_n,\n",
    "              }\n",
    "```\n",
    "\n",
    "En este caso `HIPERPARAMETRO_n` hace referencia a un hiperparámetro que se desea inspeccionar en el rango de valores `range_n` usando búsqueda por grilla `GridSearchCV`. \n",
    "\n",
    "1. Genere un modelo que haga una transformación sobre `y_train` cambiando su distribución a una distribución normal. Utilice un modelo basado en árboles de regresión, haga una búsqueda por grilla en al menos 3 hiperparámetros. Considerando que se seleccionaron características en función de la variable no transfromada, ¿espera que haya un mejor rendimiento en este modelo?\n",
    "\n",
    "**Obs**: Observe que un objeto tipo `TransformedTargetRegressor` poseen el atributo `regressor`. Para modificar los hiperparámetros de tal atributo (cuyo valor es un objeto `Estimator`) dentro de un objeto `GridSearchCV` se debe hacer uso de una notación especial, que consiste en generar diccionarios de la forma `regressor__HIPERPARAMETRO_n:value_n`. Si no se utiliza esta notación, el buscador `GridSearchCV` intentará modificar los hiperparámetros de el transformador `TransformedTargetRegressor` y no del regresor de interés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducción de Dimensionalidad\n",
    "\n",
    "Otra manera de seleccionar características es por medio de técnicas proyectivas o transformaciones desde espacios de mayor dimensión a otros de menor dimenisón. Dentro de las ténicas clasicas en este apartado, se tiene el analísis de componentes principales **PCA** por sus siglas en ingles.\n",
    "\n",
    "### PCA\n",
    "\n",
    "Este método, a grandes rasgos, consiste en ajustar un elipsoide $L$-dimensional sobre el conjunto de datos. Los ejes del elipsoide que son grandes en magnitud indican una dirección en la que hay gran variabilidad en los datos, en contraste, los ejes pequeños indican poca variabilidad. Considerando que los ejes de un elipsoide $L$-dimensional generan una base ortogonal para el espacio vectorial en el cual está contenida, al usar como representación los datos proyectados sobre los ejes con mayor magnitud, es posible construir una transformación de los datos que conserve gran parte de la variabilidad, pero que tenga menor dimensión.\n",
    "\n",
    "Para aplicar este método es necesario que los datos tengan media 0 en cada componente, es decir, el conjunto de datos esté centrado respecto a $\\vec{0}$. El procedimiento es sensible a la escala de los datos, pero no hay consenso general respecto a cuál es el mejor escalamiento para obtener una representación óptima.\n",
    "\n",
    "Sea un conjunto de datos $\\left \\{ x_i \\right \\}_{i=1}^N$, los datos se organizan en una matriz $X = \\left ( x_1,x_2,\\ldots,x_N \\right )^T \\in R^{N \\times L}$ con media empírica 0 en cada una de sus columnas.\n",
    "\n",
    "La idea es encontrar una dirección tal que proyectados los datos sobre esta, maximice la varianza, es decir, se busca $w_1$ tal que:\n",
    "\n",
    "\\begin{equation}\n",
    "w_1 = \\underset{\\left \\| w \\right \\| = 1}{\\arg\\max} \\hspace{1mm} \\sum_{i=1}^N (x_i^T w)^2\n",
    "\\end{equation}\n",
    "\n",
    "Escrito de forma matricial el problema es:\n",
    "\n",
    "\\begin{equation}\n",
    "w_1 = \n",
    "\\underset{\\left \\| w \\right \\| = 1}{\\arg\\max} \\hspace{1mm} \\left \\| Xw \\right \\|^2 =\n",
    "\\underset{\\left \\| w \\right \\| = 1}{\\arg\\max} \\hspace{1mm} w^T X^T X w = \n",
    "\\underset{}{\\arg\\max} \\hspace{1mm} \\frac{w^T X^T X w}{w^T w}\n",
    "\\end{equation}\n",
    "\n",
    "Un resultado clásico para matrices semi definidas positivas como $X^T X$, es que la cantidad $\\left \\| Xw \\right \\| $ está acotada por $\\lambda_1$, el mayor valor propio de $X^T X$ y esto se alcanza cuando $w$ es igual al vector propio asociado a $\\lambda_1$.\n",
    "\n",
    "Ya obtenido el primer eje principal, para obtener el $k$-ésimo se procede substrayendo las $k-1$ componentes principales ya extraídas de $X$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{X}_k = X - \\sum_{i=1}^{k-1} X w_i^T\n",
    "\\end{equation}\n",
    "\n",
    "Por lo tanto la $k$-ésima componente principal $w_k$ es la solución del problema:\n",
    "\n",
    "\\begin{equation}\n",
    "w_k = \n",
    "\\underset{\\left \\| w \\right \\| = 1}{\\arg\\max} \\hspace{1mm} \\left \\| \\hat{X}_kw \\right \\|^2 =\n",
    "\\underset{}{\\arg\\max} \\hspace{1mm} \\frac{w^T \\hat{X}_k^T \\hat{X}_k w}{w^T w}\n",
    "\\end{equation}\n",
    "\n",
    "La solución a este problema resulta ser el $k$-ésimo vector propio de la matriz $X^T X$.\n",
    "\n",
    "Finalmente encontrar las componentes principales se reduce a diagonalizar la matriz de covarianza de los datos, es decir, una descomposición de la forma:\n",
    "\n",
    "\\begin{equation}\n",
    "X^T X = W \\Lambda W^T\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Donde $W = \\left ( w_1, \\ldots, w_L \\right ) $ es la matriz que tiene los vectores propios de $X^T X$ en sus columnas y $\\Lambda$ es una matriz diagonal con los valores propios ordenados de forma decreciente. De esta forma la descomposición en componentes principales de los datos queda dada por:\n",
    "\n",
    "\\begin{equation}\n",
    "T = X W \\in \\mathbb{R}^{N \\times L}\n",
    "\\end{equation}\n",
    "\n",
    "Para reducir los datos a $K$ dimensiones, se toma la matriz $W_K = \\left ( w_1, \\ldots, w_K \\right )$ con los $K$ vectores propios con mayor valor propio y se proyectan los datos sobre estos:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "T_K = X W_K  \\in \\mathbb{R}^{N \\times K}\n",
    "\\end{equation}\n",
    "\n",
    "Para aplicar este método de manera computacional se hace uso de la clase `PCA` de módulo `decomposition` de Scikit-learn.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se utilizan las componentes filtradas por varianza y correlación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered = pd.read_csv('X_filtered.csv', index_col=0)\n",
    "X_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que aunque el conjunto de datos tenga 916 caraterísticas, el 99% de la varianza del dataset se puede explicar con 133 variables proyectadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 133\n",
    "pca = PCA(n_components = n_components)\n",
    "\n",
    "X_pca = pca.fit_transform(X_filtered)\n",
    "\n",
    "pca.explained_variance_ratio_.cumsum()[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se estudia el aporte de varianza por componente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(explained_variance)\n",
    "\n",
    "plt.figure(1, figsize=(15,6))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.plot(pca.components_[i])\n",
    "    plt.title(f'C. principal {i+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se observan proyecciones de 2 dimensiones basadas en las 3 primeras componentes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 133\n",
    "pca = PCA(n_components = n_components)\n",
    "\n",
    "bins = pd.cut(y_train,2)\n",
    "\n",
    "X_pca = pca.fit_transform(X_train)\n",
    "fig, ax = plt.subplots(3,1, figsize=[9,15])\n",
    "\n",
    "sns.scatterplot(X_pca[:,0], X_pca[:, 1], hue=bins,ax=ax[0])\n",
    "ax[0].set_title('Comp. 1 vs Comp. 2')\n",
    "\n",
    "sns.scatterplot(X_pca[:,0], X_pca[:, 2], hue=bins,ax=ax[1])\n",
    "ax[1].set_title('Comp. 1 vs Comp. 3')\n",
    "\n",
    "sns.scatterplot(X_pca[:,1], X_pca[:, 2], hue=bins,ax=ax[2])\n",
    "ax[2].set_title('Comp. 2 vs Comp. 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-PCA\n",
    "\n",
    "El presente método consiste en una extensión de PCA, que explotando el llamado \"truco del kernel\" permite realizar reducción de dimensionalidad en un espacio de características transformado. Esto permite capturar de mejor forma la estructura de datos que presentan comportamiento no lineal, algo que no es capás de llevar a cabo la versión clásica de PCA, limitado por la naturaleza de las operaciones lineales que componen el procedimiento.\n",
    "\n",
    "El método se apoya en la utilización de una función no lineal que mapea el conjunto de datos original a un espacio de características, con dimensión usualmente mayor:\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi: \\mathbb{R}^L \\rightarrow \\mathbb{R}^M\n",
    "\\end{equation}\n",
    "\n",
    "la intuición de esto es que un conjunto de datos que no es linealmente separable, puede ser separable en un espacio de mayor dimensión utilizando una transformación.\n",
    "\n",
    "\n",
    "Sea un conjunto de datos $\\left \\{ x_i \\right \\}_{i=1}^N \\subset \\mathbb{R}^L $, los desarrollos realizados en la sección anterior pueden ser reproducidos en el espacio transformado por la aplicación $\\phi$. Por simpleza se asume que la media empírica de los datos es 0, es decir:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{N} \\sum_{i=1}^N \\phi(x_i) = 0\n",
    "\\end{equation}\n",
    "\n",
    "de no ser así se puede sustraer la media empírica a la transformación para corregir esto.\n",
    "\n",
    "La matriz de covarianza de los datos transformados es:\n",
    "\n",
    "\\begin{equation}\n",
    "C = \\frac{1}{N} \\sum_{i=1}^N \\phi(x_i) \\phi(x_i)^T\n",
    "\\end{equation}\n",
    "\n",
    "Para $k = 1, \\ldots, M$ el problema de auto valores para $C$ es:\n",
    "\n",
    "\\begin{align}\n",
    "C v_k =& \\lambda_k v_k \\\\\n",
    "\\frac{1}{N} \\sum_{i=1}^N \\phi(x_i) \\{ \\phi(x_i)^T v_k \\} =& \\lambda_k v_k\n",
    "\\end{align}\n",
    "\n",
    "esto puede ser re escrito como:\n",
    "\n",
    "\\begin{equation}\n",
    "v_k = \\sum_{i=1}^N a_{ki}\\phi(x_i)\n",
    "\\end{equation}\n",
    "\n",
    "Al juntar las 2 ecuaciones anteriores se obtiene:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{N} \\sum_{i=1}^N \\phi(x_i)  \\phi(x_i)^T  \\sum_{j=1}^N a_{kj}\\phi(x_j) = \\lambda_k \\sum_{i=1}^N a_{ki}\\phi(x_i)\n",
    "\\end{equation}\n",
    "\n",
    "Se define la función de Kernel:\n",
    "\n",
    "\\begin{equation}\n",
    "k(x_i, x_j) = \\phi(x_i)^T \\phi(x_j)\n",
    "\\end{equation}\n",
    "\n",
    "se multiplica la ecuación anterior por $\\phi(x_l)^T$, obteniendo:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{N} \\sum_{i=1}^N k(x_l,x_i)  \\sum_{j=1}^N a_{kj} k(x_i,x_j) = \\lambda_k \\sum_{i=1}^N a_{ki}k(x_l,x_i)\n",
    "\\end{equation}\n",
    "\n",
    "Llevando esto a notación matricial:\n",
    "\n",
    "\\begin{equation}\n",
    "K^2 a_k = \\lambda_k N K a_k\n",
    "\\end{equation}\n",
    "\n",
    "Donde $K_{i,j} = k(x_i, x_j)$ y $a_k = ( a_{k1}, \\dots, a_{kN}) ^T \\in \\mathbb{R}^N$, el cual puede ser resuelto en:\n",
    "\n",
    "\\begin{equation}\n",
    "K a_k = \\lambda_k N a_k\n",
    "\\end{equation}\n",
    "\n",
    "y finalmente la transformación en componentes principales será:\n",
    "\n",
    "\\begin{equation}\n",
    "y_k(x) = \\phi(x)^T v_k = \\sum_{i=1}^N a_{ki} k(x, x_i)\n",
    "\\end{equation}\n",
    "\n",
    "Todo este preámbulo fue hecho para que fuera visible el hecho que en esta construcción se pasó de un problema de valores propios con dimensión $M$ (espacio de características trasnsformado) a uno con dimensión $N$ ($a_k$ tiene dimensión igual a la cantidad de muestras). En este sentido este método puede ser computacionalmente prohibitivo si la cantidad de muestras es demasiado grande, pero se evita tener que evaluar la tranformación $\\phi$ y calcular el producto punto, por la evaluación del kernel $k$ en el conjunto de entrenamiento.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se puede implementar el método K-PCA usando el objeto `KernelPCA` del módulo `decomposition` de Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "comps = 100\n",
    "gamma = 1e-4\n",
    "kpca = KernelPCA(n_components = comps, kernel = 'rbf', gamma = gamma, n_jobs=-1)\n",
    "\n",
    "X_kpca = kpca.fit_transform(X_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se estudia la varianza explicada por las nuevas características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores propios asociados\n",
    "explained_variance = kpca.lambdas_\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(explained_variance[:10])\n",
    "\n",
    "plt.figure(1, figsize=(17,7))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.plot(kpca.alphas_[i])\n",
    "    plt.title(f'C. principal {i+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1, figsize=[9,15])\n",
    "\n",
    "bins = pd.cut(y_train,2)\n",
    "sns.scatterplot(X_kpca[:,0], X_kpca[:, 1], hue=bins,ax=ax[0])\n",
    "ax[0].set_title('Comp. 1 vs Comp. 2')\n",
    "\n",
    "sns.scatterplot(X_kpca[:,0], X_kpca[:, 2], hue=bins,ax=ax[1])\n",
    "ax[1].set_title('Comp. 1 vs Comp. 3')\n",
    "\n",
    "sns.scatterplot(X_kpca[:,1], X_kpca[:, 2], hue=bins,ax=ax[2])\n",
    "ax[2].set_title('Comp. 2 vs Comp. 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lo anterior se puede ver que las componentes puede discriminar de cierta forma entre 2 niveles de precio, por lo menos para la componente 1 vs 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA\n",
    "\n",
    "**LDA** (**L**inear **D**discriminant **A**nalysis) en un método de reducción de dimensión lineal que aprovecha etiquetas en una base de datos, por este motivo (y a diferencia de (K)PCA) es un método supervisado.\n",
    "\n",
    "El método se enfoca en maximizar la diferencia entre los puntos proyectados de clases distintas y minimizar la distancia entre puntos de la misma clase, a la vez.\n",
    "\n",
    "Sea $\\left \\{ x_i \\right \\}_{i=1}^N \\subset \\mathbb{R}^L $ conjuntos de puntos distribuidos en $K$ clases distintas, $\\left \\{ y_i \\right \\}_{i=1}^N \\subset \\{ 1,\\ldots,K \\}^L $ es la variable que indica a que clase perteneca cada punto.\n",
    "\n",
    "Se define\n",
    "\n",
    "\\begin{align}\n",
    "C_i =& \\{x_j\\}_{j: y_j=i}\\\\\n",
    "\\mu_i =& \\frac{1}{|C_i|} \\sum_{x\\in C_i} x \\\\\n",
    "\\mu =& \\frac{1}{N} \\sum_{i=1}^N x_i\n",
    "\\end{align}\n",
    "\n",
    "adicionalmente:\n",
    "\n",
    "\\begin{align}\n",
    "\\Sigma_b =& \\sum_{i=1}^K (\\mu_i - \\mu)(\\mu_i - \\mu)^T \\\\\n",
    "\\Sigma_w =& \\sum_{i=1}^K \\sum_{x \\in } (x - \\mu_i)(x - \\mu_i)^T\n",
    "\\end{align}\n",
    "\n",
    "Donde $\\Sigma_b$ representa una medida de la variabilidad entre clases distintas y $\\Sigma_w$ la dispersión entre la misma clases.\n",
    "\n",
    "La función que se optimiza en este caso es:\n",
    "\n",
    "\\begin{equation}\n",
    "J(W) = \\frac{|W \\Sigma_b W|}{|W \\Sigma_w W|}\n",
    "\\end{equation}\n",
    "\n",
    "La solución a este problema corresponde a uno de valores propios y su formulación es la siguiente:\n",
    "\n",
    "\\begin{equation}\n",
    "S_b^{1/2}S_w^{-1}S_b^{1/2} v = \\lambda v\n",
    "\\end{equation}\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se utiliza LDA sobre los datos anteriores por medio de la clase `LinearDiscriminantAnalysis` del módulo `discriminant_analysis` de Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "bins = pd.cut(y_train,10)\n",
    "labels = [*map(str,bins)]\n",
    "\n",
    "lda = LDA(n_components=9)\n",
    "X_lda = lda.fit_transform(X_filtered, labels)\n",
    "\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(lda.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(1, figsize=(15,6))\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.plot(lda.coef_[i])\n",
    "    plt.title(f'C. principal {i+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1, figsize=[9,15])\n",
    "\n",
    "sns.scatterplot(X_lda[:,0], X_lda[:, 1], hue=bins,ax=ax[0])\n",
    "ax[0].set_title('Comp. 1 vs Comp. 2')\n",
    "\n",
    "sns.scatterplot(X_lda[:,0], X_lda[:, 2], hue=bins,ax=ax[1])\n",
    "ax[1].set_title('Comp. 1 vs Comp. 3')\n",
    "\n",
    "sns.scatterplot(X_lda[:,1], X_lda[:, 2], hue=bins,ax=ax[2])\n",
    "ax[2].set_title('Comp. 2 vs Comp. 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que al incluirse información sobre la variable de respuesta, se pueden reconocer ciertos patrones de su distribución en menores dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lda_dim.pkl', 'bw') as handler:\n",
    "    pickle.dump(lda,handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE (t-distributed stochastic neighbor embedding)\n",
    "\n",
    "PCA se basa en hipótesis sobre la varianza global de los datos analizados para proyectar de manera lineal en una cantidad menor de dimensiones, KPCA flexibiliza este concepto permitiendo proyecciones no lineales. Por su parte, LDA toma proyecciones lineales que toman en cuenta ciertas estructuras de varianza local, para ello hace uso de etiquetas en problemas de clasificación. \n",
    "\n",
    "t-SNE es una técnica que permite reducir la dimensionalidad tomando en cuenta estructuras locales y globales de similitud. En términos generales, el algoritmo busca codificar la similitud entre puntos de alta dimensión por medio de un kernel gaussiano, una vez medidas las similitudes entre puntos, se procede a generar una estimación de la densidad de probabilidad conjunta de estos, finalmente se genera una densidad de probabilidad en bajas dimensiones que se aproxime a la estimada (en altas dimensiones) en función de la divergencia de Kullback-Leibler.\n",
    "\n",
    "La formulación teórica pasa por definir $X = \\left \\{ x_i \\right \\}_{i=1}^N \\subset \\mathbb{R}^M $ como una colección de puntos. Luego,el primer paso de t-SNE consiste en calcular la probabilidad $p_{ij}$, la cual es proporcional a la similitud entre los puntos $x_i$ y $x_j$, de la siguiente forma:\n",
    "\n",
    "\\begin{equation}\n",
    "p_{j|i} = \\frac{\\exp(-\\left \\| x_i - x_j \\right \\|^2/2\\sigma_i^2)}{\\sum_{k\\neq i} \\exp(-\\left \\| x_i - x_k \\right \\|^2/2\\sigma_i^2)}\n",
    "\\end{equation}\n",
    "\n",
    "Se define:\n",
    "\n",
    "\\begin{equation}\n",
    "p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2N}\n",
    "\\end{equation}\n",
    "\n",
    "adicionalmente si $i=j$ entoces $p_{ii} = 0$.\n",
    "\n",
    "El parámetro que falta por definir es $\\sigma_i$, la varianza de cada Gaussiana centrada en $x_i$, no es claro que exista un $\\sigma_i$ óptimo para cada punto en la base de datos, este valor va a variar dependiendo de que tan densamente pobladas sean las regiones en el espacio. t-SNE utiliza busqueda binaria para encontrar in $\\sigma_i$ que produzca una distribución $P_i$ con una **perplejidad** (perplexity) igual a la definida por el usuario, en este sentido el algoritmo recibe como entrada la **perplejidad** y luego determina el respectivo $\\sigma_i$.\n",
    "\n",
    "El valor de la **perplejidad** es:\n",
    "\n",
    "\\begin{equation}\n",
    "Perp(P_i) = 2^{E(P_i)}\n",
    "\\end{equation}\n",
    "\n",
    "Donde $E(P_i)$ corresponde a la entropía de Shannon:\n",
    "\n",
    "\\begin{equation}\n",
    "E(P_i) = -\\sum_j p_{j|i} \\log p_{j|i}\n",
    "\\end{equation}\n",
    "\n",
    "t-SNE apunta a buscar un representación $d$-dimensional $Y = \\left \\{ y_i \\right \\}_{i=1}^N \\subset \\mathbb{R}^d$ que refleje las similaridades $p_{ij}$ de la mejor forma posible. Para esto se usa $q_{ij}$ el cual mide similitud entre $y_i$ e $y_j$, la definición es:\n",
    "\n",
    "\\begin{equation}\n",
    "q_{ij} = \\frac{(1 + \\left \\| y_i - y_j \\right \\|^2)^{-1}}{\\sum_{k\\neq i} (1 + \\left \\| y_i - y_k \\right \\|^2)^{-1}}\n",
    "\\end{equation}\n",
    "\n",
    "Donde acá se utiliza una ditribución-t con 1 grado de libertad, al ser esta de \"colas pesadas\", los puntos muy distintos entre si quedarán alejados en la representación de baja dimensión. Adicionalmente se define $q_{ii}$ = 0.\n",
    "\n",
    "La ubicación de los puntos $y_i$ será determinada minimizando la divergencia de Kullback-Leibler de la distribución P respecto a Q, esto es:\n",
    "\n",
    "\\begin{equation}\n",
    "C = D_{KL}(P||Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n",
    "\\end{equation}\n",
    "\n",
    "Esta forma luego es minimizada respecto a las variables $y_i$ utilizando descenso de gradiente. Dada su formulación, se pueden obtener distintas representaciones sobre los mismos parámetros, en el siguiente articulo, se discuten ciertos aspectos de interás al trabajar con t-SNE [link](https://distill.pub/2016/misread-tsne/).\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se busca una representación de 3 dimensiones para el datset con características filtradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=3, perplexity=50)\n",
    "X_tsne = tsne.fit_transform(X_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=[9, 15])\n",
    "\n",
    "sns.scatterplot(X_tsne[:, 0], X_tsne[:, 1], hue=bins, ax=ax[0])\n",
    "ax[0].set_title('Comp. 1 vs Comp. 2')\n",
    "\n",
    "sns.scatterplot(X_tsne[:, 0], X_tsne[:, 2], hue=bins, ax=ax[1])\n",
    "ax[1].set_title('Comp. 1 vs Comp. 3')\n",
    "\n",
    "sns.scatterplot(X_tsne[:, 1], X_tsne[:, 2], hue=bins, ax=ax[2])\n",
    "ax[2].set_title('Comp. 2 vs Comp. 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obs:** Las técnicas de reducción de dimensionalidad vista pueden ser utilizadas en conjunto con técnicas de clustering para obtener nuevas características, esta nuevas variables pueden ser añadidas al dataset incial donde pueden ser estudiadas con las técnicas ya vistas. Se debe ser muy cauteloso en este procedimiento pues los clusters obtenidos luego de reducir la dimensionalidad pueden no ser interpretables desde la óptica original del problema (ej: t-sne + clustering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métodos Básicos de Estimación\n",
    "\n",
    "Ya hemos descrito los aspectos fundamentales en manejo de características, ya sea en la generación de nuevas variables como en la reducción de estas. A continuación se realizará una revisión general de algoritmos de aprendizaje y ciertos aspectos relacionados a los procesos de entrenamiento.\n",
    "\n",
    "### Métodos de ensamblaje\n",
    "\n",
    "Uno de los problemas de los arboles de desición es la inestabilidad de sus estructuras. Pequeñas modificaciones en el conjunto utilizado para el proceso de entrenamiento pueden generar cambios radicales en el proceso de bifurcaciones, esto hace que la interpretabilidad de estos métodos sea precaria para ciertos contextos.\n",
    "\n",
    "La principal razón de la instabilidad de los arboles es la naturaleza jerárquica del procedimiento. Modificaciones en la bifurcación de nodos cercanos a la raíz genera diferencias que se propagan de forma acumulativa, de esta forma la topología del arbol puede cambiar radicalmente a lo largo del proceso al variar el conjunto de datos.\n",
    "\n",
    "Una forma de lograr modelos más robustos es por medio de **ensamblaje**, esta práctica consiste en combinar estimadores base, de manera tal, que su efecto combinado mejore la capacidad de generalización. Por lo general, los métodos de ensamblaje conllevan esquemas de votación entre modelos de alta varianza para prevenir predicciones fuera de rango y sobreajuste, por otra parte se pueden utilizar técnicas de *levantamiento* (*boosting*) para mejorar el rendiento de un conjunto de modelos *débiles* o menos expresivos (*weak learners*).\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "El modelo más básico de ensamblaje es **voto de la mayoria**, este funciona en un contexto de clasificación. Acá se tienen $n$ modelos $\\lbrace h_1, \\ldots, h_n \\rbrace$, donde para un valor de entrada $x$, la clasificación se hace según la moda sobre los valores $h_i(x)$ para todo $i = 1, \\ldots, n$. Consideremos un ejemplo de clasificación binaria, para que este tipo de modelos funcione, se requiere que el error de cada modelo $h_i$ sea menor que el 50% (mejor que adivinar al azar). \n",
    "\n",
    "Se puede calcular la probabilidad $P(m)$ de hacer una predicción errónea dado que $m$ clasificadores (con $m > \\lceil {n/2} \\rceil $) predicen la misma etiqueta. Para esto, se define el conjunto $\\mathcal{C}_m$, consistente de todos los $n \\choose m$ subconjuntos de $\\lbrace 1, \\ldots, n \\rbrace$ con tamaño $m$. Así, si la probabilidad de que el clasificador $h_i$ falle en predicción es de $\\varepsilon_i$, se puede modelar la distribución de aciertos y fallos como una variable aleatoria $Ber(\\varepsilon_i)$. Utilizando la ley de probabilidades totales se puede llegar entonces al siguiente resultado:\n",
    "\n",
    "$$\n",
    "P(m) = \\sum_{c \\in C_m} P(m |\\text{solo fallan } h_i  \\forall i \\in c) P(\\text{solo fallan } h_i \\forall i \\in c)\n",
    "$$\n",
    "\n",
    "Dado que se tiene voto por mayoria y $|c| = m > \\lceil {n/2} \\rceil$  la expresión anterior se reduce a\n",
    "\n",
    "$$\n",
    "P(m) = \\sum_{c \\in C_m} P(\\text{solo fallan } h_i \\forall i \\in c)\n",
    "$$\n",
    "\n",
    "Que corresponde a\n",
    "\n",
    "$$\n",
    "P(m) = \\sum_{c \\in C_m} \\prod_{j \\in c} \\varepsilon_j \\prod_{k \\in \\lbrace 1, \\ldots, n \\rbrace / c} (1-\\varepsilon_k)\n",
    "$$\n",
    "\n",
    "Finalmente la probabilidad de que el ensamblaje falle es de \n",
    "\n",
    "$$\n",
    "\\varepsilon_{ens} = \\sum_{m > \\lceil n/2 \\rceil} P(m) = \\sum_{m > \\lceil n/2 \\rceil} \\sum_{c \\in C_m} \\prod_{j \\in c} \\varepsilon_j \\prod_{k \\in \\lbrace 1, \\ldots, n \\rbrace / c} (1-\\varepsilon_k)\n",
    "$$\n",
    "\n",
    "Consideremos por ejemplo que se tienen $n=15$ modelos y $\\varepsilon = 0.3$ para cada uno de los $n$ modelos. Según lo anterior, se tiene que el error de ensamblaje es:\n",
    "\n",
    "$$\n",
    "\\varepsilon_{ens} = \\sum_{m = 8}^{15} {11 \\choose m} 0.3^m(1-0.3)^{(15-m)} = 0.05\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import binom\n",
    "\n",
    "n = 15\n",
    "eps = .3\n",
    "\n",
    "eps_ens = np.sum([binom(n,m)*(eps**(m))*((1- eps)**(n-m)) for m in range(int(np.ceil(n/2)),n+1)])\n",
    "eps_ens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta técnica sugiere generar un gran número de mododelos de clasificación y unificarlos por la moda de sus respuesta. Cuando se trabaja con clasificadores probabílisiticos se puede generar un modelo de ensamblaje por **voto de mayoria suave**. En este tipo de modelo, se genera una combinanación convexa sobre las probabilidades de predicción de etiqueta entregadas por los modelos a ensamblar, posteriormente, se clasifica según la probabilidad más alta. En términos matemáticos, si $p_{i,j}$ es la probabilidad de predicción para la etiqueta $j$, entregada por el modelo $i$, para los pesos no negativos $w_i$, tales que $\\sum_i^n w_i = 1$, la predicción se esamblaje viene dada por:\n",
    "$$\n",
    "\\hat{y}=\\arg \\max _{j} \\sum_{i=1}^{n} w_{i} p_{i, j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Utilice `VotingClassifier` del módulo `sklearn.ensemble` para implementar voto por mayoría en un problema de clasificación simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging\n",
    "\n",
    "Cómo fue mencionado recientemente, un problema estructural de los métodos basados en arboles es su alta varianza respecto a las predicciones. Una idea razonable puede ser utilizar una gran cantidad modelos basados en árboles en conjunto con algún método de voto por mayoría. Dicha idea se puede elaborar utilizando el técnicas de sub-muestreo de datos (*boostraping*) y una ligera modificación del voto por mayoría conocida como **bagging**. \n",
    "\n",
    "En términos simples, el sub muestreo o *bootstraping* consiste en una muestra de tamaño $n$ obtenida **con reemplazo** a partir de un conjunto de datos original $\\mathcal{D}$ con $|\\mathcal{D}| = n$. Esto produce que algunos ejemplos de entrenamiento se dupliquen en cada sub-muestra, por consiguiente, existen también observaciones que no aparecen en en todas las sub-muestras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Demuestre que de manera asintótica sobre el tamaño de la muestra, si se eligen $n$ observaciones de manera uniforme y con reemplazo (boostrap) entonces la submuestra generada tendrá alproximadamente el 63.2% de los valores únicos. Consecuentemente, habrá un 37.2% de observaciones que no aparecerán en la sub-muestra. *Hint*: Relacione la probablidad de que una muestra no sea seleccionada con el número $e$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un esquema de bagging consiste en generar $m$ sub-muestras por boostrap, luego para cada muestra se entrena un modelo de predicción $h_i$, finalmente el modelo final de predicción corresponde a un ensamblaje de voto por mayoria (promedio en regresión) de los modelos generados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting:\n",
    "\n",
    "El concepto de **boosting** es una de las hearramientas más poderosas introducidas en los últimos 20 años. La motivación de este método de ensamblaje consiste en combinar el resultado de varios predictores \"débiles\" (ej: un clasificador que se relaciona débilmente con las etiquetas reales) para construir un clasificador con mucha mayor eficacia y presición. En general existen 2 tipos de boosting: boosting adaptativo y boosting por gradiente.\n",
    "\n",
    "El proceso de boosting es iterativo y consiste en asignar pesos a cada elemento del conjunto de entrenamiento. Estos pesos se asignan de manera secuencial y su calculo se basa en los errores hechos por los modelos débiles o base. En términos generales, el algoritmo de boosting se reduce a:\n",
    "\n",
    "1. Inicializar un vector de pesos uniforme, a cada observación del conjunto de entrenamiento se le asigna un peso.\n",
    "\n",
    "2. Iterar:\n",
    "    1. Entrenar un modelo débil $h_m$ minimizando un funcional de error $J_m$. Acá cada muestra pondera un error de predicción proporcional a su peso asignado.\n",
    "    \n",
    "    2. Aumentar el peso de las observaciones mal clasificadas.\n",
    "\n",
    "3. Se genera un esquema de voto por mayoría en los predictores entrenados. \n",
    "\n",
    "**AdaBoost (Boosting Adaptativo)**\n",
    "\n",
    "Para el caso de clasificación binaria, el algoritmo AdaBoost implementa el esquema anterior de la siguiente forma:\n",
    "\n",
    "* Inicializa k: número de iteraciones.\n",
    "* Inicializa $\\mathcal{D}$: conjunto de entrenamiento $\\mathcal{D}=\\left\\{ \\left( \\mathbf{x}^{[1]}, y^{[1]} \\right), \\ldots, \\left( \\mathbf{x}^{[n]}, y^{[n]}\\right)  \\right\\}$.\n",
    "* Inicializa $w_{1}(i)=1 / n, \\quad i=1, \\ldots, n, \\quad \\mathbf{w}_{1} \\in \\mathbb{R}^{n}$\n",
    "\n",
    "1. Itera desde $r=1$ hasta $k$:\n",
    "    1. Para todo $i$: $\\mathbf{w}_r(i) := w_r(i)/{\\sum_j w_r(j)}$\n",
    "    2. Entrena $h_r$ modelo en $\\mathcal{D,\\mathbf{w_r}}$\n",
    "    3. $\\varepsilon_r = \\sum_i w_r(i) \\mathbf{1}\\left(h_{r}(i) \\neq y_{i}\\right)$, si $\\varepsilon_r > 1/2$ Terminar proceso.\n",
    "    4. $\\alpha_r := 1/2 \\log[(1-\\varepsilon_r)/\\varepsilon_r]$\n",
    "    5. \n",
    "    $$\n",
    "    w_{r-1}(i):=w_{r}(i) \\times\\left\\{\\begin{array}{ll}\\mathrm{e}^{-\\alpha_{r}} & \\text { if } h_{r}\\left(\\mathrm{x}^{[l]}\\right)=y^{[i]} \\\\ \\mathrm{e}^{\\alpha_{r}} & \\text { if } h_{r}\\left(\\mathrm{x}^{[i]}\\right) \\neq y^{[i]}\\end{array}\\right.\n",
    "    $$\n",
    "    \n",
    "Se predice según $h_{k}(\\mathbf{x})=\\arg \\max _{j} \\sum_{r} \\alpha_{r} \\mathbf{1}\\left[h_{r}(\\mathbf{x})=j\\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "Generalice el algoritmo anterior para trabajar en clasificación con $c$ clases. Para ello tendrá que modificar $\\varepsilon_r$ y $\\alpha_r$. ¿Como se puede generalizar este esquema a regresión?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo**\n",
    "\n",
    "Se aplica AdaBoost sobre la reducción de dimensionalidad propuesta por LDA sobre el conjunto de datos filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lda_dim.pkl', 'br') as handler:\n",
    "    lda_dim = pickle.load(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "ab = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(),\n",
    "                       n_estimators=600)\n",
    "\n",
    "X_train_tree, X_test_tree, Y_train_tree, Y_test_tree = train_test_split(\n",
    "    lda_dim.transform(X_filtered), y_train, test_size=.2)\n",
    "\n",
    "grid = {\n",
    "    'base_estimator__max_depth': range(7, 9),\n",
    "    'base_estimator__ccp_alpha': np.power(2, np.linspace(14, 16, num=2)),\n",
    "    'base_estimator__min_samples_leaf': range(3, 5),\n",
    "    'base_estimator__min_impurity_decrease': np.power(2, np.linspace(12.5, 13, num=2))\n",
    "    }\n",
    "\n",
    "cv = GridSearchCV(ab, param_grid=grid, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "cv.fit(X_train_tree, Y_train_tree)\n",
    "\n",
    "ab_reg_best = cv.best_estimator_\n",
    "ab_reg_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res_gs = {\n",
    "    'R2': ab_reg_best.score(X_train_tree, Y_train_tree),\n",
    "    'rmse': rmse(ab_reg_best.predict(X_train_tree), Y_train_tree)\n",
    "}\n",
    "\n",
    "print('Resultados train: \\n', train_res_gs, '\\n')\n",
    "\n",
    "test_res_gs = {\n",
    "    'R2': ab_reg_best.score(X_test_tree, Y_test_tree),\n",
    "    'rmse': rmse(ab_reg_best.predict(X_test_tree), Y_test_tree)\n",
    "}\n",
    "\n",
    "print('Resultados test: \\n', test_res_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lda_ada_boost.pkl','bw') as handler:\n",
    "    pickle.dump(ab_reg_best,handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosting**\n",
    "\n",
    "En contraste con AdaBoost, acá no se ajustan los pesos de las observaciones mal clasificadas. En su lugar, se optimiza una función de perdida suave (L2 por ejemplo) para un predictor débil. El output de este método es un modelo aditivo en base a un conjunto de predictores base, es decir, no se aplica votación. Así, para la función de pérdidia diferenciable $L(y,\\hat{y})$, el algoritmo consiste en:\n",
    "\n",
    "1. Inicializar un modelo constante $h_{0}(x)=\\arg \\min _{\\gamma} \\sum_{i=1}^{n} L\\left(y_{i}, \\gamma\\right)$\n",
    "\n",
    "2. Itera desde $m=1$ hasta $M$:\n",
    "    1. Obtener los residuos\n",
    "    $$\n",
    "    r_{i m}=-\\left[\\frac{\\partial L\\left(y_{i}, F\\left(x_{i}\\right)\\right)}{\\partial F\\left(x_{i}\\right)}\\right]_{F(x)=F_{m-1}(x)} \\quad$ for $i=1, \\ldots, n\n",
    "    $$\n",
    "    2. Entrenar un modelo base $h_m(x)$ sobre el conjunto $\\left\\{\\left(x_{i}, r_{i m}\\right)\\right\\}_{i=1}^{n}$ \n",
    "    3. Calcular los ponderadores de boosting $\\gamma_m$ \n",
    "    $$\n",
    "    \\gamma_{m}=\\underset{\\gamma}{\\arg \\min } \\sum_{i=1}^{n} L\\left(y_{i}, F_{m-1}\\left(x_{i}\\right)+\\gamma h_{m}\\left(x_{i}\\right)\\right)\n",
    "    $$\n",
    "    4. Actualizar el modelo $F_{m}(x)=F_{m-1}(x)+\\gamma_{m} h_{m}(x)$\n",
    "\n",
    "3. Output $F_{M}(x)$\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se utiliza un regresor basado en gradient boosting sobre los datos transformados por LDA. Este método esta basado en arboles como predictores débiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "\n",
    "gb = GradientBoostingRegressor(n_estimators=600)\n",
    "\n",
    "X_train_tree, X_test_tree, Y_train_tree, Y_test_tree = train_test_split(\n",
    "    lda_dim.transform(X_filtered), y_train, test_size=.2)\n",
    "\n",
    "grid = {\n",
    "    'max_depth': range(7, 9),\n",
    "    'ccp_alpha': np.power(2, np.linspace(14, 16, num=2)),\n",
    "    'min_samples_leaf': range(3, 5),\n",
    "    'min_impurity_decrease': np.power(2, np.linspace(12.5, 13, num=2))\n",
    "    }\n",
    "\n",
    "cv = GridSearchCV(gb, param_grid=grid, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "cv.fit(X_train_tree, Y_train_tree)\n",
    "\n",
    "gb_reg_best = cv.best_estimator_\n",
    "gb_reg_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res_gs = {\n",
    "    'R2': gb_reg_best.score(X_train_tree, Y_train_tree),\n",
    "    'rmse': rmse(gb_reg_best.predict(X_train_tree), Y_train_tree)\n",
    "}\n",
    "\n",
    "print('Resultados train: \\n', train_res_gs, '\\n')\n",
    "\n",
    "test_res_gs = {\n",
    "    'R2': gb_reg_best.score(X_test_tree, Y_test_tree),\n",
    "    'rmse': rmse(gb_reg_best.predict(X_test_tree), Y_test_tree)\n",
    "}\n",
    "\n",
    "print('Resultados test: \\n', test_res_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking \n",
    "\n",
    "El algoritmo de stacking consiste en ensamblar modelos basados en sus predicciones. Sobre tales predicciones se construye un *meta* modelos que las combina minimizando el error asociado. El algorimto de este método consiste en:\n",
    "\n",
    "1. Input: Datos de entrenamiento $\\mathcal{D} = \\lbrace x_i, y_i \\rbrace_{i=1}^{m}$\n",
    "2. Output: Clasificador ensamblado $H$. \n",
    "\n",
    "Etapa inicial:\n",
    "1. Entrena $T$ modelos $h_i$ base sobre $D$\n",
    "\n",
    "Etapa secundaria:\n",
    "1. Se construye un nuevo conjunto de datos:\n",
    "$$\n",
    "D_{h}=\\left\\{x_{i}^{\\prime}, y_{i}\\right\\},\\text{ donde }x_{i}^{\\prime}=\\left\\{h_{1}\\left(x_{i}\\right), \\ldots, h_{T}\\left(x_{i}\\right)\\right\\}\n",
    "$$\n",
    "\n",
    "Etapa final:\n",
    "\n",
    "Entrenar $H$ sobre $D_{h}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Utilice la clase `StackingRegressor` de módulo `sklearn.ensemble` para entrenar un modelo de ensamblaje en regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos Clasicos\n",
    "\n",
    "## Selección de Caracterísitcas implicita"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
