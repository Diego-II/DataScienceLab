{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MA6202: Laboratorio de Ciencia de Datos\n",
    "\n",
    "**Profesor: Nicolás Caro**\n",
    "\n",
    "**10/06/2020 - C11 S7**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingeniería de Características y Algoritmos de Aprendizaje Automático\n",
    "\n",
    "La práctica de la ciencia de datos se fundamenta en la obtención, exploración, limpieza, tratamiento y transformación de la información contenida en distintas fuentes de datos. Tales procedimientos constituyen gran parte del desarrollo de proyectos basados datos y su finalidad es la facilitar la obtención de conocimiento. Este último paso se lleva a cabo por medio de sistemas de aprendizaje automático. En general esto se puede resumir en las siguientes etapas:\n",
    "\n",
    "1. **Obtención de la información**: Corresponde a la recolección de datos referentes al fenómeno de interés. Tal información puede ser tanto estructurada como no estructurada.\n",
    "\n",
    "2. **Preprocesamiento y exploración**: Una vez reunida una porción de información significativa, se procede a explorarla y procesarla. En este apartado se aplican métodos de reducción de dimensionalidad, escalamiento, muestreo (conjuntos de entrenamiento y test), selección de características y transformación. \n",
    "\n",
    "3. **Aprendizaje**: Cuando los datos han sido estudiados y transformados de buena manera, se procede a la aplicación de modelos de aprendizaje de máquinas. Acá se estudian métricas de rendimiento y se optimizan hiperparámetros. \n",
    "\n",
    "4. **Evaluación**: Seleccionado un modelo de optimo, se generan esquemas de evaluación sobre conjuntos de datos no observados pero representativos del fenómeno estudiado, esto permite adquirir nociones sobre el comportamiento del sistema modelado fuera del conjunto de entrenamiento. \n",
    "\n",
    "5. **Producción**: Finalmente se genera un entorno en cual el modelo obtenido es utilizado para la obtención de información, elaboración de predicciones, clasificaciones o perfilamientos en función de la materia estudiada. \n",
    "\n",
    "Estos procesos son interdependientes y no presentan un orden lineal, más bien, se entrelazan circulando de manera natural entre los pasos 2 y 4. También se relacionan los pasos 1 y 6 a medida que llegan nuevos datos al sistema. \n",
    "\n",
    "Hasta el momento nos hemos centrado en las etapas de obtención, preprocesamiento y exploración de datos. El paso siguiente corresponde a un *refinamiento* de la información, con el fin de entrenar modelos de aprendizaje de alto rendimiento sobre los datos que se poseen. Este refinamiento ve su primera etapa en los procesos exploratorios (perfilamiento de datos y preprocesamiento inicial), para luego generar transformaciones y selecciones sobre las variables disponible. Dichas transformaciones y selecciones logran un mejor rendimiento en la medida que se obtienen por medio de *conocimiento del área* (*domain kwnlegde*), es decir, utilizando el conocimiento existente, con respecto al problema que se desea modelar. Sin embargo, existen ciertas técnicas *estándar* y algunas *automáticas* que permiten depurar los datos para obtener atributos y transformaciones de interés. \n",
    "\n",
    "En general, el proceso de refinamiento de datos se conoce como **ingeniería de características**, donde las características o *features* hacen referencia a los atributos y sus transformaciones. El proceso de ingeniería de características pasa generalmente por:\n",
    "\n",
    "Explorar los datos y ajustar las variables disponibles, de manera tal, que se gane compatibilidad con las hipótesis de ciertos modelos de aprendizaje automático.\n",
    "\n",
    "Posteriormente, se derivan características de interés, ya sea por medio de composición de atributos o transformaciones sobre estos. En esta parte, es una buena práctica, buscar transformaciones que hagan sentido con el problema modelado, esto no siempre se cumple (ej: una transformación polinomial sobre los datos no necesariamente es interpretable en ciertos problemas, pero puede aumentar considerablemente el rendimiento). \n",
    "\n",
    "Se continua, seleccionando caraterísticas, esto se puede hacer por medio de análisis estadísticos y algoritmos puntuación. Es necesario entender, que aunque en primera instancia más variables deberían llevarnos a mejores resultados, en la realidad, pueden aparecer caraterísticas que añadan ruido al sistema, ya sea por no estar relacionadas con el fenómeno estudiado o por presentarse problemas en su recolección. En tal contexto, cobra importancia la exlusión de dichas variables o de manera reciproca, la selección de aquellas que aseguren el mejor desempeño. \n",
    "\n",
    "Finalmente, se procede a entrenar y evaluar modelos sobre las métricas relevantes con el problema a trabajar, esta etapa se comunica constantemente con la de generación y selección de caraterísticas, pues las transformaciones que se decidan utilizar dependen en cierta medida de los modelos que se implementarán. \n",
    "\n",
    "A continuación se estudian ciertas técnicas de ingeniería de características estándar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones de Atributos\n",
    "\n",
    "Ya hemos estudiado técnicas de transformación por medio de preprocesamiento. A continuación se añaden y estudian nuevas metodologias además de aplicar algunas técnicas previas en la base de datos `HousePricing`. \n",
    "\n",
    "### Preprocesamiento inicial\n",
    "\n",
    "Se cargan los datos y se agrupan sus columnas según tipo de dato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../S5/data/train.csv', index_col = 'Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las técnicas más básicas de ingeniería de características es la **modificación de tipo de dato**, en este caso se transforman los datos reconocido como tipo `object` a tipo `str`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_type_set = [col for col in df.columns if df[col].dtype == 'O']\n",
    "df = df.astype({col:'str' for col in object_type_set})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para trabajar de manera más sencilla con los datos se procede a agrupar los atributos por tipo de dato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    'MSZoning', 'Street', 'Alley', 'LandContour', 'LotConfig', 'Neighborhood',\n",
    "    'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle',\n",
    "    'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation',\n",
    "    'Heating', 'CentralAir', 'Electrical', 'GarageType', 'PavedDrive',\n",
    "    'MiscFeature', 'SaleType', 'SaleCondition'\n",
    "]\n",
    "\n",
    "ordinal_cols = [\n",
    "    'LotShape', 'Utilities', 'LandSlope', 'ExterQual', 'ExterCond', 'BsmtQual',\n",
    "    'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC',\n",
    "    'KitchenQual', 'Functional', 'FireplaceQu', 'GarageFinish', 'GarageQual',\n",
    "    'GarageCond', 'PoolQC', 'Fence'\n",
    "]\n",
    "\n",
    "\n",
    "num_cols = [\n",
    "    'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n",
    "    'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n",
    "    'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n",
    "    'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n",
    "    'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n",
    "    'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n",
    "    'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n",
    "    'MoSold', 'YrSold','SalePrice'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se genera un *mapping* para obtener un datset multindexado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = [('numeric', col) for col in num_cols]\n",
    "mapping.extend([('categorical', col) for col in cat_cols])\n",
    "mapping.extend([('ordinal', col) for col in ordinal_cols])\n",
    "\n",
    "df = df.reindex(columns=num_cols + cat_cols + ordinal_cols)\n",
    "df.columns = pd.MultiIndex.from_tuples(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos visualizaciones según tipo de variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_plotter(df, dtype, nrows=6):\n",
    "    '''Permite graficar subconjuntos de variables en un dataframe multindexado\n",
    "    \n",
    "    Toma como argumento un dataframe de pandas df multi-indexado y un nivel \n",
    "    de multi - indice asociado a un subconjunto de columnas. Obtiene graficos\n",
    "    univariados asociados a dicho subconjunto de varaibles.\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    \n",
    "    df: DataFrame\n",
    "        El conjunto de datos a explorar.\n",
    "    \n",
    "    dtype: String\n",
    "        El nombre del nivel a visualizar\n",
    "    \n",
    "    nrwos: int\n",
    "        La cantidad de filas a generar en la matriz de graficos\n",
    "    \n",
    "    Returns: \n",
    "    -------        \n",
    "        None\n",
    "        Se genera una visualizacion de matplotlib.\n",
    "    '''\n",
    "\n",
    "    cols = df[dtype].shape[1] // nrows\n",
    "    resto = df[dtype].shape[1] % nrows\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=nrows + 1, ncols=cols, figsize=[17, 17])\n",
    "\n",
    "    # Se remueve el resto de plots\n",
    "    list(map(lambda a: a.remove(), ax[-1, -(cols - resto):]))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    # Se define un titulo y su ubicacion\n",
    "    fig.suptitle('Distribuciones Univariadas typo: ' + dtype,\n",
    "                 fontsize=20,\n",
    "                 x=0.5,\n",
    "                 y=1.05)\n",
    "    '''\n",
    "    Se recorre cada axis, para cada columna del dataframe, se genera un grafico \n",
    "    distinto en funcion del tipo de dato.\n",
    "\n",
    "    '''\n",
    "    df_cols = df[dtype].columns\n",
    "    for axis, col in zip(ax.flatten(), df_cols):\n",
    "        try:\n",
    "            # Graficos para datos numericos\n",
    "            sns.distplot(df[(dtype, col)], ax=axis, rug=True)\n",
    "\n",
    "        except RuntimeError:\n",
    "            sns.distplot(df[(dtype, col)], ax=axis, rug=True, kde=False)\n",
    "\n",
    "        except ValueError:\n",
    "            sns.countplot(df[(dtype, col)], ax=axis)\n",
    "\n",
    "        axis.set_xlabel(col, fontsize=15)\n",
    "\n",
    "    # Se ajusta el espaciado interno entre subplots\n",
    "    w, h = (.4, .4)\n",
    "    plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos visualizaciones para las variables numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_plotter(df,'numeric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El manejo de valores faltantes corresponde a una técnica de preprocesamiento ya estudiada y previa a las transformaciones de atributos buscadas en ingeniería de características. Por tal motivo, se hace un tratamiento rápido de los valores faltantes, sin ahondar en detalles. En este apartado, se reemplazan las variables con texto 'nan' por su objeto equivalente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('nan',np.nan, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa la distribución porcentual de los valores perdidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_per(df):\n",
    "    na_percent = (df.isnull().sum()/len(df))[(df.isnull().sum()/len(df))>0].sort_values(ascending=False)\n",
    "\n",
    "    return pd.DataFrame({'Missing Percentage':na_percent*100})\n",
    "\n",
    "find_missing_per(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se genera una función auxiliar para tratar con multi indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexer(cols, t_c = df.columns):\n",
    "    '''Genera columnas multinivel a partir de nombres de columna planos.'''\n",
    "    \n",
    "    set_to_tuple = set(*[cols])\n",
    "\n",
    "    tuples = [\n",
    "        i for i in t_c if set_to_tuple.intersection(set(i))\n",
    "    ]\n",
    "    \n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se llenan los valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se transforma nan -> None\n",
    "to_none = [\n",
    "    'FireplaceQu', 'Fence', 'Alley', 'MiscFeature', 'PoolQC', 'MSSubClass',\n",
    "    'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual',\n",
    "    'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'\n",
    "]\n",
    "\n",
    "# Fill mapper\n",
    "to_fill = {key: 'None' for key in indexer(to_none)}\n",
    "\n",
    "# Se transforma nan -> 0\n",
    "to_0 = [\n",
    "    'GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2',\n",
    "    'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'\n",
    "]\n",
    "to_fill.update({key: 0 for key in indexer(to_0)})\n",
    "\n",
    "# Se llenan los valores faltantes\n",
    "df.fillna(to_fill, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se estudia el dataset luego de llenar los valores anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_missing_per(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se tiene que las variables `LotFrontage` y `Electrical` presentan valores faltantes. Para la variable `Electrical` esto representa una cantidad muy baja de observaciones, por lo que simplemente se pueden eliminar, mientras que para la variable `LotFrontage` se escoge llenar los valores faltantes por medio de una subagrupación en función de la variable 'Neighborhood', con esto, se asigna el valor de la mediana de 'LotFrontage' en para cada subgrupo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_median = lambda x: x.fillna(x.median())\n",
    "\n",
    "df.dropna(axis=0, subset=indexer(['Electrical']), inplace= True)\n",
    "\n",
    "df[indexer(['LotFrontage'])] = df.groupby(indexer(['Neighborhood']))[indexer(['LotFrontage'])].transform(\n",
    "    group_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finalmente no se tienen valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(find_missing_per(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es trabajar con los valores anómalos. Nuevamente, este paso es previo a la transformación de atributos por lo que no se estudia en profundidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varlist = [\n",
    "    'GrLivArea', 'TotalBsmtSF', '1stFlrSF', 'MasVnrArea', 'GarageArea',\n",
    "    'TotRmsAbvGrd'\n",
    "]\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "def scatter_vs_price(df=df,\n",
    "                     varlist=varlist,\n",
    "                     nrows=3,\n",
    "                     db_min_samples=5,\n",
    "                     db_eps=0.99):\n",
    "    '''Grafica una lista de variables contra SalePrice.\n",
    "    \n",
    "    Acepta como argumento una lista de variables, cada una de estas es \n",
    "    escalada de manera robusta, posteriormente se entrena una aglomeracion\n",
    "    usando dbscan para resaltar los posible outliers.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    df: Pandas DataFrame\n",
    "        El conjunto de datos de HousePricing\n",
    "    \n",
    "    varlist : List\n",
    "              Lista de variables a estudiar\n",
    "    \n",
    "    nrows: int\n",
    "           Cantidad de filas a graficar (argumento de subplots)\n",
    "    \n",
    "    db_min, db_eps: int, float\n",
    "                    Hiperparametros de DBSCAN\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "        Visualizacion: None\n",
    "                       Visualizacion de las variables estudiadas contra el precio\n",
    "        \n",
    "        Outliers: Dict\n",
    "                  Diccionario de posibles outliers\n",
    "    '''\n",
    "    outliers = dict()\n",
    "    \n",
    "    ncols = len(varlist) // nrows\n",
    "    resto = len(varlist) % nrows\n",
    "\n",
    "    fig, ax = plt.subplots(nrows + 1, ncols, figsize=[13, 10])\n",
    "    # Se borran las axis innecesarias\n",
    "    for a in ax[-1, -(ncols - resto):]:\n",
    "        a.remove()\n",
    "    ax = ax.ravel()\n",
    "\n",
    "    for a, var in zip(ax, varlist):\n",
    "        \n",
    "        X = df[indexer([var, 'SalePrice'])]\n",
    "        scaler = RobustScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "        outlier_detection = DBSCAN(min_samples=db_min_samples, eps=db_eps)\n",
    "        C = outlier_detection.fit_predict(X)\n",
    "\n",
    "        a.scatter(x=X[:, 0], y=X[:, 1], c=C, alpha=0.5)\n",
    "\n",
    "        #a.axvline(x=4600, color='r', linestyle='-')\n",
    "        title = 'Scaled' + var + '- Price scatter plot'\n",
    "        a.set_title(title, fontsize=15, weight='bold')\n",
    "        outliers.update({var:C})\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se estudian las variables seleccionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = scatter_vs_price()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por simplicidad, se eliminan los valores anómalos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se recolectan los indices de valores anomalos\n",
    "outlier_idx = pd.Index({})\n",
    "for key, val in outliers.items():\n",
    "\n",
    "    outlier_locs = [*map(bool, val)]\n",
    "    columns = indexer([key])\n",
    "\n",
    "    outlier_idx = outlier_idx.union(df.loc[outlier_locs, columns].index)\n",
    "    \n",
    "# Se eliminan las filas con valores anomalos\n",
    "df.drop(index=outlier_idx, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien, luego de este preprocesamiento se puede trabajar en transformaciones sobre los datos, se debe recordar, que las etapas de preprocesamiento y manejo de caraterísticas se comunican constantemente, generando ciclos en el flujo de trabajo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manejo de Carácteristicas Básico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se procede a producir características relevantes, una primera aproximación a este procedimiento es comenzar por introducir transformaciones sobre las variables y modelar la interacción entre estas. En este apartado, es necesario discutir el término de **Data Leakage**. \n",
    "\n",
    "**Data Leakage** hace referencia a la *fuga* de información entre conjuntos destinados para entrenamiento y test. Si se produce esta fuga, significa que se esta utilizando información no valida en procesos de entrenamiento, lo cual genera modelos sobre optimistas en sus métricas de evaluación. En la práctica, un modelo con fuga de información puede ser incluso inútil en producción. Para evitar este fenómeno, basta realizar las transformaciones de datos pertinentes dentro de los procesos de validación y entrenamiento. Otra buena práctica en este aspecto, es generar un conjunto de validación adicional para comprobar métricas de rendimiento luego de realizar selección de modelo.\n",
    "\n",
    "A modo de ejemplo: si en un problema de modelamiento se normalizan ciertas variables utilizando la información total del dataset y luego se procede a generar particiones de entrenamiento y validación, entonces los elementos ubicados en los conjuntos de validación habrán aportado información al proceso de normalización pues influyeron en los calculos de media y desviación estándar. Esto puede mejorar de cierta manera el rendimiento en predicción, dando como resultado un modelo sesgado.\n",
    "\n",
    "Lo primero que se hará es generar un conjunto de validación utilizando el 10% de los datos, este conjunto se utiliza al final del proceso de selección de modelos para verificar la capacidad de generalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_val = train_test_split(df, test_size = .1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se procede a estudiar ciertos atributos del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_color_codes(palette='deep')\n",
    "\n",
    "\n",
    "sns.distplot(df_train['numeric']['SalePrice'], color=\"b\");\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Frecuancia\")\n",
    "ax.set(xlabel=\"SalePrice\")\n",
    "ax.set(title=\"Distribucion de SalePrice\")\n",
    "\n",
    "sns.despine(trim=True, left=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tiene similitud a una distribución normal, para ello se hace un test de normalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def norm_test(data = df_train['numeric']['SalePrice']):\n",
    "\n",
    "    k, p = stats.normaltest(data)\n",
    "\n",
    "    alpha = 1e-3\n",
    "    print(\"p =\",p)\n",
    "\n",
    "    if p < alpha: \n",
    "        print(\"Se puede rechazar la hipotesis nula\")\n",
    "    else:\n",
    "        print(\"No se puede rechazar la hipotesis nula\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego la variable `SalePrice` no se comporta como una normal. En estos casos. Una técnica de manejo de características básica es la transformación de Box-Cox (ya vista), Podemos transformar esta variable en función de tal método"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Se utiliza una funcion auxiliar para transformar la serie\n",
    "to_array = lambda pd_series: pd_series.values.reshape(-1, 1)\n",
    "y = to_array(df_train['numeric']['SalePrice'])\n",
    "\n",
    "transformer_bc = PowerTransformer(method='box-cox')\n",
    "\n",
    "y = transformer_bc.fit_transform(y)\n",
    "\n",
    "# Se aplica el test de normalidad a la variable de interes\n",
    "norm_test(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego la transformación de Box - Cox permite normalizar de manera sencilla la distribución de la variable `SalePrice`. En general, todas las técnicas de preprocesamiento vistas anteriormente (escalar, normalizar,códificación dummy, etc...) entran en el manejo de básico de características. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Visualice las variables numéricas, calcule su asimetría estadística (skewness) por medio del método `.skew()`. Defina un valor umbral (ej: 0.6). Luego aplique una transformación de potencia sobre las variables más asimétricas. \n",
    "\n",
    "**Obs**: Lo anterior permite normalizar de manera más robusta aquellas variables que por naturaleza se alejan de ser normales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra manera de generar características de interés es por medio de interacciones. En este caso se hace uso del conocimiento del área que se trabaja. \n",
    "\n",
    "Por ejemplo, las variables `TotalBsmtSF`, `1stFlrSF` y `2ndFlrSF` representan unidades de área en pies cuadrados para el subterraneo, primer y segundo piso. Con esta información se puede crear la variable `TotalSF` que representa la superficie total de una vivienda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[('numeric','TotalSF')] = df_train['numeric']['TotalBsmtSF'] + df_train['numeric']['1stFlrSF'] + df_train[\n",
    "    'numeric']['2ndFlrSF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede también general la variable `YearsSinceRem` que permite codificar la cantidad de años que han pasado desde la remodelación de una vivienda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[('numeric','YearsSinceRemodel')] = df_train['numeric']['YrSold'] - df_train['numeric']['YearRemodAdd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma manera, se puede agregar la variable `TotalQual` que representa el puntaje total de calidad asociado a una vivienda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[('numeric','TotalQual')]  = df_train['numeric']['OverallQual'] + df_train['numeric']['OverallCond']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, se pueden aplicar transformaciones exponenciales (como en el caso de Box-Cox), lineales (como los dos casos anteriores) o de cualquier otro tipo (trigonometricas, polinomiales) en función del tipo de problema que se trabaje, siempre y cuando la transformación tenga un trasfondo en el fenómeno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "Otra forma de inducir interacciones es por medio de variables dummy, en este caso se procede a generar codificaciones one hot. En este contexto, si se posee una variable codificada de esta forma y se multiplica por otra, ocurre que la nueva variable generada modela la interesección de eventos entre esas variables. \n",
    "\n",
    "1. Visualice las variables categóricas, genere un esquema de codificación y modele interacciones entre variables. Cuantifique estadísticamente el efecto de las variables de interacción con la variable de respuesta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de características \n",
    "\n",
    "En conjunción con el estudio, perfilamiento y aplicación de transformaciones iniciales a los datos, se hace necesario utilizar técnicas de selección de característcas. Estas permiten mejorar métricas de rendimiento en predicción. Sci-kit Learn provee de la API `sklearn.feature_selection ` que proporciona métodos de ayuda en este apartado.\n",
    "\n",
    "A continuación se estudian métodos de **selección de caraterísticas univariadas**.\n",
    "\n",
    "### Umbral de varianza \n",
    "\n",
    "Este método es un acercamiento simple y se basa en la premisa de que aquellas características con muy poca varianza no aportan información al modelo. \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se observan las características ordinales del conjunto de entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_plotter(df_train,dtype='ordinal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se observa que las variables `Functional` y `PoolQc` poseen una baja varianza (entre otras variables). Se importa el método de seleccion de caraterísticas por medio de umbral de varianza y se estudia su efecto en el conjunto de entrenamiento, para ello, es necesario codificar las variables estudiadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se generan pipelines sobre las variables ordinales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adquieren las categorias de cada variable\n",
    "ordinal_cat = [['Reg', 'IR1', 'IR2', 'IR3'],\n",
    "               ['AllPub', 'NoSewr', 'NoSeWa', 'ELO'], ['Gtl', 'Mod', 'Sev'],\n",
    "               ['Po', 'Fa', 'TA', 'Gd', 'Ex'], ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "               ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "               ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "               ['None', 'No', 'Mn', 'Av', 'Gd'],\n",
    "               ['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n",
    "               ['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n",
    "               ['Po', 'Fa', 'TA', 'Gd', 'Ex'], ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "               ['Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'],\n",
    "               ['None', 'Po', 'Fa', 'TA', 'Gd',\n",
    "                'Ex'], ['None', 'Unf', 'RFn', 'Fin'],\n",
    "               ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "               ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "               ['None', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "               ['None', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv']]\n",
    "\n",
    "ord_pipe = Pipeline(\n",
    "    steps=[('ordinal', OrdinalEncoder(categories = ordinal_cat))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(('numeric','SalePrice'), axis=1).dropna(how = 'all').copy()\n",
    "\n",
    "# Variable dependiente\n",
    "y_train = df_train['numeric']['SalePrice'].copy()\n",
    "\n",
    "# Se preparan los datos\n",
    "X_prep = ord_pipe.fit_transform(X_train['ordinal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc = pd.DataFrame(X_prep, columns=X_train['ordinal'].columns).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las varianzas asociadas al conjunto de características anterior viene dado por:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importa el método de umbral de varianza y se aplica a las características del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "sel = VarianceThreshold(threshold=.47)\n",
    "X_filtered = sel.fit_transform(X_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " * 1 * np.array(X_train['ordinal'].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lo que se filtran 9 caracteristicas, para obtener las caraterísticas seleccionadas se hace uso del método `.get_support()` para obtener una mascara de los indices seleccionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['ordinal'].columns[sel.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos de Scoring\n",
    "### Seleccion según percentil\n",
    "### RFE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
