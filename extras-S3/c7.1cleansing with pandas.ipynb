{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lang": "en"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "importación glob\n",
    "importar pandas como pd\n",
    "importar numpy como np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "# Llenado de valores faltantes\n",
    "\n",
    "**Objetivo**: El objetivo de esta sección es mostrar algunas de las técnicas que pueden ser usadas para rellenar datos faltantes. En este caso de estudio abordaremos los métodos `fillna` e `interpolate` de la librería `pandas`.\n",
    "\n",
    "## Calidad del aire de Beijing\n",
    "\n",
    "El caso de estudio se centra en un conjunto de datos de la siguiente publicación\n",
    "\n",
    "- \\[1\\] [Zhang, S., Guo, B., Dong, A., He, J., Xu, Z. and Chen, S.X. (2017) Cautionary Tales on Air-Quality Improvement in Beijing. Proceedings of the Royal Society A, Volume 473, No. 2205, Pages 20170457.](https://royalsocietypublishing.org/doi/pdf/10.1098/rspa.2017.0457)\n",
    "\n",
    "Disponible en el repositorio de [UCI](https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data)\n",
    "\n",
    "Las columnas incluidas en este conjunto de datos son las siguientes:\n",
    "\n",
    "- No: número de fila\n",
    "- year: año de datos en esta fila\n",
    "- month: mes de datos en esta fila\n",
    "- day: día de datos en esta fila\n",
    "- hour: hora de datos en esta fila\n",
    "- PM2.5: concentración de PM2.5 (ug / m ^ 3)\n",
    "- PM10: concentración de PM10 (ug / m ^ 3)\n",
    "- SO2: concentración de SO2 (ug / m ^ 3)\n",
    "- NO2: concentración de NO2 (ug / m ^ 3)\n",
    "- CO: concentración de CO (ug / m ^ 3)\n",
    "- O3: concentración de O3 (ug / m ^ 3)\n",
    "- TEMP: temperatura (grado Celsius)\n",
    "- PRES: presión (hPa)\n",
    "- DEWP: temperatura de rocío (grados Celsius) - temperatura de rocío es la más alta temperatura a la que empieza a condensarse el vapor de agua contenido en el aire.\n",
    "- RAIN: precipitación (mm)\n",
    "- wd: dirección del viento\n",
    "- WSPM: velocidad del viento (m / s)\n",
    "- station: nombre del sitio de monitoreo de la calidad del aire\n",
    "\n",
    "Las estaciones están todas ubicadas en el área metropolitana de Beijing. En la siguiente figura los puntos violetas muestran su ubicación, los triángulos son estaciones meteorológicas y los puntos rojos son otras estaciones no incluidas en el conjunto de datos. Las lineas azul y naranjas fividen a la metropolis en zona norte, centro y sur.     \n",
    "\n",
    "<img src=\"img/Beijing_AQ.png\" style='width:600px'>\n",
    "<center><b>Figura 1:</b> Ubicación de las estaciones de monitoreo - Fuente [1].</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consolidar conjunto de datos**  \n",
    "En primer lugar cargamos el conjunto de datos en memoria. Cada csv contiene las series de tiempo con frecuencia horaria de las 12 estaciones mencionadas, para 4 años calendario, desde marzo 2013 hasta febrero 2017. Por este motivo, todos los archivos tienen la misma cantidad de filas y columnas.\n",
    "\n",
    "Para trabajar todo el conjunto de datos en un solo archivo, construimos un `DataFrame` con un multiíndice en las columnas. En el primer nivel usaremos el nombre de la estación y en la segunda las variables anteriormente descritas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contruye una lista con las rutas de los archivos csv a cargar\n",
    "rutas_csv = sorted(glob.glob('data_UCI/PRSA_Data_20130301-20170228/*'))\n",
    "\n",
    "# inicializa el diccionario que contendra los DataFrame de cadad estacion\n",
    "df_dict = {}\n",
    "\n",
    "for csv_p in rutas_csv:\n",
    "    \n",
    "    # reporta el archivo cargado y su numero de filas y columas\n",
    "    print(f'cargando el archivo: {csv_p}')\n",
    "    df_ = pd.read_csv(csv_p, index_col=0)\n",
    "    print(f'shape: {df_.shape}\\n')\n",
    "    \n",
    "    # obten el nombre de la estacion ara usarla como llave del diccionario\n",
    "    estacion_arreglo = df_.station.unique()\n",
    "    estacion_name = estacion_arreglo[0]\n",
    "\n",
    "    df_dict[estacion_name] = df_\n",
    "\n",
    "# construye el DataFrame consolidado \n",
    "baq_df = pd.concat(df_dict, axis=1)\n",
    "baq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contar número de valores nulos**  \n",
    "Para mostrar las diferentes maneras de llenado de valores faltantes primero estudiaremos la cantidad de valores nulos que presentan los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuenta el numero de valores nulos y agrupa por columna en el segundo nivel\n",
    "grupo = baq_df.isnull().sum().groupby(level=1, sort=False)\n",
    "\n",
    "# suma los valos nulos de cada grupo\n",
    "n_nulos = grupo.sum()\n",
    "\n",
    "# reporta los resultados en numero absoluto y porcentaje \n",
    "df_nulos = pd.concat([n_nulos, n_nulos * 100 / (len(df_dict) * baq_df.shape[0])],\n",
    "                     keys=['n_nulos', 'porcentaje'], axis=1)\n",
    "df_nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contar la cantidad máxima de valores nulos contiguos**  \n",
    "Además de contar la cantidad de nulos, dado que se trata de series de tiempo, nos interesa saber la cantidad máxima de valores nulos contiguos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encontrar_max_contiguo(srs):\n",
    "    '''\n",
    "    Encuentra la maxima suma de `1` adyacentes en una\n",
    "    en un `iterable` de `0` y `1`\n",
    "    \n",
    "    Parametros\n",
    "    ----------\n",
    "    srs : iterable,\n",
    "        compuesto solo por elementos `0` o `1`.\n",
    "        \n",
    "    Retorna\n",
    "    -------\n",
    "    max : int,\n",
    "        el numero maximo de `1` contiguos\n",
    "    '''\n",
    "    \n",
    "    # inicializa valores maximos\n",
    "    max_ = 0\n",
    "    contiguo_max = 0\n",
    "    \n",
    "    # recorre los\n",
    "    for a in srs:\n",
    "        if a != 0:\n",
    "            contiguo_max += a\n",
    "            if max_ < contiguo_max:\n",
    "                max_ = contiguo_max\n",
    "        else:\n",
    "            contiguo_max = 0\n",
    "    return max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcula cantidad maxima de valores nulos contiguos en las\n",
    "# columnas que presentan valores nulos\n",
    "nulos_contiguos = baq_df.isnull().loc[:, baq_df.isnull().any(0)].apply(encontrar_max_contiguo)\n",
    "nulos_contiguos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcula  el promedio de nulos contiguos por variable\n",
    "grupo_nulos_contiguos = nulos_contiguos.groupby(level=1, sort=False)\n",
    "grupo_nulos_contiguos.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método `fillna`\n",
    "Para las columnas en las que el porcentaje de nulos es menor al 1% llenaremos los valores faltantes con métodos simples que están disponibles en el método `fillna`. Además, en estas columnas los valores faltantes consisten en a lo más un periodo de 13 horas y en su mayoría no sobrepasa las 6 horas corridas (ver celda anterior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# este objeto sera util para trabajar con índices multiples\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# muestra en pantalla las variables que presentan valores nulos menores al 1%\n",
    "df_nulos.index[(df_nulos['porcentaje'] < 1) & (df_nulos['porcentaje'] > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Llenado con una constante**  \n",
    "Para la variable `'RAIN'` asumiremos arbitrariamente que si el valor es nulo, se debe a ausencia de lluvia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muestra en pantalla las filas con valores nulos\n",
    "col_a_llenar = 'RAIN'\n",
    "filas_con_nulos = baq_df.loc(axis=1)[:, col_a_llenar].isnull().any(1)\n",
    "baq_df.loc[filas_con_nulos, idx[:, col_a_llenar]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rellena los valores nulos\n",
    "baq_df.loc(axis=1)[:, col_a_llenar] = baq_df.loc(axis=1)[:, col_a_llenar].fillna(0)\n",
    "baq_df.loc[filas_con_nulos, idx[:, col_a_llenar]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Llenado con la moda**  \n",
    "Para nuestra única variable categórica, `'wd'` (dirección del viento). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muestra en pantalla las filas con valores nulos\n",
    "col_a_llenar = 'wd'\n",
    "filas_con_nulos = baq_df.loc(axis=1)[:, col_a_llenar].isnull().any(1)\n",
    "baq_df.loc[filas_con_nulos, idx[:, col_a_llenar]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcula la moda por columna\n",
    "moda_wd = baq_df.loc(axis=1)[:, col_a_llenar].mode()\n",
    "moda_wd.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rellena los valores nulos\n",
    "baq_df.loc(axis=1)[:, col_a_llenar] = baq_df.loc(axis=1)[:, col_a_llenar].fillna(\n",
    "    moda_wd.iloc[0], axis=0)\n",
    "baq_df.loc[filas_con_nulos, idx[:, col_a_llenar]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Llenado con la mediana**  \n",
    "Para la columna `'WSPM'` (velocidad del viento). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muestra en pantalla las filas con valores nulos\n",
    "col_a_llenar = 'WSPM'\n",
    "filas_con_nulos = baq_df.loc(axis=1)[:, col_a_llenar].isnull().any(1)\n",
    "baq_df.loc[filas_con_nulos, idx[:, col_a_llenar]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcula la mediana por columna\n",
    "mediana_wspm = baq_df.loc(axis=1)[:, col_a_llenar].median()\n",
    "mediana_wspm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rellena los valores nulos\n",
    "baq_df.loc(axis=1)[:, col_a_llenar] = baq_df.loc(axis=1)[:, col_a_llenar].fillna(\n",
    "    mediana_wspm, axis=0)\n",
    "baq_df.loc[filas_con_nulos, idx[:, col_a_llenar]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Llenado con el promedio**  \n",
    "Para las columna `'PRES'` (presión)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muestra en pantalla las filas con valores nulos\n",
    "col_a_llenar = 'PRES'\n",
    "filas_con_nulos = baq_df.loc(axis=1)[:, col_a_llenar].isnull().any(1)\n",
    "baq_df.loc[filas_con_nulos, idx[:, col_a_llenar]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcula la moda por columna\n",
    "promedio_pres = baq_df.loc(axis=1)[:, col_a_llenar].mean()\n",
    "promedio_pres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rellena los valores nulos\n",
    "baq_df.loc(axis=1)[:, col_a_llenar] = baq_df.loc(axis=1)[:, col_a_llenar].fillna(\n",
    "    promedio_pres, axis=0)\n",
    "baq_df.loc[filas_con_nulos, idx[:, col_a_llenar]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Llenado con métodos `'ffill'` y `'bfill'`**  \n",
    "\n",
    "El método `fillna` tiene dos métodos que son útiles para DataFrame's donde que dos valores sean contiguos implica alguna noción de proximidad. En el caso de estudio, al tratarse de series de tiempo, dos valores contiguos  ocurren cerca el espacio temporal. Estos métodos son:\n",
    "1. Llenado hacia adelante (`'ffill'`): donde el último valor no nulo es propagado hacia adelante\n",
    "2. Llenado hacia atrás (`'bfill'`): donde los valores nulos son reemplazados por el siguiente valor no nulo.\n",
    "  \n",
    "<bk>  \n",
    "    \n",
    "- Así, para la columna `'Temp'` (temperatura), usaremos el método de llenado hacia adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muestra en pantalla las filas con valores nulos\n",
    "col_a_llenar = 'TEMP'\n",
    "filas_con_nulos = baq_df.loc(axis=1)[:, col_a_llenar].isnull().any(1)\n",
    "baq_df.loc[filas_con_nulos, idx[:, col_a_llenar]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rellena los valores nulos\n",
    "baq_df.loc(axis=1)[:, col_a_llenar] = baq_df.loc(axis=1)[:, col_a_llenar].fillna(\n",
    "    method='ffill')\n",
    "baq_df.loc[filas_con_nulos, idx[:, col_a_llenar]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Por su parte para la columna `'DEWP'`, usaremos el método de llenado hacia atrás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muestra en pantalla las filas con valores nulos\n",
    "col_a_llenar = 'DEWP'\n",
    "filas_con_nulos = baq_df.loc(axis=1)[:, col_a_llenar].isnull().any(1)\n",
    "baq_df.loc[filas_con_nulos, idx[:, col_a_llenar]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rellena los valores nulos\n",
    "baq_df.loc(axis=1)[:, col_a_llenar] = baq_df.loc(axis=1)[:, col_a_llenar].fillna(\n",
    "    method='bfill')\n",
    "baq_df.loc[filas_con_nulos, idx[:, col_a_llenar]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comprobar el procedimiento**  \n",
    "Ahora que ya hemos llenado los valores nulos de todas las columnas que presentaban un porcentaje menor a 1%, comprobemos que esto se ha ejecutado satisfactoriamente, calculando nuevamente los valores faltantes por columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuenta el numero de valores nulos y agrupa por columna en el segundo nivel\n",
    "grupo_despues_fillna = baq_df.isnull().sum().groupby(level=1, sort=False)\n",
    "\n",
    "# suma los valos nulos de cada grupo\n",
    "n_nulos_despues_fillna = grupo_despues_fillna.sum()\n",
    "\n",
    "# reporta los resultados en numero absoluto y porcentaje \n",
    "df_nulos_despues_fillna = pd.concat([n_nulos_despues_fillna, n_nulos_despues_fillna * 100 / (len(df_dict) * baq_df.shape[0])],\n",
    "                     keys=['n_nulos', 'porcentaje'], axis=1)\n",
    "print('antes fillna'); display(df_nulos)\n",
    "print('\\n\\ndespues fillna'); display(df_nulos_despues_fillna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método `interpolate`\n",
    "Los valores que faltantes restantes, serán rellenados mediante el método `interpolate` (su nombre es autoexplicativo).\n",
    "- Se podría estudiar cual es la interpolación que tiene mejor fit con cada una de los contaminantes. (**Ejercicio?:** 80, 20 para train y test y medida de performance RMSE.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = baq_df.loc[:, baq_df.isnull().any(0)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BREAK\n",
    "# NO INCLUIR (Puede servir de base para MOGP)\n",
    "El objetivo de la publicación \\[1\\] [Zhang, S., Guo, B., Dong, A., He, J., Xu, Z. and Chen, S.X. (2017) Cautionary Tales on Air-Quality Improvement in Beijing. Proceedings of the Royal Society A, Volume 473, No. 2205, Pages 20170457.](https://royalsocietypublishing.org/doi/pdf/10.1098/rspa.2017.0457) es calcular el promedio de PM2.5 en el año 2017.  \n",
    "- se puede estudiar el modelo de regresión propuesto en el paper\n",
    "- se puede usar MOGP para calcular dicho promedio, tomando la esperanza en el área de la ciudad completa y con un intervalo de confianza. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estacion_max_nul = baq_df.isnull().groupby(axis=1, level=0).sum().sum().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = baq_df.xs(estacion_max_nul, axis=1, level=0).copy()\n",
    "df = df_dict[estacion_max_nul]\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verificar que se tienen 4 años de datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_temp = ['year', 'month', 'day', 'hour']\n",
    "for ct in columnas_temp:\n",
    "    print(f'\\nvalue counts para columna {ct}')\n",
    "    display(df[ct].value_counts(sort=False).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizar la distribución de los valores nulos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulos_df = df.loc[:, df.isnull().any(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(~ nulos_df.isnull(), cbar=False)\n",
    "plt.title('Matriz de presencia de nulos');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular matriz de correlación de presencia de nulos\n",
    "nulos_corr = nulos_df.isnull().corr()\n",
    "\n",
    "def dibuja_matriz_de_correlación(corr_df):\n",
    "    '''Funcion personalizada para dibugjar la priz de correlación'''\n",
    "    sns.set(style=\"white\", font_scale=1.25)\n",
    "\n",
    "    # Generar una mascara para el triangulo superior\n",
    "    mascara = np.triu(np.ones_like(corr_df.values, dtype=np.bool))\n",
    "\n",
    "    # Iniciaizar fugura para configurar tamanño\n",
    "    f, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # Generar un colormap apropiado\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Dibujar matriz de correlación\n",
    "    sns.heatmap(corr_df.values, mask=mascara, cbar=True, cmap=cmap, vmin=-1, vmax=1,\n",
    "                annot=True, square=True, fmt='.2f', linewidths=.5, \n",
    "                cbar_kws={\"shrink\": .5}, yticklabels=corr_df.columns.values, \n",
    "                xticklabels=corr_df.columns.values)\n",
    "    \n",
    "\n",
    "dibuja_matriz_de_correlación(nulos_corr)\n",
    "plt.title('Correlación de la matriz de presencia de nulos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz nos indica la cuan fuertemente la presencia o ausencia de una variable afecta la presencia de la otra.  \n",
    "- si dicha correlación es cercana a 1, la presencia de una variable *asegura* la presencia de la otra.\n",
    "- si es cercana a -1, la presencia de una variable *asegura* la ausencia de la otra\n",
    "- si es cercana a 0, la presencia de una variable no tiene relación con la presencia de la otra.\n",
    "\n",
    "Así da la impresión que los censores fuertemente relacionados en cuanto a presencia de valores faltantes son:\n",
    "1. Sensores de tempreatura, presión, temperatura de rocío y velocidad del viento.\n",
    "2. Los sensores de PM10 y PM2.5\n",
    "3. Los sensores de CO y NO2\n",
    "\n",
    "En cuanto a correlaciones de presencia de valores faltantes de mediana intensidad:\n",
    "\n",
    "1. Los sensores de contaminantes entre ellos con excepción de los pares mencionados arriba.\n",
    "2. El sensor de dirección de viento con el sensor de velocidad del viento.\n",
    "\n",
    "Además podemos inferir gracias al rectángulo gris que la presencia de valores faltantes en los censores de los contaminantes (PM2, PM10, SO2, NO2, CO y O3) no tiene relación con la presencia de valores faltantes en los de condiciones atmosféricas (el resto de variables). \n",
    "\n",
    "Nos podríamos preguntar si esa última observación también se observa en la matriz de correlación de los valores observados en dichas variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular matriz de correlacion\n",
    "corr = nulos_df.corr()\n",
    "\n",
    "# dibuja matriz de correlación\n",
    "dibuja_matriz_de_correlación(corr)\n",
    "plt.title('Matriz de correlacion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos observar que existe una correlación importante entre la mayoría de los contaminantes, excepto el ozono (O3), que por suparte presenta una correlación importante con la temperatura.\n",
    "- Propongo hacer un MOGP con los contaminantes y la temperatura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NO INCLUIR\n",
    "**Estudio de nulos valores de temperatura**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.concat(df_dict, axis=1).xs('TEMP', axis=1, level=1)\n",
    "plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(temp_df.isnull(), cbar=False, linewidth=0)\n",
    "plt.title('Matriz de presencia de nulos: columnas `TEMP`');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_srs = temp_df.isnull().sum()\n",
    "plot_srs.plot(kind='bar', color='grey')\n",
    "for i, patch in enumerate(plt.gca().patches):\n",
    "    \n",
    "    # obtener coordenadas y medidas\n",
    "    x = patch.get_x()\n",
    "    h = patch.get_height()\n",
    "    w = patch.get_width()\n",
    "    \n",
    "    # adjuntar texto con valores de cada barra\n",
    "    plt.annotate(f'{plot_srs.iloc[i]}', xy=(x + w / 2, h), ha='center', va='bottom')\n",
    "    \n",
    "plt.ylim([1.1 * y_ for y_ in plt.gca().get_ylim()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dibuja_matriz_de_correlación(temp_df.isnull().corr())\n",
    "plt.title('Correlación de la matriz de presencia de nulos en TEMPERATURA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "# Expresiones regulares\n",
    "Limpieza de texto con expresiones regulares.\n",
    "\n",
    "## Datos de Twitter\n",
    "La [Competencia Kaggle: Twitter Sentiment Extraction](https://www.kaggle.com/c/tweet-sentiment-extraction) está actualmente en linea (el premio es 15 mil dólares!) y se trata de encontrar es substring del tweet que contiene el sentimiento del tweet completo.\n",
    "\n",
    "El objetivo de este pequeño ejercicio es simplemente dar una introducción a las **expresiones regulares**, ya que son muy útiles como herramienta para la limpieza de texto.\n",
    "\n",
    "**Importar librerias**  \n",
    "La librería para el uso de expresiones regulares en Python es `re`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "import string "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cargar datos**  \n",
    "El conjunto de datos de entrenamiento tiene 4 columnas:\n",
    "- textID: indetificar del tweet\n",
    "- text: contenido del tweet\n",
    "- selected_text: extracto del tweet que expresa el sentimiento\n",
    "- sentiment: sentimiento del tweet. Puede ser `'negative'`,`'neutral'`,`'positive'`\n",
    "\n",
    "Naturalmente, las columnas que nos interesan limpiar son `'text'`y `'selected_text`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_tse_entrenamiento = 'data_kaggle/twitter/train.csv'\n",
    "tse_df = pd.read_csv(ruta_tse_entrenamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srs = tse_df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- convertir el texto en minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srs = srs.str.lower()\n",
    "srs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_con_regex(srs, regex, verbose=True):\n",
    "    '''Funcion personalizada para eliminar texto de la serie `srs` \n",
    "    mediante la expresión regular `regex`.\n",
    "    '''\n",
    "    \n",
    "    if verbose:\n",
    "        \n",
    "        # encunetra filas que coinciden con la expresión regular\n",
    "        bools_ = srs.str.contains(regex, na=False)\n",
    "        \n",
    "        # reporta las filas que tienen coincidencia \n",
    "        print('antes de eliminar:')\n",
    "        display(srs[bools_])\n",
    "    \n",
    "    # elimina el substring\n",
    "    ret = srs.str.replace(regex, '')\n",
    "    \n",
    "    if verbose:\n",
    "        \n",
    "        # reporta las filas que tienen coincidencia \n",
    "        print('\\n\\ndespues de eliminar:')\n",
    "        display(ret[bools_])\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- quitar texto entre paréntesis cuadrados, pues estos son usados para denotar un lugar geolocalizado. Para esto usamos la expresión regular `'\\[.*?\\]'`. Esta está compuesta por:\n",
    "    - `'\\['` y `\\]` que denotan simplemente los caracteres `'['` y `']'`. Esto debido a que para las expresiones regulares, todos los carácteres hacen coincidencia con si mismos salvo `'+'`, `'?'`, `'.'`, `'*'`, `'^'`, `'$'`, `'('`, `')'`, `'['`, `']'`, `'{'`, `'}'`, `'|'` y `'\\'` que son caracteres especiales.\n",
    "    - `'.'` que denota cualquier carácter, excepto la nueva línea.\n",
    "    - `'*'` es un cuantificador de 0 o más ocurrencias de la expresión precedente. Cunado un cuantificador es sucedido por un `'?'` se transforma en un cuantificado flojo, lo que implica que a penas la expresión regular completa haga coincidencia, no seguirá buscando otra que la contenga. Por ejemplo en el string `'10100000100'` la expresión regular `'1.*?1'` entregara el substring `'101'` como coincidencia. Por su parte, con la expresión regular `'1.*1'` entregará `'101000001'`  \n",
    "    \n",
    "  Por lo tanto, la expresión denota cualquier texto entre paréntesis cuadrados que no contenga un salto de línea.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srs = eliminar_con_regex(srs, '\\[.*?\\]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eliminar los links: Para ello se usa la expresión regular `'https?://\\S+|www\\.\\S+'` que está compuesta por:\n",
    "    - `'?'` es un cuantificador que acepta 0 o 1 ocurrencia de la expresión regular que la antecede. En este caso 0 o 1 ocurrencias de `'s'`.\n",
    "    - `'\\S'` coincide con todo caracter que no sea espacio (i.e. `'\\t'`, `'\\r'`, `'\\n'` o `'\\f'`)\n",
    "    - `'+'` es un cuantificador que acepta 1 o más ocurrencias de la expresión regular que la antecede (patricularmente `'\\S'`)\n",
    "    - `'|'` denota un **o**, es decir `'https?://\\S+'` **o** `'www\\.\\S+'`\n",
    "    - `'\\.'` denota `'.'`, pues este es un carácter especial. Podemos inferir que `'\\'` se usa como carácter de escape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srs = eliminar_con_regex(srs, 'https?://\\S+|www\\.\\S+');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eliminar todos los caracteres de puntuación. Para ellos usamos el atributo `punctuation` de la libreria `string`. Además se usan los carácteres especiales `'['` y `']'` que permiten especificar todos los posibles valores que puede tomar un carácter para coincidir con la expresión regular, así `'[python]'` es una expresión regular que coincide con los caracteres `'h'`,  `'n'`, `'o'`, `'p'`, `'t'` o `'y'`.  Notamos también que no es necesario usar el carácter de escape excepto para `'['` y `']'` para los demás caracteres especiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srs = eliminar_con_regex(srs, f'[{punctuation}]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eliminar las palabras que contienen dígitos. Para ello usamos la expresión `'\\w*\\d\\w*'`. Esta está compuesta por:\n",
    "    - `'\\w'` coincide con todos los caracteres que pueden componer palabras (alfanumericos y guíon bajo).\n",
    "    - `'\\d'` coincide con dígitos (equivalente a `'[0-9]'`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srs = eliminar_con_regex(srs, '\\w*\\d\\w*');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tse_df.loc[:, 'text'] = srs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Outliers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "## IQR\n",
    "**Construyendo el puntaje IQR**\n",
    "\n",
    "El puntaje IQR es un método común para eliminar valores atípicos (u *outliers*). El siguiente diagrama ilustra cómo se comporta el puntaje IQR en una población distribuida normalmente\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Boxplot_vs_PDF.svg/800px-Boxplot_vs_PDF.svg.png\" alt=\"Stages 1\" style=\"width: 450px;\" align=\"center\" frameborder=\"200\"/>\n",
    "\n",
    "<bk>  \n",
    "    \n",
    "    \n",
    "<center><b>Fuente:</b> \n",
    "     <a href=\"https://en.wikipedia.org/wiki/Interquartile_range\">Wikipedia Interquartile range webpage</a> \n",
    "</center>\n",
    "\n",
    "Esta método de detección de valores atípicos es útil ya que depende de estadísticos robustos a la presencia de los mismos.  \n",
    "    \n",
    "Definimos la siguiente función que trabaja sobre valores de Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lang": "es"
   },
   "outputs": [],
   "source": [
    "# función personalizada para construir la puntuación IQR\n",
    "def q1 (srs):\n",
    "    \"\"\"\n",
    "    Calcula el primer cuartil de una serie.\n",
    "    \"\"\"\n",
    "    return srs.quantile (0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lang": "es"
   },
   "outputs": [],
   "source": [
    "def q3 (srs):\n",
    "    \"\"\"\n",
    "    Calcula el primer cuartil de una serie.\n",
    "    \"\"\"\n",
    "    return srs.quantile(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula_puntaje_RIC(df_, col, verbose=True):\n",
    "    \"\"\"Agrega una columna de puntaje de rango intercuartilico a `df`, \n",
    "    basado en una columna que que contiene `col` como columna.\n",
    "\n",
    "    Parametros\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        donde se agregará la columna de puntaje RIC\n",
    "    col : inmutable, \n",
    "        define la columna en la que se obtendrá la puntuación RIC \n",
    "        calculado\n",
    "    verbose : bool, \n",
    "        ya sea que muestre o no el calculo del RIC\n",
    "\n",
    "    Retorna\n",
    "    ----------\n",
    "    Copia de `df` con la columna de ``PRIC_{`col`}``\n",
    "    \"\"\"\n",
    "    \n",
    "    # crea una copia del data frame \n",
    "    df = df_.copy()\n",
    "\n",
    "    # calcula cuartiles 1 y 3\n",
    "    ric_df = df[[col]].aggregate([q1, q3])\n",
    "\n",
    "    # calcula RIC\n",
    "    ric_df.loc['RIC'] = ric_df.loc['q3'].iloc[0] - ric_df.loc['q1'].iloc[0]\n",
    "\n",
    "    if verbose:\n",
    "        # print('RIC:')\n",
    "        display(ric_df)\n",
    "\n",
    "    # inicializa la columna de puntaje en cero\n",
    "    df[f'PRIC_{col}'] = 0\n",
    "\n",
    "    # identifica filas menores a Q1\n",
    "    bools_menor_Q1 = (df[col] <= ric_df.loc['q1'].iloc[0])\n",
    "    sel_idx_Q1 = df.index[bools_menor_Q1]\n",
    "\n",
    "    # identifica filas mayores a Q3 \n",
    "    bools_mayor_Q3 = (df[col] >= ric_df.loc['q3'].iloc[0])\n",
    "    sel_idx_Q3 = df.index[bools_mayor_Q3]\n",
    "    \n",
    "    # calcula puntaje RIC para filas menores a Q1\n",
    "    df.loc[sel_idx_Q1, f'PRIC_{col}'] = (\n",
    "        ric_df.loc['q1'].iloc[0] - df.loc[sel_idx_Q1, col].values) / \\\n",
    "        ric_df.loc['RIC'].iloc[0]\n",
    "\n",
    "    # calcula puntaje RIC para filas menores a Q1\n",
    "    df.loc[sel_idx_Q3, f'PRIC_{col}'] = (\n",
    "        df.loc[sel_idx_Q3, col].values - ric_df.loc['q3'].iloc[0]) / \\\n",
    "        ric_df.loc['RIC'].iloc[0]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip \"data_UCI/household_power_consumption.txt.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electricidad_df = pd.read_csv('household_power_consumption.txt', sep=';', na_values='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for j in range(2, electricidad_df.shape[1]):\n",
    "    plt.figure()\n",
    "    sns.boxplot(electricidad_df.iloc[:, j])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado el plot de arriba, lo usaría solamente en 2 a 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = None\n",
    "for col in electricidad_df.columns[2:6]:\n",
    "    if df_ is None:\n",
    "        df_ = calcula_puntaje_RIC(electricidad_df, col)\n",
    "        \n",
    "    else:\n",
    "        df_ = calcula_puntaje_RIC(df_, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STD\n",
    "[El notebook de kaggle más votado](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python): interés especial en la sección **Out Liars!** se usa un criterio de std  \n",
    "\n",
    "El dataset ya está descargado en:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('data_kaggle/house-prices-advanced-regression-techniques/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "# Prueba de hipótesis: regresión sobre los precios de la vivienda\n",
    "[El notebook de kaggle más votado](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python): interés especial en la sección 5 donde se analizan los supuestos más importantes de regresión lineal ... esto podría ser un marco para la prueba de hipótesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos árboles\n",
    "para llenar datos faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selenium36",
   "language": "python",
   "name": "selenium36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "es",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "es",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
